{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b7973eef-7e5e-4ca4-844a-1d016f5a638b",
      "metadata": {
        "id": "b7973eef-7e5e-4ca4-844a-1d016f5a638b"
      },
      "source": [
        "[Referenece YouTube Video by LlamaIndex](https://www.youtube.com/watch?v=T0bgevj0vto)\n",
        "\n",
        "### Gemini Summary\n",
        "\n",
        "Four concepts of agents:\n",
        "- **Level 1: Tool Use** - This is the most simple level where the agent can decide what tool to pick and the arguments to use with that tool.\n",
        "- **Level 2: Reasoning Loop with Conversation Memory** - This level includes some sort of reasoning loop with conversation memory. This means the agent can maintain the state of earlier conversation and use it to inform future responses.\n",
        "- **Level 3: Combine Tool Use with Reasoning Loop and Memory** - This level combines tool use with a reasoning loop and memory. The agent can select a set of tools to use and then loop back to see if the task has been solved.\n",
        "- **Level 4: Fancy Reasoning Loop, Memory, and Tool Use** - This is the most advanced level where the agent can not only plan the next step but also plan out an entire query plan to achieve the task at hand.\n",
        "\n",
        "Also defined the different modules and linking them together. The modules include:\n",
        "- Agent Input\n",
        "- React Prompt to LLM\n",
        "- React Output Parser\n",
        "- Run Tool\n",
        "- Process Agent Response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461d6f50",
      "metadata": {},
      "source": [
        "# Building an Agent around a Query Pipeline\n",
        "\n",
        "\n",
        "In this cookbook we show you how to build an agent around a query pipeline.\n",
        "\n",
        "Agents offer the ability to do complex, sequential reasoning on top of any query DAG that you have setup. Conceptually this is also one of the ways you can add a \"loop\" to the graph.\n",
        "\n",
        "In this tutorial we show you how to build a full ReAct agent that can do tool picking from scratch.\n",
        "\n",
        "We will be using LlamaIndex v0.10 - https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e56c5390",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install --upgrade llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "85208cc9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install llama-index-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9abc01fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: llama-index\n",
            "Version: 0.10.24\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\n",
            "Requires: llama-index-agent-openai, llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-legacy, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse\n",
            "Required-by: llama-hub\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0127df72",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: llama-index-core\n",
            "Version: 0.10.24.post1\n",
            "Summary: Interface between LLMs and your data\n",
            "Home-page: https://llamaindex.ai\n",
            "Author: Jerry Liu\n",
            "Author-email: jerry@llamaindex.ai\n",
            "License: MIT\n",
            "Location: c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\n",
            "Requires: aiohttp, dataclasses-json, deprecated, dirtyjson, fsspec, httpx, llamaindex-py-client, nest-asyncio, networkx, nltk, numpy, openai, pandas, pillow, PyYAML, requests, SQLAlchemy, tenacity, tiktoken, tqdm, typing-extensions, typing-inspect\n",
            "Required-by: llama-index, llama-index-agent-openai, llama-index-cli, llama-index-embeddings-huggingface, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-multi-modal-llms-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index-readers-file, llama-index-readers-llama-parse, llama-index-vector-stores-chroma, llama-index-vector-stores-faiss, llama-parse\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show llama-index-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0aacdcad-f0c1-40f4-b319-6c4cf3b309c7",
      "metadata": {
        "id": "0aacdcad-f0c1-40f4-b319-6c4cf3b309c7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_pipeline import QueryPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "122dce22-6d3a-4d4a-a265-a6a3d3f90d26",
      "metadata": {
        "id": "122dce22-6d3a-4d4a-a265-a6a3d3f90d26"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Setup Data\n",
        "\n",
        "We use the chinook database as sample data. [Source](https://www.sqlitetutorial.net/sqlite-sample-database/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cce0a58f",
      "metadata": {
        "id": "cce0a58f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index-llms-openai in c:\\users\\vibud\\miniconda3\\lib\\site-packages (0.1.13)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.24 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-llms-openai) (0.10.24.post1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.6.0)\n",
            "Requirement already satisfied: httpx in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.25.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.8.1)\n",
            "Requirement already satisfied: numpy in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.12.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (9.5.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (4.10.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.15.0)\n",
            "Requirement already satisfied: pydantic>=1.10 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.10.11)\n",
            "Requirement already satisfied: certifi in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.18.0)\n",
            "Requirement already satisfied: idna in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.4)\n",
            "Requirement already satisfied: sniffio in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: click in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (8.1.7)\n",
            "Requirement already satisfied: joblib in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.8.8)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.5.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.26.18)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.0.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (3.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2022.7)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (2023.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (0.14.0)\n",
            "Requirement already satisfied: packaging>=17.0 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vibud\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.24->llama-index-llms-openai) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "926b79ba-1868-46ca-bb7e-2bd3c907773c",
      "metadata": {
        "id": "926b79ba-1868-46ca-bb7e-2bd3c907773c",
        "outputId": "b43c8c2b-d986-49f0-e570-a1c5d516a336",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  298k  100  298k    0     0   339k      0 --:--:-- --:--:-- --:--:--  339k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: .\n",
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!curl \"https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\" -O ./chinook.zip\n",
        "!unzip -o ./chinook.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "763a442a-cfcd-4e63-9121-e3a45dc3acff",
      "metadata": {
        "id": "763a442a-cfcd-4e63-9121-e3a45dc3acff",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SQLDatabase\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    select,\n",
        "    column,\n",
        ")\n",
        "\n",
        "engine = create_engine(\"sqlite:///chinook.db\")\n",
        "sql_database = SQLDatabase(engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419f97cf-63c1-456b-babd-b07d9ce4b937",
      "metadata": {
        "id": "419f97cf-63c1-456b-babd-b07d9ce4b937"
      },
      "source": [
        "### Setup Calback Manager\n",
        "\n",
        "We setup a global callback manager (helps in case you want to plug in downstream observability integrations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5fb303b7-2fb1-496b-aff8-57844cdc519b",
      "metadata": {
        "id": "5fb303b7-2fb1-496b-aff8-57844cdc519b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define global callback setting\n",
        "from llama_index.core.settings import Settings\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "\n",
        "callback_manager = CallbackManager()\n",
        "Settings.callback_manager = callback_manager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbc38d8-2a78-4457-888d-cfe8960a9c6b",
      "metadata": {
        "id": "fbbc38d8-2a78-4457-888d-cfe8960a9c6b"
      },
      "source": [
        "## Setup Text-to-SQL Query Engine / Tool\n",
        "\n",
        "Now we setup a simple text-to-SQL tool: given a query, translate text to SQL, execute against database, and get back a result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d72eb79e-05b4-4260-b086-e837b01cce77",
      "metadata": {
        "id": "d72eb79e-05b4-4260-b086-e837b01cce77",
        "tags": []
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:41\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     40\u001b[0m     llm \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[1;32m---> 41\u001b[0m     validate_openai_api_key(llm\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\llms\\openai\\utils.py:398\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
            "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquery_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NLSQLTableQueryEngine\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QueryEngineTool\n\u001b[1;32m----> 4\u001b[0m sql_query_engine \u001b[38;5;241m=\u001b[39m NLSQLTableQueryEngine(\n\u001b[0;32m      5\u001b[0m     sql_database\u001b[38;5;241m=\u001b[39msql_database,\n\u001b[0;32m      6\u001b[0m     tables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbums\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtracks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martists\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m sql_tool \u001b[38;5;241m=\u001b[39m QueryEngineTool\u001b[38;5;241m.\u001b[39mfrom_defaults(\n\u001b[0;32m     10\u001b[0m     query_engine\u001b[38;5;241m=\u001b[39msql_query_engine,\n\u001b[0;32m     11\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_tool\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ),\n\u001b[0;32m     15\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\indices\\struct_store\\sql_query.py:447\u001b[0m, in \u001b[0;36mNLSQLTableQueryEngine.__init__\u001b[1;34m(self, sql_database, llm, text_to_sql_prompt, context_query_kwargs, synthesize_response, response_synthesis_prompt, refine_synthesis_prompt, tables, service_context, context_str_prefix, sql_only, callback_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize params.\"\"\"\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# self._tables = tables\u001b[39;00m\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_retriever \u001b[38;5;241m=\u001b[39m NLSQLRetriever(\n\u001b[0;32m    448\u001b[0m     sql_database,\n\u001b[0;32m    449\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m    450\u001b[0m     text_to_sql_prompt\u001b[38;5;241m=\u001b[39mtext_to_sql_prompt,\n\u001b[0;32m    451\u001b[0m     context_query_kwargs\u001b[38;5;241m=\u001b[39mcontext_query_kwargs,\n\u001b[0;32m    452\u001b[0m     tables\u001b[38;5;241m=\u001b[39mtables,\n\u001b[0;32m    453\u001b[0m     context_str_prefix\u001b[38;5;241m=\u001b[39mcontext_str_prefix,\n\u001b[0;32m    454\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m    455\u001b[0m     sql_only\u001b[38;5;241m=\u001b[39msql_only,\n\u001b[0;32m    456\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m    457\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    460\u001b[0m     synthesize_response\u001b[38;5;241m=\u001b[39msynthesize_response,\n\u001b[0;32m    461\u001b[0m     response_synthesis_prompt\u001b[38;5;241m=\u001b[39mresponse_synthesis_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    468\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\indices\\struct_store\\sql_retriever.py:216\u001b[0m, in \u001b[0;36mNLSQLRetriever.__init__\u001b[1;34m(self, sql_database, text_to_sql_prompt, context_query_kwargs, tables, table_retriever, context_str_prefix, sql_parser_mode, llm, embed_model, service_context, return_raw, handle_sql_errors, sql_only, callback_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_get_tables_fn(\n\u001b[0;32m    213\u001b[0m     sql_database, tables, context_query_kwargs, table_retriever\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context_str_prefix \u001b[38;5;241m=\u001b[39m context_str_prefix\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m llm \u001b[38;5;129;01mor\u001b[39;00m llm_from_settings_or_context(Settings, service_context)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_to_sql_prompt \u001b[38;5;241m=\u001b[39m text_to_sql_prompt \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TEXT_TO_SQL_PROMPT\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sql_parser_mode \u001b[38;5;241m=\u001b[39m sql_parser_mode\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\settings.py:264\u001b[0m, in \u001b[0;36mllm_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mllm\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mllm\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\settings.py:39\u001b[0m, in \u001b[0;36m_Settings.llm\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm \u001b[38;5;241m=\u001b[39m resolve_llm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\llms\\utils.py:48\u001b[0m, in \u001b[0;36mresolve_llm\u001b[1;34m(llm, callback_manager)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-llms-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-llms-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m         )\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo disable the LLM entirely, set llm=None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m         )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(llm, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m     59\u001b[0m     splits \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nTo disable the LLM entirely, set llm=None.\n******"
          ]
        }
      ],
      "source": [
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "sql_query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=sql_database,\n",
        "    tables=[\"albums\", \"tracks\", \"artists\"],\n",
        "    verbose=True,\n",
        ")\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    name=\"sql_tool\",\n",
        "    description=(\n",
        "        \"Useful for translating a natural language query into a SQL query\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af9d8d26-65c8-4788-a0ad-8d2448b23254",
      "metadata": {
        "id": "af9d8d26-65c8-4788-a0ad-8d2448b23254"
      },
      "source": [
        "## Setup ReAct Agent Pipeline\n",
        "\n",
        "We now setup a ReAct pipeline for a single step using our Query Pipeline syntax. This is a multi-part process that does the following:\n",
        "1. Takes in agent inputs\n",
        "2. Calls ReAct prompt using LLM to generate next action/tool (or returns a response).\n",
        "3. If tool/action is selected, call tool pipeline to execute tool + collect response.\n",
        "4. If response is generated, get response.\n",
        "\n",
        "Throughout this we'll use a variety of agent-specific query components. Unlike normal query pipelines, these are specifically designed for query pipelines that are used in a `QueryPipelineAgentWorker`:\n",
        "- An `AgentInputComponent` that allows you to convert the agent inputs (Task, state dictionary) into a set of inputs for the query pipeline.\n",
        "- An `AgentFnComponent`: a general processor that allows you to take in the current Task, state, as well as any arbitrary inputs, and returns an output. In this cookbook we define a function component to format the ReAct prompt. However, you can put this anywhere.\n",
        "- [Not used in this notebook] An `CustomAgentComponent`: similar to `AgentFnComponent`, you can implement `_run_component` to define your own logic, with access to Task and state. It is more verbose but more flexible than `AgentFnComponent` (e.g. you can define init variables, and callbacks are in the base class).\n",
        "\n",
        "Note that any function passed into `AgentFnComponent` and `AgentInputComponent` MUST include `task` and `state` as input variables, as these are inputs passed from the agent.\n",
        "\n",
        "Note that the output of an agentic query pipeline MUST be `Tuple[AgentChatResponse, bool]`. You'll see this below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9884b8c0-0ec3-4b14-9538-db9f5be63466",
      "metadata": {
        "id": "9884b8c0-0ec3-4b14-9538-db9f5be63466",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
        "\n",
        "qp = QP(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a024201f-a39b-4e23-a567-9737026dd771",
      "metadata": {
        "id": "a024201f-a39b-4e23-a567-9737026dd771"
      },
      "source": [
        "### Define Agent Input Component\n",
        "\n",
        "Here we define the agent input component, called at the beginning of every agent step. Besides passing along the input, we also do initialization/state modification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1a9252-57df-47cc-9fcf-84d87b1e0102",
      "metadata": {
        "id": "9b1a9252-57df-47cc-9fcf-84d87b1e0102",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.react.types import (\n",
        "    ActionReasoningStep,\n",
        "    ObservationReasoningStep,\n",
        "    ResponseReasoningStep,\n",
        ")\n",
        "from llama_index.core.agent import Task, AgentChatResponse\n",
        "from llama_index.core.query_pipeline import (\n",
        "    AgentInputComponent,\n",
        "    AgentFnComponent,\n",
        "    CustomAgentComponent,\n",
        "    QueryComponent,\n",
        "    ToolRunnerComponent,\n",
        ")\n",
        "from llama_index.core.llms import MessageRole\n",
        "from typing import Dict, Any, Optional, Tuple, List, cast\n",
        "\n",
        "\n",
        "## Agent Input Component\n",
        "## This is the component that produces agent inputs to the rest of the components\n",
        "## Can also put initialization logic here.\n",
        "def agent_input_fn(task: Task, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Agent input function.\n",
        "\n",
        "    Returns:\n",
        "        A Dictionary of output keys and values. If you are specifying\n",
        "        src_key when defining links between this component and other\n",
        "        components, make sure the src_key matches the specified output_key.\n",
        "\n",
        "    \"\"\"\n",
        "    # initialize current_reasoning\n",
        "    if \"current_reasoning\" not in state:\n",
        "        state[\"current_reasoning\"] = []\n",
        "    reasoning_step = ObservationReasoningStep(observation=task.input)\n",
        "    state[\"current_reasoning\"].append(reasoning_step)\n",
        "    return {\"input\": task.input}\n",
        "\n",
        "\n",
        "agent_input_component = AgentInputComponent(fn=agent_input_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd914690-ee4d-4b3b-bb95-9cce0994b9c1",
      "metadata": {
        "id": "dd914690-ee4d-4b3b-bb95-9cce0994b9c1"
      },
      "source": [
        "### Define Agent Prompt\n",
        "\n",
        "Here we define the agent component that generates a ReAct prompt, and after the output is generated from the LLM, parses into a structured object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1957da-bc2b-4186-abf0-11148a23dba7",
      "metadata": {
        "id": "8a1957da-bc2b-4186-abf0-11148a23dba7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import ReActChatFormatter\n",
        "from llama_index.core.query_pipeline import InputComponent, Link\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool\n",
        "\n",
        "\n",
        "## define prompt function\n",
        "def react_prompt_fn(\n",
        "    task: Task, state: Dict[str, Any], input: str, tools: List[BaseTool]\n",
        ") -> List[ChatMessage]:\n",
        "    # Add input to reasoning\n",
        "    chat_formatter = ReActChatFormatter()\n",
        "    return chat_formatter.format(\n",
        "        tools,\n",
        "        chat_history=task.memory.get() + state[\"memory\"].get_all(),\n",
        "        current_reasoning=state[\"current_reasoning\"],\n",
        "    )\n",
        "\n",
        "\n",
        "react_prompt_component = AgentFnComponent(\n",
        "    fn=react_prompt_fn, partial_dict={\"tools\": [sql_tool]}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4f7b40-7a07-4116-95b7-6a85e9386344",
      "metadata": {
        "id": "ca4f7b40-7a07-4116-95b7-6a85e9386344",
        "outputId": "f23f33e8-3bfa-45ec-88b5-c97b9502e04d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "system: \n",
            "You are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\n",
            "\n",
            "## Tools\n",
            "You have access to a wide variety of tools. You are responsible for using\n",
            "the tools in any sequence you deem appropriate to complete the task at hand.\n",
            "This may require breaking the task into subtasks and using different tools\n",
            "to complete each subtask.\n",
            "\n",
            "You have access to the following tools:\n",
            "> Tool Name: sql_tool\n",
            "Tool Description: Useful for translating a natural language query into a SQL query\n",
            "Tool Args: {\"type\": \"object\", \"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"]}\n",
            "\n",
            "\n",
            "## Output Format\n",
            "To answer the question, please use the following format.\n",
            "\n",
            "```\n",
            "Thought: I need to use a tool to help me answer the question.\n",
            "Action: tool name (one of sql_tool) if using a tool.\n",
            "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\n",
            "```\n",
            "\n",
            "Please ALWAYS start with a Thought.\n",
            "\n",
            "Please use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\n",
            "\n",
            "If this format is used, the user will respond in the following format:\n",
            "\n",
            "```\n",
            "Observation: tool response\n",
            "```\n",
            "\n",
            "You should keep repeating the above format until you have enough information\n",
            "to answer the question without using any more tools. At that point, you MUST respond\n",
            "in the one of the following two formats:\n",
            "\n",
            "```\n",
            "Thought: I can answer without using any more tools.\n",
            "Answer: [your answer here]\n",
            "```\n",
            "\n",
            "```\n",
            "Thought: I cannot answer the question with the provided tools.\n",
            "Answer: Sorry, I cannot answer your query.\n",
            "```\n",
            "\n",
            "## Current Conversation\n",
            "Below is the current conversation consisting of interleaving human and assistant messages.\n",
            "\n",
            "\n",
            "assistant: \n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.llms.generic_utils import messages_to_prompt\n",
        "\n",
        "chat_formatter = ReActChatFormatter()\n",
        "msgs = chat_formatter.format(\n",
        "    [sql_tool],\n",
        "    chat_history=[],\n",
        "    current_reasoning=[]\n",
        ")\n",
        "print(messages_to_prompt(msgs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7acd7088-7804-48c5-8c7f-0c5c112d27f0",
      "metadata": {
        "id": "7acd7088-7804-48c5-8c7f-0c5c112d27f0"
      },
      "source": [
        "### Define Agent Output Parser + Tool Pipeline\n",
        "\n",
        "Once the LLM gives an output, we have a decision tree:\n",
        "1. If an answer is given, then we're done. Process the output\n",
        "2. If an action is given, we need to execute the specified tool with the specified args, and then process the output.\n",
        "\n",
        "Tool calling can be done via the `ToolRunnerComponent` module. This is a simple wrapper module that takes in a list of tools, and can be \"executed\" with the specified tool name (every tool has a name) and tool action.\n",
        "\n",
        "We implement this overall module `OutputAgentComponent` that subclasses `CustomAgentComponent`.\n",
        "\n",
        "Note: we also implement `sub_query_components` to pass through higher-level callback managers to the tool runner submodule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702e3070-40df-491b-b468-d19ba98edc6b",
      "metadata": {
        "id": "702e3070-40df-491b-b468-d19ba98edc6b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from typing import Set, Optional\n",
        "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
        "from llama_index.core.llms import ChatResponse\n",
        "from llama_index.core.agent.types import Task\n",
        "\n",
        "\n",
        "def parse_react_output_fn(\n",
        "    task: Task, state: Dict[str, Any], chat_response: ChatResponse\n",
        "):\n",
        "    \"\"\"Parse ReAct output into a reasoning step.\"\"\"\n",
        "    output_parser = ReActOutputParser()\n",
        "    reasoning_step = output_parser.parse(chat_response.message.content)\n",
        "    return {\"done\": reasoning_step.is_done, \"reasoning_step\": reasoning_step}\n",
        "\n",
        "\n",
        "parse_react_output = AgentFnComponent(fn=parse_react_output_fn)\n",
        "\n",
        "\n",
        "def run_tool_fn(\n",
        "    task: Task, state: Dict[str, Any], reasoning_step: ActionReasoningStep\n",
        "):\n",
        "    \"\"\"Run tool and process tool output.\"\"\"\n",
        "    tool_runner_component = ToolRunnerComponent(\n",
        "        [sql_tool], callback_manager=task.callback_manager\n",
        "    )\n",
        "    tool_output = tool_runner_component.run_component(\n",
        "        tool_name=reasoning_step.action,\n",
        "        tool_input=reasoning_step.action_input,\n",
        "    )\n",
        "    observation_step = ObservationReasoningStep(observation=str(tool_output))\n",
        "    state[\"current_reasoning\"].append(observation_step)\n",
        "    # TODO: get output\n",
        "\n",
        "    return {\"response_str\": observation_step.get_content(), \"is_done\": False}\n",
        "\n",
        "\n",
        "run_tool = AgentFnComponent(fn=run_tool_fn)\n",
        "\n",
        "\n",
        "def process_response_fn(\n",
        "    task: Task, state: Dict[str, Any], response_step: ResponseReasoningStep\n",
        "):\n",
        "    \"\"\"Process response.\"\"\"\n",
        "    state[\"current_reasoning\"].append(response_step)\n",
        "    response_str = response_step.response\n",
        "    # Now that we're done with this step, put into memory\n",
        "    state[\"memory\"].put(ChatMessage(content=task.input, role=MessageRole.USER))\n",
        "    state[\"memory\"].put(\n",
        "        ChatMessage(content=response_str, role=MessageRole.ASSISTANT)\n",
        "    )\n",
        "\n",
        "    return {\"response_str\": response_str, \"is_done\": True}\n",
        "\n",
        "\n",
        "process_response = AgentFnComponent(fn=process_response_fn)\n",
        "\n",
        "\n",
        "def process_agent_response_fn(\n",
        "    task: Task, state: Dict[str, Any], response_dict: dict\n",
        "):\n",
        "    \"\"\"Process agent response.\"\"\"\n",
        "    return (\n",
        "        AgentChatResponse(response_dict[\"response_str\"]),\n",
        "        response_dict[\"is_done\"],\n",
        "    )\n",
        "\n",
        "\n",
        "process_agent_response = AgentFnComponent(fn=process_agent_response_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c1382b-f9ef-42a6-b117-91d7b2a14921",
      "metadata": {
        "id": "03c1382b-f9ef-42a6-b117-91d7b2a14921"
      },
      "source": [
        "### Stitch together Agent Query Pipeline\n",
        "\n",
        "We can now stitch together the top-level agent pipeline: agent_input -> react_prompt -> llm -> react_output.\n",
        "\n",
        "The last component is the if-else component that calls sub-components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219ec2ed-474e-450e-866c-15772055b8b1",
      "metadata": {
        "id": "219ec2ed-474e-450e-866c-15772055b8b1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "qp.add_modules(\n",
        "    {\n",
        "        \"agent_input\": agent_input_component,\n",
        "        \"react_prompt\": react_prompt_component,\n",
        "        \"llm\": OpenAI(model=\"gpt-4-1106-preview\"),\n",
        "        \"react_output_parser\": parse_react_output,\n",
        "        \"run_tool\": run_tool,\n",
        "        \"process_response\": process_response,\n",
        "        \"process_agent_response\": process_agent_response,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be90408b-0893-4cab-a7c7-32206979eecd",
      "metadata": {
        "id": "be90408b-0893-4cab-a7c7-32206979eecd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# link input to react prompt to parsed out response (either tool action/input or observation)\n",
        "qp.add_chain([\"agent_input\", \"react_prompt\", \"llm\", \"react_output_parser\"])\n",
        "\n",
        "# add conditional link from react output to tool call (if not done)\n",
        "qp.add_link(\n",
        "    \"react_output_parser\",\n",
        "    \"run_tool\",\n",
        "    condition_fn=lambda x: not x[\"done\"],\n",
        "    input_fn=lambda x: x[\"reasoning_step\"],\n",
        ")\n",
        "# add conditional link from react output to final response processing (if done)\n",
        "qp.add_link(\n",
        "    \"react_output_parser\",\n",
        "    \"process_response\",\n",
        "    condition_fn=lambda x: x[\"done\"],\n",
        "    input_fn=lambda x: x[\"reasoning_step\"],\n",
        ")\n",
        "\n",
        "# whether response processing or tool output processing, add link to final agent response\n",
        "qp.add_link(\"process_response\", \"process_agent_response\")\n",
        "qp.add_link(\"run_tool\", \"process_agent_response\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207ff8b0-efc3-482d-8224-f3b82c9a1c2c",
      "metadata": {
        "id": "207ff8b0-efc3-482d-8224-f3b82c9a1c2c"
      },
      "source": [
        "### Visualize Query Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5e739b-ef99-44dd-91a3-a495fa11e4f7",
      "metadata": {
        "id": "cd5e739b-ef99-44dd-91a3-a495fa11e4f7",
        "outputId": "fb5f4e01-fdf7-4edf-c348-b988b2e409bd",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agent_dag.html\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600px\"\n",
              "            src=\"agent_dag.html\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x2b4dc7b80>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvis.network import Network\n",
        "\n",
        "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "net.from_nx(qp.clean_dag)\n",
        "net.show(\"agent_dag.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7785354-1fe0-49b4-8b48-08ce51853a4e",
      "metadata": {
        "id": "f7785354-1fe0-49b4-8b48-08ce51853a4e"
      },
      "source": [
        "### Setup Agent Worker around Text-to-SQL Query Pipeline\n",
        "\n",
        "This is our way to setup an agent around a text-to-SQL Query Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01ea94b-3d3f-4b26-b71a-eea3d1d1ec0d",
      "metadata": {
        "id": "d01ea94b-3d3f-4b26-b71a-eea3d1d1ec0d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import QueryPipelineAgentWorker, AgentRunner\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "\n",
        "agent_worker = QueryPipelineAgentWorker(qp)\n",
        "agent = AgentRunner(\n",
        "    agent_worker, callback_manager=CallbackManager([]), verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "373bd661-4a6b-4c5b-b35b-0082921cc3b9",
      "metadata": {
        "id": "373bd661-4a6b-4c5b-b35b-0082921cc3b9"
      },
      "source": [
        "### Run the Agent\n",
        "\n",
        "Let's try the agent on some sample queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ea55110-98fb-4acf-b2f8-eee2b15c087b",
      "metadata": {
        "id": "6ea55110-98fb-4acf-b2f8-eee2b15c087b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# start task\n",
        "task = agent.create_task(\n",
        "    \"What are some tracks from the artist AC/DC? Limit it to 3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f35e816-e76b-43ba-8eb4-ddda8ee97a0b",
      "metadata": {
        "id": "7f35e816-e76b-43ba-8eb4-ddda8ee97a0b",
        "outputId": "9942a26a-8142-4e7b-9b76-13dc823925f2",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Running step e9a672f8-4db0-4ee6-8db8-c39a3545a0f7. Step input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
            "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
            "task: task_id='ed2d3d39-5611-43d4-ab31-9f12b8ea0972' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
            "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output_parser with input: \n",
            "chat_response: assistant: Thought: I need to use a tool to help me answer the question.\n",
            "Action: sql_tool\n",
            "Action Input: {\"input\": \"SELECT track_name FROM tracks WHERE artist_name = 'AC/DC' LIMIT 3\"}\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module run_tool with input: \n",
            "reasoning_step: thought='I need to use a tool to help me answer the question.' action='sql_tool' action_input={'input': \"SELECT track_name FROM tracks WHERE artist_name = 'AC/DC' LIMIT 3\"}\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_agent_response with input: \n",
            "response_dict: {'response_str': 'Observation: {\\'output\\': ToolOutput(content=\\'The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\\\\\'s Get It Up\".\\', tool_nam...\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "step_output = agent.run_step(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bad589-22bd-41c9-9225-6c06bf8b6f38",
      "metadata": {
        "id": "59bad589-22bd-41c9-9225-6c06bf8b6f38",
        "outputId": "85714848-9dd4-4020-ffb8-bb49154ece89",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "step_output.is_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a13e449-db6c-43a3-99b7-70d23687e8b8",
      "metadata": {
        "id": "2a13e449-db6c-43a3-99b7-70d23687e8b8",
        "outputId": "97040527-1385-4fcb-db7d-f117804cd6ae",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Running step 103a02cd-a729-45c9-92d4-32091c215b4b. Step input: None\n",
            "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
            "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
            "task: task_id='ed2d3d39-5611-43d4-ab31-9f12b8ea0972' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
            "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output_parser with input: \n",
            "chat_response: assistant: Thought: The user has repeated the request for tracks from the artist AC/DC, but they have already been provided with this information in the previous observation.\n",
            "\n",
            "Answer: The top 3 tracks...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_response with input: \n",
            "response_step: thought='The user has repeated the request for tracks from the artist AC/DC, but they have already been provided with this information in the previous observation.' response='The top 3 tracks by AC/DC...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_agent_response with input: \n",
            "response_dict: {'response_str': 'The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\'s Get It Up\".', 'is_done': True}\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "step_output = agent.run_step(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dffeec-651e-43e7-8929-827566c8f6da",
      "metadata": {
        "id": "22dffeec-651e-43e7-8929-827566c8f6da",
        "outputId": "d7a9c8eb-e6ff-4f60-e7f4-ffb9cdfab12e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "step_output.is_last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44372a37-0b1e-4d90-8f42-9816546cc66e",
      "metadata": {
        "id": "44372a37-0b1e-4d90-8f42-9816546cc66e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "response = agent.finalize_response(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba842bd-cc86-422b-8c17-2ff87c1962b6",
      "metadata": {
        "id": "7ba842bd-cc86-422b-8c17-2ff87c1962b6",
        "outputId": "20f142c7-9c2a-49f5-99e8-299936ee78bb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let's Get It Up\".\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ef125b-ccd7-4937-820a-e3b0a8b1f825",
      "metadata": {
        "id": "c6ef125b-ccd7-4937-820a-e3b0a8b1f825",
        "outputId": "4133db2e-360d-4f34-932b-e418e6aff277",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Running step 3ec42899-be2d-4bea-88ed-acc50867ccb7. Step input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
            "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
            "task: task_id='c4276071-d7f3-4be4-8604-556819744ef9' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
            "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output_parser with input: \n",
            "chat_response: assistant: Thought: I need to use a tool to help me answer the question.\n",
            "Action: sql_tool\n",
            "Action Input: {\"input\": \"Select track_name from tracks where artist_name = 'AC/DC' limit 3\"}\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module run_tool with input: \n",
            "reasoning_step: thought='I need to use a tool to help me answer the question.' action='sql_tool' action_input={'input': \"Select track_name from tracks where artist_name = 'AC/DC' limit 3\"}\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_agent_response with input: \n",
            "response_dict: {'response_str': 'Observation: {\\'output\\': ToolOutput(content=\\'The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\\\\\'s Get It Up\".\\', tool_nam...\n",
            "\n",
            "\u001b[0m> Running step 805657a8-0a40-4b21-b5a0-40836793ba2d. Step input: None\n",
            "\u001b[1;3;38;2;155;135;227m> Running module agent_input with input: \n",
            "state: {'sources': [], 'memory': ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all'), chat_store=SimpleChatSto...\n",
            "task: task_id='c4276071-d7f3-4be4-8604-556819744ef9' input='What are some tracks from the artist AC/DC? Limit it to 3' memory=ChatMemoryBuffer(token_limit=3000, tokenizer_fn=functools.partial(<bound method ...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_prompt with input: \n",
            "input: What are some tracks from the artist AC/DC? Limit it to 3\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
            "messages: [ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Too...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module react_output_parser with input: \n",
            "chat_response: assistant: Thought: The user has repeated the request, possibly not noticing the previous Observation which already contains the answer. I will restate the information provided in the Observation.\n",
            "\n",
            "An...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_response with input: \n",
            "response_step: thought='The user has repeated the request, possibly not noticing the previous Observation which already contains the answer. I will restate the information provided in the Observation.' response='The...\n",
            "\n",
            "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module process_agent_response with input: \n",
            "response_dict: {'response_str': 'The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let\\'s Get It Up\".', 'is_done': True}\n",
            "\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# run this e2e\n",
        "agent.reset()\n",
        "response = agent.chat(\n",
        "    \"What are some tracks from the artist AC/DC? Limit it to 3\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de05be63-319f-48e3-bc16-6155b23b13a0",
      "metadata": {
        "id": "de05be63-319f-48e3-bc16-6155b23b13a0",
        "outputId": "ce34acfd-7a02-4759-cef8-6e2eeceaf0be",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The top 3 tracks by AC/DC are \"For Those About To Rock (We Salute You)\", \"Put The Finger On You\", and \"Let's Get It Up\".\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ac702d9-6d44-4c5f-8dda-f3470fbbe1cf",
      "metadata": {
        "id": "5ac702d9-6d44-4c5f-8dda-f3470fbbe1cf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
