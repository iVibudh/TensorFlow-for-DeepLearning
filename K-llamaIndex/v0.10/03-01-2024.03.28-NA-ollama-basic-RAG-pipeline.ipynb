{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this when core llama-index functions do not work\n",
    "# !pip install llama-index --upgrade --no-cache-dir --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-embeddings-openai\n",
    "# %pip install llama-index-postprocessor-cohere-rerank\n",
    "# %pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "# from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is AI Assistant. I am here to help you with any questions or information you need. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"nous-hermes2\", \n",
    "             request_timeout=30.0)\n",
    "\n",
    "resp = llm.complete(\"Who are you? What is your name?\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Chunking the text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-vector-stores-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional for joining PDF pages if each page is treated as a seperate document\n",
    "\n",
    "\n",
    "# def concatenate_documents(documents):\n",
    "#     # Step 1: Create a list of unique file names\n",
    "#     unique_file_names = set(doc.metadata['file_name'] for doc in documents)\n",
    "\n",
    "#     # Step 2-5: Concatenate documents with the same file name\n",
    "#     concatenated_documents = []\n",
    "#     for file_name in unique_file_names:\n",
    "#         concatenated_text = ''\n",
    "#         initialized_doc = None\n",
    "#         for doc in documents:\n",
    "#             if doc.metadata['file_name'] == file_name:\n",
    "#                 if initialized_doc is None:\n",
    "#                     initialized_doc = doc\n",
    "#                 else:\n",
    "#                     initialized_doc.text += doc.text\n",
    "#         concatenated_documents.append(initialized_doc)\n",
    "\n",
    "#     return concatenated_documents\n",
    "\n",
    "\n",
    "# documents = concatenate_documents(documents_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextNode(id_='55c89e74-de07-469d-a3f8-0adc85a5b9e3', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\vibud\\\\Desktop\\\\Github\\\\TensorFlow-for-DeepLearning\\\\K-llamaIndex\\\\v0.10\\\\data\\\\HelloTest.txt', 'file_name': 'HelloTest.txt', 'file_type': 'text/plain', 'file_size': 14608, 'creation_date': '2024-03-27', 'last_modified_date': '2024-03-27'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3c3eafa1-da8b-43ff-a062-fdb7ddd0260c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'c:\\\\Users\\\\vibud\\\\Desktop\\\\Github\\\\TensorFlow-for-DeepLearning\\\\K-llamaIndex\\\\v0.10\\\\data\\\\HelloTest.txt', 'file_name': 'HelloTest.txt', 'file_type': 'text/plain', 'file_size': 14608, 'creation_date': '2024-03-27', 'last_modified_date': '2024-03-27'}, hash='e06bcd69db8d7a432df988c8f08523adbb236b9e85399b996f1c165f45b73eaf'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3e67f852-8608-479c-9993-dfefd4b936ce', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0e8676c33c71356db622658f268580c1c79e36ca929ab9913e35500ccec64fe5')}, text='Feb 12, 2024\\r\\n\\r\\nLlamaIndex v0.10\\r\\nLLM\\r\\nLlamaindex\\r\\nUpgrade\\r\\nAI\\r\\nToday we’re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (see this gargantuan PR), and it takes a massive step towards making LlamaIndex a next-generation, production-ready data framework for your LLM applications.\\r\\n\\r\\nLlamaIndex v0.10 contains some major updates:\\r\\n\\r\\nWe have created a llama-index-core package, and split all integrations and templates into separate packages: Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still usefrom llama_index.llms.openai import OpenAI for a LLM.\\r\\nLlamaHub will be the central hub for all integrations: the former llama-hub repo itself is consolidated into the main llama_index repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!\\r\\nServiceContext is deprecated: Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.\\r\\nUpgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we’ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.\\r\\n\\r\\nCheck out the below sections for more details, and go to the very last section for resource links to everything.\\r\\n\\r\\nSplitting into `llama-index-core` and integration packages\\r\\nThe first and biggest change we’ve made is a massive packaging refactor.\\r\\n\\r\\nLlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\\r\\n\\r\\n150+ data loaders\\r\\n35+ agent tools\\r\\n50+ LlamaPack templates\\r\\n50+ LLMs\\r\\n25+ embeddings\\r\\n40+ vector stores\\r\\nand more across the llama_index and llama-hub repos. The rapid growth of our ecosystem has been awesome to see, but it’s also come with growing pains:\\r\\n\\r\\nMany of the integrations lack proper tests\\r\\nUsers are responsible for figuring out dependencies\\r\\nIf an integration updates, users will have to update their entire llama-index Python package.\\r\\nIn response to this, we’ve done the following.\\r\\n\\r\\nCreated llama-index-core : This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.\\r\\nCreated separate packages for all integrations/templates: Every integration is now available as a separate package. This includes all integrations, including those on LlamaHub! See our Notion registry page for a full list of all packages.\\r\\nThe llama-index package still exists, and it imports llama-index-core and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (llama-index-llms-openai, llama-index-embeddings-openai, and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in llama-index-readers-file).\\r\\n\\r\\nNOTE: if you don’t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining llama-index-legacy (pinned to the latest release 0.9.48) for the foreseeable future.\\r\\n\\r\\nRevamped Folder Structure\\r\\nWe’ve completely revamped the folder structure in the llama_index repo. The most important folders you should care about are:\\r\\n\\r\\nllama-index-core : This folder contains all core LlamaIndex abstractions.\\r\\nllama-index-integrations: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.\\r\\nllama-index-packs : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user’s application.\\r\\nOther folders:\\r\\n\\r\\nllama-index-legacy : contains the legacy LlamaIndex code.\\r\\nllama-index-experimental : contains experimental features. Largely unused right now (outside parameter tuning).\\r\\nllama-index-finetuning : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.', start_char_idx=0, end_char_idx=4349, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "node_parser = SentenceSplitter(chunk_size=1000, chunk_overlap = 200)#factsheet_textract_1000\n",
    "\n",
    "#node_parser = TokenTextSplitter()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "print(len(nodes))\n",
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_date_from_text(text: str) -> str:\n",
    "#     date_pattern = r'\\b(AS OF ?:JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER) \\d{1,2}, \\d{4}\\b'\n",
    "#     match = re.search(date_pattern, text)\n",
    "#     try:\n",
    "#         if match:\n",
    "#             date_string = match.group(0)\n",
    "#             parsed_date = dateutil.parser.parse(date_string)\n",
    "#             formatted_date = parsed_date.strftime(\"%Y%m%d\")\n",
    "#             print(formatted_date)\n",
    "#             return formatted_date\n",
    "#         else:\n",
    "#             date_pattern = r'\\b(AS OF ?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}\\b'\n",
    "#             date_pattern = r'\\b(AS OF ?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b'\n",
    "#             match = re.search(date_pattern, text)\n",
    "#             date_string = match.group(0)\n",
    "#             parsed_date = dateutil.parser.parse(date_string)\n",
    "#             formatted_date = parsed_date.strftime(\"%Y%m%d\")\n",
    "#             #last_day = last_day_of_month(int(str(formatted_date)[0:4]), int(str(formatted_date)[4:]))\n",
    "#             #formatted_date = str(formatted_date)+str(last_day)\n",
    "#             print(formatted_date)\n",
    "#             return formatted_date\n",
    "#     except Exception as e:\n",
    "#         return '19990101'\n",
    "\n",
    "\n",
    "# for i, node in enumerate(nodes):\n",
    "#     \"\"\"Metadata:\n",
    "#     doc_name\n",
    "#     Company_Name\n",
    "#     DocType: Ammendment 1 or Original Contract\n",
    "#     chunk\n",
    "#     index\n",
    "#     \"\"\"\n",
    "#     doc_name = node.metadata['file_name'].split(\".\")[0]\n",
    "#     #node.excluded_llm_metadata_keys=('chunk')\n",
    "#     if \"LukkaAmd1\" in doc_name:\n",
    "#         node.metadata[\"Doc_Name\"] = doc_name\n",
    "#         node.metadata[\"Company_Name\"] = \"Lukka, Inc.\"\n",
    "#         node.metadata[\"DocType\"] = \"Ammendment 1\"\n",
    "#         node.metadata['chunk'] = i+1\n",
    "#         node.metadata['index'] = node.metadata['file_name'].split(\".\")[0]\n",
    "        \n",
    "#     elif \"Lukka_SPDJI_Data Master\" in doc_name:\n",
    "#         node.metadata[\"Doc_Name\"] = doc_name\n",
    "#         node.metadata[\"Company_Name\"] = \"Lukka, Inc.\"\n",
    "#         node.metadata[\"DocType\"] = \"Original Contract\"\n",
    "#         node.metadata['chunk'] = i+1\n",
    "#         node.metadata['index'] = node.metadata['file_name'].split(\".\")[0]\n",
    "\n",
    "#     # node.metadata['file_type'] = 'Finance'\n",
    "#     # node.metadata['chunk'] = i+1\n",
    "#     # node.metadata['index'] = node.metadata['file_name'].split(\".\")[0]\n",
    "#     # node.metadata['doc_name'] = node.metadata['file_name'].split(\".\")[0]\n",
    "#     #node.metadata['version'] = extract_date_from_text(node.text)\n",
    "#     #node.excluded_llm_metadata_keys=('chunk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating vectorStore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# from llama_index.index_store.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\vibud\\AppData\\Local\\Temp\\ipykernel_16532\\1138279434.py\", line 5, in <module>\n",
      "    chroma_client = chromadb.Client()\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\__init__.py\", line 274, in Client\n",
      "    return ClientCreator(tenant=tenant, database=database, settings=settings)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 143, in __init__\n",
      "    self._admin_client = AdminClient.from_system(self._system)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 495, in from_system\n",
      "    instance = cls(settings=system.settings)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 470, in __init__\n",
      "    self._server = self._system.instance(ServerAPI)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 366, in instance\n",
      "    self._instances = {}\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\segment.py\", line 103, in __init__\n",
      "    self._manager = self.require(SegmentManager)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 274, in require\n",
      "    self._dependencies = set()\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 366, in instance\n",
      "    self._instances = {}\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\segment\\impl\\manager\\local.py\", line 65, in __init__\n",
      "    if system.settings.chroma_segment_cache_policy == \"LRU\" and system.settings.chroma_memory_limit_bytes > 0:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Settings' object has no attribute 'chroma_segment_cache_policy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# create Chroma vector store\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_store = ChromaVectorStore(chroma_client)\n",
    "\n",
    "# create index using Chroma vector store\n",
    "index = VectorStoreIndex.from_documents(documents, vector_store=chroma_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\vibud\\AppData\\Local\\Temp\\ipykernel_16532\\2167038500.py\", line 2, in <module>\n",
      "    chroma_client = chromadb.Client()\n",
      "                    ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\__init__.py\", line 274, in Client\n",
      "    return ClientCreator(tenant=tenant, database=database, settings=settings)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 143, in __init__\n",
      "    self._admin_client = AdminClient.from_system(self._system)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 495, in from_system\n",
      "    instance = cls(settings=system.settings)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py\", line 470, in __init__\n",
      "    self._server = self._system.instance(ServerAPI)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 366, in instance\n",
      "    self._instances = {}\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\segment.py\", line 103, in __init__\n",
      "    self._manager = self.require(SegmentManager)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 274, in require\n",
      "    self._dependencies = set()\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py\", line 366, in instance\n",
      "    self._instances = {}\n",
      "               ^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\segment\\impl\\manager\\local.py\", line 65, in __init__\n",
      "    if system.settings.chroma_segment_cache_policy == \"LRU\" and system.settings.chroma_memory_limit_bytes > 0:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Settings' object has no attribute 'chroma_segment_cache_policy'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"c:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# create Chroma vector store\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_store = ChromaVectorStore(chroma_client)\n",
    "\n",
    "# create index using Chroma vector store\n",
    "index = VectorStoreIndex.from_documents(documents, vector_store=chroma_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Settings' object has no attribute 'chroma_segment_cache_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchromadb\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# create client and a new collection\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mEphemeralClient()\n\u001b[0;32m      4\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m chroma_client\u001b[38;5;241m.\u001b[39mcreate_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-K\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\__init__.py:119\u001b[0m, in \u001b[0;36mEphemeralClient\u001b[1;34m(settings, tenant, database)\u001b[0m\n\u001b[0;32m    113\u001b[0m     settings\u001b[38;5;241m.\u001b[39mis_persistent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ClientCreator(settings\u001b[38;5;241m=\u001b[39msettings, tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPersistentClient\u001b[39m(\n\u001b[1;32m--> 119\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    120\u001b[0m     settings: Optional[Settings] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    121\u001b[0m     tenant: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_TENANT,\n\u001b[0;32m    122\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[0;32m    123\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClientAPI:\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m    Creates a persistent instance of Chroma that saves to disk. This is useful for\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m    testing and development, but not recommended for production use.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m        database: The database to use for this client. Defaults to the default database.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m settings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py:143\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, tenant, database, settings)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase \u001b[38;5;241m=\u001b[39m database\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Create an admin client for verifying that databases and tenants exist\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_admin_client \u001b[38;5;241m=\u001b[39m AdminClient\u001b[38;5;241m.\u001b[39mfrom_system(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tenant_database(tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Get the root system component we want to interact with\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py:495\u001b[0m, in \u001b[0;36mAdminClient.from_system\u001b[1;34m(cls, system)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_system\u001b[39m(\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    492\u001b[0m     system: System,\n\u001b[0;32m    493\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdminClient\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    494\u001b[0m     SharedSystemClient\u001b[38;5;241m.\u001b[39m_populate_data_from_system(system)\n\u001b[1;32m--> 495\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(settings\u001b[38;5;241m=\u001b[39msystem\u001b[38;5;241m.\u001b[39msettings)\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m instance\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\client.py:470\u001b[0m, in \u001b[0;36mAdminClient.__init__\u001b[1;34m(self, settings)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, settings: Settings \u001b[38;5;241m=\u001b[39m Settings()) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(settings)\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system\u001b[38;5;241m.\u001b[39minstance(ServerAPI)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py:366\u001b[0m, in \u001b[0;36minstance\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    361\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_server_nofile is not supported on Windows. chroma_server_nofile will not be set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         )\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m=\u001b[39m settings\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instances \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\api\\segment.py:103\u001b[0m, in \u001b[0;36mSegmentAPI.__init__\u001b[1;34m(self, system)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings \u001b[38;5;241m=\u001b[39m system\u001b[38;5;241m.\u001b[39msettings\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire(SysDB)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire(SegmentManager)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quota \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire(QuotaEnforcer)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_product_telemetry_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire(ProductTelemetryClient)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py:274\u001b[0m, in \u001b[0;36mrequire\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, system: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSystem\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dependencies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system \u001b[38;5;241m=\u001b[39m system\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\config.py:366\u001b[0m, in \u001b[0;36minstance\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    361\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    362\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchroma_server_nofile is not supported on Windows. chroma_server_nofile will not be set.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         )\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings \u001b[38;5;241m=\u001b[39m settings\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instances \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\chromadb\\segment\\impl\\manager\\local.py:65\u001b[0m, in \u001b[0;36mLocalSegmentManager.__init__\u001b[1;34m(self, system)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instances \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_cache: Dict[SegmentScope, SegmentCache] \u001b[38;5;241m=\u001b[39m {SegmentScope\u001b[38;5;241m.\u001b[39mMETADATA: BasicCache()}\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m system\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mchroma_segment_cache_policy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLRU\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m system\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mchroma_memory_limit_bytes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_cache[SegmentScope\u001b[38;5;241m.\u001b[39mVECTOR] \u001b[38;5;241m=\u001b[39m SegmentLRUCache(capacity\u001b[38;5;241m=\u001b[39msystem\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mchroma_memory_limit_bytes,callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m k, v: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_cache_evict(v), size_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_segment_disk_size(k))\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Settings' object has no attribute 'chroma_segment_cache_policy'"
     ]
    }
   ],
   "source": [
    "import chromadb \n",
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"10-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Load the BGE-small-en-v1.5 model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chromadb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load from disk\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m db2 \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./chroma_db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m db2\u001b[38;5;241m.\u001b[39mget_or_create_collection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m ChromaVectorStore(chroma_collection\u001b[38;5;241m=\u001b[39mchroma_collection)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chromadb' is not defined"
     ]
    }
   ],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "chroma_collection = db2.get_or_create_collection(\"DB_collection\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "document = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(document))\n",
    "print(type(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core import (\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"storage\"):\n",
    "    index = VectorStoreIndex.from_documents(docs)\n",
    "    # save index to disk\n",
    "    index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(\"./storage\")\n",
    "else:\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context, index_id=\"vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown model 'local-model'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m prompt_tmpl \u001b[38;5;241m=\u001b[39m PromptTemplate(prompt_str)\n\u001b[0;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, \n\u001b[0;32m      9\u001b[0m             max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m p \u001b[38;5;241m=\u001b[39m QueryPipeline(chain\u001b[38;5;241m=\u001b[39m[prompt_tmpl, llm], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:182\u001b[0m, in \u001b[0;36mQueryPipeline.__init__\u001b[1;34m(self, callback_manager, chain, modules, links, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    171\u001b[0m     callback_manager: Optional[CallbackManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    176\u001b[0m ):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    178\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager \u001b[38;5;129;01mor\u001b[39;00m CallbackManager([]),\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    180\u001b[0m     )\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_graph(chain\u001b[38;5;241m=\u001b[39mchain, modules\u001b[38;5;241m=\u001b[39mmodules, links\u001b[38;5;241m=\u001b[39mlinks)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:194\u001b[0m, in \u001b[0;36mQueryPipeline._init_graph\u001b[1;34m(self, chain, modules, links)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m links \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both chain and modules/links in init.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_chain(chain)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_modules(modules)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:213\u001b[0m, in \u001b[0;36mQueryPipeline.add_chain\u001b[1;34m(self, chain)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, get_args(QUERY_COMPONENT_TYPE)):\n\u001b[0;32m    212\u001b[0m     module_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4())\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(module_key, cast(QUERY_COMPONENT_TYPE, module))\n\u001b[0;32m    214\u001b[0m     module_keys\u001b[38;5;241m.\u001b[39mappend(module_key)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:247\u001b[0m, in \u001b[0;36mQueryPipeline.add\u001b[1;34m(self, module_key, module)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists in pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, ChainableMixin):\n\u001b[1;32m--> 247\u001b[0m     module \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mas_query_component()\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:142\u001b[0m, in \u001b[0;36mChainableMixin.as_query_component\u001b[1;34m(self, partial, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_query_component\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m, partial: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryComponent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get query component.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     component \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_as_query_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     component\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(partial \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m component\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:282\u001b[0m, in \u001b[0;36mLLM._as_query_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_query_component\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m QueryComponent:\n\u001b[0;32m    281\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return query component.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mis_chat_model:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m LLMChatComponent(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:274\u001b[0m, in \u001b[0;36mOpenAI.metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMMetadata:\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMMetadata(\n\u001b[1;32m--> 274\u001b[0m         context_window\u001b[38;5;241m=\u001b[39mopenai_modelname_to_contextsize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_name()),\n\u001b[0;32m    275\u001b[0m         num_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    276\u001b[0m         is_chat_model\u001b[38;5;241m=\u001b[39mis_chat_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_name()),\n\u001b[0;32m    277\u001b[0m         is_function_calling_model\u001b[38;5;241m=\u001b[39mis_function_calling_model(\n\u001b[0;32m    278\u001b[0m             model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_name()\n\u001b[0;32m    279\u001b[0m         ),\n\u001b[0;32m    280\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    281\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\llms\\openai\\utils.py:203\u001b[0m, in \u001b[0;36mopenai_modelname_to_contextsize\u001b[1;34m(modelname)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAI model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been discontinued. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose another model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modelname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ALL_AVAILABLE_MODELS:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodelname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. Please provide a valid OpenAI model name in:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ALL_AVAILABLE_MODELS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ALL_AVAILABLE_MODELS[modelname]\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown model 'local-model'. Please provide a valid OpenAI model name in: gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_pipeline import QueryPipeline\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# try chaining basic prompts\n",
    "prompt_str = \"Please generate packages  related to {disease_name}\"\n",
    "prompt_tmpl = PromptTemplate(prompt_str)\n",
    "llm = OpenAI(model=\"local-model\",\n",
    "            temperature=0.7, \n",
    "            max_tokens=256)\n",
    "\n",
    "p = QueryPipeline(chain=[prompt_tmpl, llm], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module 6d204937-c259-41b3-8ae8-6bec0f71bfe6 with input: \n",
      "disease_name: Call back Manager\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 65e68ba9-a0c8-4461-bc92-c0a073ab3056 with input: \n",
      "messages: Please generate packages  related to Call back Manager\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.04620972861039607 seconds as it raised InternalServerError: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 1.54540937889328 seconds as it raised InternalServerError: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 3.5407211030882673 seconds as it raised InternalServerError: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 0.35979374943517595 seconds as it raised InternalServerError: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._chat in 3.2271254198093438 seconds as it raised InternalServerError: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}.\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mrun(disease_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall back Manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:319\u001b[0m, in \u001b[0;36mQueryPipeline.run\u001b[1;34m(self, return_values_direct, callback_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     query_payload \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mstr\u001b[39m(kwargs))\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    317\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_payload}\n\u001b[0;32m    318\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;241m*\u001b[39margs, return_values_direct\u001b[38;5;241m=\u001b[39mreturn_values_direct, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    321\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:442\u001b[0m, in \u001b[0;36mQueryPipeline._run\u001b[1;34m(self, return_values_direct, *args, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m root_key, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_root_key_and_kwargs(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m# call run_multi with one root key\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m result_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_multi({root_key: kwargs})\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_result_output(result_outputs, return_values_direct)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\query_pipeline\\query.py:544\u001b[0m, in \u001b[0;36mQueryPipeline._run_multi\u001b[1;34m(self, module_input_dict)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    543\u001b[0m     print_debug_input(module_key, module_input)\n\u001b[1;32m--> 544\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mrun_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_input)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# get new nodes and is_leaf\u001b[39;00m\n\u001b[0;32m    547\u001b[0m queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_component_output(\n\u001b[0;32m    548\u001b[0m     queue, output_dict, module_key, all_module_inputs, result_outputs\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\base\\query_pipeline\\query.py:199\u001b[0m, in \u001b[0;36mQueryComponent.run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_dict)\n\u001b[0;32m    198\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_inputs(kwargs)\n\u001b[1;32m--> 199\u001b[0m component_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_component(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_component_outputs(component_outputs)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\llms\\llm.py:736\u001b[0m, in \u001b[0;36mLLMChatComponent._run_component\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mstream_chat(messages)\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 736\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: response}\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:130\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[1;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    116\u001b[0m     LLMChatStartEvent(\n\u001b[0;32m    117\u001b[0m         model_dict\u001b[38;5;241m=\u001b[39m_self\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    122\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[0;32m    123\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[0;32m    124\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     },\n\u001b[0;32m    129\u001b[0m )\n\u001b[1;32m--> 130\u001b[0m f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f_return_val, Generator):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_gen\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResponseGen:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:289\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\llms\\openai\\base.py:355\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[1;34m(self, messages, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[0;32m    354\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[1;32m--> 355\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    356\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts,\n\u001b[0;32m    357\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    359\u001b[0m )\n\u001b[0;32m    360\u001b[0m openai_message \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m    361\u001b[0m message \u001b[38;5;241m=\u001b[39m from_openai_message(openai_message)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    669\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    670\u001b[0m             {\n\u001b[0;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    691\u001b[0m             },\n\u001b[0;32m    692\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    693\u001b[0m         ),\n\u001b[0;32m    694\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    695\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    696\u001b[0m         ),\n\u001b[0;32m    697\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    698\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    699\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    700\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1208\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1196\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1204\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1205\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1206\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1207\u001b[0m     )\n\u001b[1;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:897\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    890\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    895\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    898\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    899\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    902\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    903\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    974\u001b[0m         options,\n\u001b[0;32m    975\u001b[0m         cast_to,\n\u001b[0;32m    976\u001b[0m         retries,\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1027\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    974\u001b[0m         options,\n\u001b[0;32m    975\u001b[0m         cast_to,\n\u001b[0;32m    976\u001b[0m         retries,\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1027\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:973\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    972\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    974\u001b[0m         options,\n\u001b[0;32m    975\u001b[0m         cast_to,\n\u001b[0;32m    976\u001b[0m         retries,\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    978\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    979\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    980\u001b[0m     )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1021\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1022\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1023\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1024\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m   1025\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1026\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1027\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:988\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    985\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    987\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    991\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    992\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    995\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    996\u001b[0m )\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 500 - {'error': \"Model with key 'gpt-3.5-turbo' not loaded.\"}"
     ]
    }
   ],
   "source": [
    "output = p.run(disease_name=\"Call back Manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the global service context to use this embedding model\n",
    "# from llama_index import ServiceContext, set_global_service_context\n",
    "# service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "# set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = \"Not Needed\"\n",
    "# os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:1234/v1\"\n",
    "\n",
    "# llm = OpenAI(temperature=0.7, max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
