{"docstore/metadata": {"67365505-7a3a-4156-9bfc-78c910b42d30": {"doc_hash": "2fc4900744ed4dc18fe92aff4724ce3788f092ed474ddffc6527797c455a1010"}, "31452cf6-75a4-4cc3-a155-3455b6ce6279": {"doc_hash": "194968a1f591a17c81dc15e7a2c03d75c6ab1a9c65dd5c5164d4b804a1f26cea", "ref_doc_id": "67365505-7a3a-4156-9bfc-78c910b42d30"}, "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0": {"doc_hash": "7f0fd1b900b321c7373961beb463c6ba3720dafab187f31a6ccd677934d5f322", "ref_doc_id": "67365505-7a3a-4156-9bfc-78c910b42d30"}, "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9": {"doc_hash": "6d7d06fb547fcaed793f726a8b5750ac5c68d9993a2b1bd157e2f0a0dc1b7140", "ref_doc_id": "67365505-7a3a-4156-9bfc-78c910b42d30"}, "74633148-14ea-465f-8f9d-dff82c8156ba": {"doc_hash": "4d32f76700489b17ccc08048212a24d3cc347a32f5980f825593869803cf8c09", "ref_doc_id": "67365505-7a3a-4156-9bfc-78c910b42d30"}}, "docstore/data": {"31452cf6-75a4-4cc3-a155-3455b6ce6279": {"__data__": {"id_": "31452cf6-75a4-4cc3-a155-3455b6ce6279", "embedding": null, "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67365505-7a3a-4156-9bfc-78c910b42d30", "node_type": "4", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "2fc4900744ed4dc18fe92aff4724ce3788f092ed474ddffc6527797c455a1010", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0", "node_type": "1", "metadata": {}, "hash": "9922f366c48792ec599e8063e65f66c5398c7bbe9ede5874e2f752ceb8fc7fed", "class_name": "RelatedNodeInfo"}}, "text": "Feb 12, 2024\r\n\r\nLlamaIndex v0.10\r\nLLM\r\nLlamaindex\r\nUpgrade\r\nAI\r\nToday we\u2019re excited to launch LlamaIndex v0.10.0. It is by far the biggest update to our Python package to date (see this gargantuan PR), and it takes a massive step towards making LlamaIndex a next-generation, production-ready data framework for your LLM applications.\r\n\r\nLlamaIndex v0.10 contains some major updates:\r\n\r\nWe have created a llama-index-core package, and split all integrations and templates into separate packages: Hundreds of integrations (LLMs, embeddings, vector stores, data loaders, callbacks, agent tools, and more) are now versioned and packaged as a separate PyPI packages, while preserving namespace imports: for example, you can still usefrom llama_index.llms.openai import OpenAI for a LLM.\r\nLlamaHub will be the central hub for all integrations: the former llama-hub repo itself is consolidated into the main llama_index repo. Instead of integrations being split between the core library and LlamaHub, every integration will be listed on LlamaHub. We are actively working on updating the site, stay tuned!\r\nServiceContext is deprecated: Every LlamaIndex user is familiar with ServiceContext, which over time has become a clunky, unneeded abstraction for managing LLMs, embeddings, chunk sizes, callbacks, and more. As a result we are completely deprecating it; you can now either directly specify arguments or set a default.\r\nUpgrading your codebase to LlamaIndex v0.10 may lead to some breakages, primarily around our integrations/packaging changes, but fortunately we\u2019ve included some scripts to make it as easy as possible to migrate your codebase to use LlamaIndex v0.10.\r\n\r\nCheck out the below sections for more details, and go to the very last section for resource links to everything.\r\n\r\nSplitting into `llama-index-core` and integration packages\r\nThe first and biggest change we\u2019ve made is a massive packaging refactor.\r\n\r\nLlamaIndex has evolved into a broad toolkit containing hundreds of integrations:\r\n\r\n150+ data loaders\r\n35+ agent tools\r\n50+ LlamaPack templates\r\n50+ LLMs\r\n25+ embeddings\r\n40+ vector stores\r\nand more across the llama_index and llama-hub repos. The rapid growth of our ecosystem has been awesome to see, but it\u2019s also come with growing pains:\r\n\r\nMany of the integrations lack proper tests\r\nUsers are responsible for figuring out dependencies\r\nIf an integration updates, users will have to update their entire llama-index Python package.\r\nIn response to this, we\u2019ve done the following.\r\n\r\nCreated llama-index-core : This is a slimmed-down package that contains the core LlamaIndex abstractions and components, without any integrations.\r\nCreated separate packages for all integrations/templates: Every integration is now available as a separate package. This includes all integrations, including those on LlamaHub! See our Notion registry page for a full list of all packages.\r\nThe llama-index package still exists, and it imports llama-index-core and a minimal set of integrations. Since we use OpenAI by default, this includes OpenAI packages (llama-index-llms-openai, llama-index-embeddings-openai, and OpenAI programs/question generation/multimodal), as well as our beloved SimpleDirectoryReader (which is in llama-index-readers-file).\r\n\r\nNOTE: if you don\u2019t want to migrate to v0.10 yet and want to continue using the current LlamaIndex abstractions, we are maintaining llama-index-legacy (pinned to the latest release 0.9.48) for the foreseeable future.\r\n\r\nRevamped Folder Structure\r\nWe\u2019ve completely revamped the folder structure in the llama_index repo. The most important folders you should care about are:\r\n\r\nllama-index-core : This folder contains all core LlamaIndex abstractions.\r\nllama-index-integrations: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.\r\nllama-index-packs : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user\u2019s application.\r\nOther folders:\r\n\r\nllama-index-legacy : contains the legacy LlamaIndex code.\r\nllama-index-experimental : contains experimental features. Largely unused right now (outside parameter tuning).\r\nllama-index-finetuning : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.\r\nThe sub-directories in integrations and packs represent individual packages. The name of the folder corresponds to the package name.", "start_char_idx": 0, "end_char_idx": 4483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0": {"__data__": {"id_": "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0", "embedding": null, "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67365505-7a3a-4156-9bfc-78c910b42d30", "node_type": "4", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "2fc4900744ed4dc18fe92aff4724ce3788f092ed474ddffc6527797c455a1010", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31452cf6-75a4-4cc3-a155-3455b6ce6279", "node_type": "1", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "194968a1f591a17c81dc15e7a2c03d75c6ab1a9c65dd5c5164d4b804a1f26cea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9", "node_type": "1", "metadata": {}, "hash": "5822c1231dddbf7847f283ad59398ec4b48a540c13d4056e2d214d1d43f26949", "class_name": "RelatedNodeInfo"}}, "text": "The most important folders you should care about are:\r\n\r\nllama-index-core : This folder contains all core LlamaIndex abstractions.\r\nllama-index-integrations: This folder contains third-party integrations for 19 LlamaIndex abstractions. This includes data loaders, LLMs, embedding models, vector stores, and more. See below for more details.\r\nllama-index-packs : This folder contains our 50+ LlamaPacks, which are templates designed to kickstart a user\u2019s application.\r\nOther folders:\r\n\r\nllama-index-legacy : contains the legacy LlamaIndex code.\r\nllama-index-experimental : contains experimental features. Largely unused right now (outside parameter tuning).\r\nllama-index-finetuning : contains LlamaIndex fine-tuning abstractions. These are still relatively experimental.\r\nThe sub-directories in integrations and packs represent individual packages. The name of the folder corresponds to the package name. For instance, llama-index-integrations/llms/llama-index-llms-gemini corresponds to the llama-index-llms-gemini PyPI package.\r\n\r\nWithin each package folder, the source files are arranged in the same paths that you use to import them. For example, in the Gemini LLM package, you\u2019ll see a folder called llama_index/llms/gemini containing the source files. This folder structure is what allows you to preserve the top-level llama_index namespace during importing. In the case of Gemini LLM, you would pip install llama-index-llms-gemini and then import using from llama_index.llms.gemini import Gemini.\r\n\r\nEvery one of these subfolders also has the resources needed to packagify it: pyproject.toml, poetry.lock, and a Makefile , along with a script to automatically create a package.\r\n\r\nIf you\u2019re looking to contribute an integration or pack, don\u2019t worry! We have a full contributing guide designed to make this as seamless as possible, make sure to check it out.\r\n\r\nIntegrations\r\nAll third-party integrations are now under llama-index-integrations. There are 19 folders in here. The main integration categories are:\r\n\r\nllms\r\nembeddings\r\nmulti_modal_llms\r\nreaders\r\ntools\r\nvector_stores\r\nFor completeness here are all the other categories: agent, callbacks, evaluation, extractors, graph_stores, indices, output_parsers, postprocessor, program, question_gen, response_synthesizers, retrievers, storage, tools.\r\n\r\nThe integrations in the most common categories can be found in our temporary Notion package registry page. All integrations can be found in our Github repo. The folder name of each integration package corresponds to the name of the package \u2014 so if you find an integration you like, you now know how to pip install it!\r\n\r\nWe are actively working to make all integrations viewable on LlamaHub. Our vision for LlamaHub is to be the hub for all third-party integrations.\r\n\r\nIf you\u2019re interested in contributing a package, see our contributing section below!\r\n\r\nUsage Example\r\nHere is a simple example of installing and using an Anthropic LLM.\r\n\r\npip install llama-index-llms-anthropic\r\n\r\nfrom llama_index.llms.anthropic import Anthropic\r\nllm = Anthropic(api_key=\"&lt;api_key&gt;\")\r\nHere is an example of using a data loader.\r\n\r\npip install llama-index-readers-notion\r\n\r\nfrom llama_index.readers.notion import NotionPageReader\r\nintegration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\r\npage_ids = [\"&lt;page_id&gt;\"]\r\nreader = NotionPageReader(integration_token=integration_token)\r\ndocuments = reader.load_data(page_ids=page_ids)\r\nHere is an example of using a LlamaPack:\r\n\r\npip install llama-index-packs-sentence-window-retriever\r\n\r\nfrom llama_index.packs.sentence_window_retriever import SentenceWindowRetrieverPack\r\n# create the pack\r\n# get documents from any data loader\r\nsentence_window_retriever_pack = SentenceWindowRetrieverPack(\r\n  documents\r\n)\r\nresponse = sentence_window_retriever_pack.run(\"Tell me a bout a Music celebritiy.\")\r\nDealing with Breaking Changes\r\nThis update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:\r\n\r\n# no more using `llama_index.llms` as a top-level package\r\nfrom llama_index.llms import OpenAI\r\n# no more using `llama_index.vector_stores` as a top-level package\r\nfrom llama_index.vector_stores import PineconeVectorStore\r\n# llama_hub imports are now no longer supported.", "start_char_idx": 3580, "end_char_idx": 7854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9": {"__data__": {"id_": "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9", "embedding": null, "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67365505-7a3a-4156-9bfc-78c910b42d30", "node_type": "4", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "2fc4900744ed4dc18fe92aff4724ce3788f092ed474ddffc6527797c455a1010", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0", "node_type": "1", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "7f0fd1b900b321c7373961beb463c6ba3720dafab187f31a6ccd677934d5f322", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74633148-14ea-465f-8f9d-dff82c8156ba", "node_type": "1", "metadata": {}, "hash": "2ffd2ff8792fa07b58c63e3d956b8c160a2534deedb4c747b4e6ea8585f9c735", "class_name": "RelatedNodeInfo"}}, "text": "Dealing with Breaking Changes\r\nThis update comes with breaking changes, mostly around imports. For all integrations, you can no longer do any of these:\r\n\r\n# no more using `llama_index.llms` as a top-level package\r\nfrom llama_index.llms import OpenAI\r\n# no more using `llama_index.vector_stores` as a top-level package\r\nfrom llama_index.vector_stores import PineconeVectorStore\r\n# llama_hub imports are now no longer supported.\r\nfrom llama_hub.slack.base import SlackReader\r\nInstead you can do these:\r\n\r\nfrom llama_index.llms.openai import OpenAI\r\nfrom llama_index.vector_stores.pinecone import PineconeVectorStore\r\n# NOTE: no longer import a separate llama_hub package\r\nfrom llama_index.readers.slack import SlackReader\r\nSee our migration guide (also described below) for more details.\r\n\r\nLlamaHub as a Central Hub for Integrations\r\nWith these packaging updates, we\u2019re expanding the concept of LlamaHub to become a central hub of all LlamaIndex integrations to fulfill its vision of becoming an integration site at the center of the LLM ecosystem. This expands beyond its existing domain of loaders, tools, packs, and datasets, to include LLMs, embeddings, vector stores, callbacks, and more.\r\n\r\nThis effort is still a WIP. If you go to llamahub.ai today, you\u2019ll see that the site has not been updated yet, and it still contains the current set of integrations (data loaders, tools, LlamaPacks, datasets). Rest assured we\u2019ll be updating the site in a few weeks; in the meantime check out our Notion package registry / repo for a list of all integrations/packages.\r\n\r\nSunsetting llama-hub repo\r\nSince all integrations have been moved to the llama_index repo, we are sunsetting the llama-hub repo (but LlamaHub itself lives on!). We did the painstaking work of migrating and packaging all existing llama-hub integrations. For all future contributions please submit directly to the llama_index repo!\r\n\r\n`download` syntax\r\nA popular UX for fetching integrations through LlamaHub has been the download syntax: download_loader , download_llama_pack , and more.\r\n\r\nThis will still work, but have different behavior. Check out the details below:\r\n\r\ndownload_llama_pack : Will download a pack under llama-index-packs to a local file on your disk. This allows you to directly use and modify the source code from the template.\r\nEvery other download function download_loader , download_tool : This will directly run pip install on the relevant integration package.\r\nDeprecating ServiceContext\r\nLast but not least, we are deprecating our ServiceContext construct and as a result improving the developer experience of LlamaIndex.\r\n\r\nOur ServiceContext object existed as a general configuration container containing an LLM, embedding model, callback, and more; it was created before we had proper LLM, embedding, prompt abstractions and was meant to be an intermediate user-facing layer to let users define these parameters.\r\n\r\nOver time however, this object became increasingly difficult to use. Passing in an entire service_context container to any module (index, retriever, post-processor, etc.) made it hard to reason about which component was actually getting used. Since all modules use OpenAI by default, users were getting asked to unnecessarily specify their OpenAI key even in cases where they\u2019d want to use a local model (because the embedding model default was still OpenAI). It was also laborious to import and type out.\r\n\r\nAnother related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the service_context to all modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.\r\n\r\nTherefore we\u2019ve made the following changes:\r\n\r\nServiceContext is now deprecated: You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.\r\nYou can now define global settings: Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.\r\nAll references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.", "start_char_idx": 7428, "end_char_idx": 11723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74633148-14ea-465f-8f9d-dff82c8156ba": {"__data__": {"id_": "74633148-14ea-465f-8f9d-dff82c8156ba", "embedding": null, "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67365505-7a3a-4156-9bfc-78c910b42d30", "node_type": "4", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "2fc4900744ed4dc18fe92aff4724ce3788f092ed474ddffc6527797c455a1010", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9", "node_type": "1", "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}, "hash": "6d7d06fb547fcaed793f726a8b5750ac5c68d9993a2b1bd157e2f0a0dc1b7140", "class_name": "RelatedNodeInfo"}}, "text": "It was also laborious to import and type out.\r\n\r\nAnother related pain point was that if you had a custom model or especially a custom callback, you had to manually pass in the service_context to all modules. This was laborious and it was easy for users to forget, resulting in missed callbacks or inconsistent model usage.\r\n\r\nTherefore we\u2019ve made the following changes:\r\n\r\nServiceContext is now deprecated: You should now directly pass in relevant parameters to modules, such as the embedding model for indexing and the LLM for querying/response synthesis.\r\nYou can now define global settings: Define this once, and don\u2019t worry about specifying any custom parameters at all in your downstream code. This is especially useful for callbacks.\r\nAll references to ServiceContext in our docs/notebooks have been removed and changed to use either direct modules or the global settings object. See our usage example below as well.\r\n\r\nUsage Example\r\nTo build a VectorStoreIndex and then query it, you can now pass in the embedding model and LLM directly\r\n\r\nfrom llama_index.embeddings.openai import OpenAIEmbedding\r\nfrom llama_index.llms.openai import OpenAI\r\nfrom llama_index.core.callbacks import CallbackManager\r\n\r\nembed_model = OpenAIEmbedding()\r\nllm = OpenAI()\r\ncallback_manager = CallbackManager()\r\nindex = VectorStoreIndex.from_documents(\r\n documents, embed_model=embed_model, callback_manager=callback_manager\r\n)\r\nquery_engine = index.as_query_engine(llm=llm)\r\nOr you can define a global settings object\r\n\r\nfrom llama_index.core.settings import Settings\r\nSettings.llm = llm\r\nSettings.embed_model = embed_model\r\nSettings.callback_manager = callback_manager\r\nindex = VectorStoreIndex.from_documents(documents)\r\nquery_engine = index.as_query_engine()\r\nContributing to LlamaIndex v0.10\r\nv0.10 makes the llama_index repo the central place for all community contributions, whether you are interested in contributing core refactors, or integrations/packs!\r\n\r\nIf you\u2019re contributing an integration/pack, v0.10 makes it way easier for you to contribute something that can be independently versioned, tested, and packaged.\r\n\r\nWe have utility scripts to make the package creation process for an integration or pack effortless:\r\n\r\n# create a new pack\r\ncd ./llama-index-packs\r\nllamaindex-cli new-package --kind \"packs\" --name \"my new pack\"\r\n\r\n# create a new integration\r\ncd ./llama-index-integrations/readers\r\nllamaindex-cli new-pacakge --kind \"readers\" --name \"new reader\"\r\nTake a look at our updated contributing guide here for more details.\r\n\r\nMigration to v0.10\r\nIf you want to use LlamaIndex v0.10, you will need to do two main things:\r\n\r\nAdjust imports to fit the new package structure for core modules/integrations\r\nDeprecate ServiceContext\r\nLuckily, we\u2019ve created a comprehensive migration guide that also contains a CLI tool to automatically upgrade your existing code and notebooks to v0.10!\r\n\r\nJust do\r\n\r\nllamaindex-cli upgrade <source-dir>\r\nCheck out the full migration guide here.\r\n\r\nNext Steps\r\nWe\u2019ve painstakingly revamped all of our README, documentation and notebooks to reflect these v0.10 changes. Check out the below section for a compiled list of all resources.\r\n\r\nDocumentation\r\nv0.10 Documentation\r\n\r\nv0.10 Installation Guide\r\n\r\nv0.10 Quickstart\r\n\r\nUpdated Contribution Guide\r\n\r\nTemporary v0.10 Package Registry\r\n\r\nv0.10 Migration Guide\r\n\r\nRepo\r\nRepo README\r\n\r\nllama-index-integrations\r\n\r\nllama-index-packs\r\n\r\nExample Notebooks\r\nThese are mostly to show our updated import syntax.\r\n\r\nSub-Question Query Engine (primarily uses core)\r\nWeaviate Vector Store Demo\r\nOpenAI Agent over RAG Pipelines\r\nBug reports\r\nWe\u2019ll be actively monitoring our Github Issues and Discord. If you run into any issues don\u2019t hesitate to hop into either of these channels!", "start_char_idx": 10801, "end_char_idx": 14556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"67365505-7a3a-4156-9bfc-78c910b42d30": {"node_ids": ["31452cf6-75a4-4cc3-a155-3455b6ce6279", "a3422e0d-bf35-41cc-aec5-32d8ba6ee8d0", "37da6e0b-827c-47bb-a14e-b7dc1ad9ccf9", "74633148-14ea-465f-8f9d-dff82c8156ba"], "metadata": {"file_path": "data\\HelloTest.txt", "file_name": "HelloTest.txt", "file_type": "text/plain", "file_size": 14608, "creation_date": "2024-03-27", "last_modified_date": "2024-03-27"}}}}