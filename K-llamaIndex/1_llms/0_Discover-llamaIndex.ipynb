{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover llamaindex -1\n",
    "\n",
    "#### 1. Low Level Usage \n",
    "\n",
    "##### Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Original code\n",
    "# from llama_index.llms import OpenAI\n",
    "\n",
    "# Ilm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\", max_tokens=256)\n",
    "# response = Ilm.complete(\"Tell me a joke!\")\n",
    "\n",
    "# print(response.text)\n",
    "# > \"Sure, here's a classic one for you: Why don't scientists trust atoms? Because they make\n",
    "# up everything!\n",
    "\n",
    "# print(response.raw)\n",
    "# > \"(raw openai JSON response)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n",
      "{'id': 'chatcmpl-cdfmss2bin5xyz1n3quf7e', 'choices': [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Why don't scientists trust atoms?\\nBecause they make up everything!\", role='assistant', function_call=None, tool_calls=None))], 'created': 1707503180, 'model': 'D:\\\\LLM-Studio-LLM-models\\\\MaziyarPanahi\\\\SciPhi-Mistral-7B-32k-Mistral-7B-Instruct-v0.2-slerp-GGUF\\\\SciPhi-Mistral-7B-32k-Mistral-7B-Instruct-v0.2-slerp.Q8_0.gguf', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=15, prompt_tokens=15, total_tokens=30)}\n"
     ]
    }
   ],
   "source": [
    "## Modified for LM Studio API\n",
    "from llama_index.llms import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Not Needed\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:1234/v1\"\n",
    "\n",
    "Ilm = OpenAI(temperature=0, \n",
    "            # model=\"gpt-3.5-turbo\", \n",
    "            max_tokens=256)\n",
    "response = Ilm.complete(\"Tell me a joke!\")\n",
    "\n",
    "print(response.text)\n",
    "# the joke\n",
    "\n",
    "print(response.raw)\n",
    "# JSON response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## Original Code\n",
    "# from llama_index.llms import OpenAI, ChatMessage\n",
    "\n",
    "# llm = OpenAI(temperature=0, \n",
    "#             # model=\"gpt-3.5-turbo\", \n",
    "#             max_tokens=256)\n",
    "# messages = [\n",
    "#     ChatMessage(role=\"user\", content=\"Tell me a joke!\"),\n",
    "#     ChatMessage(role=\"system\", content=\"Talk like a pirate in responses.\")\n",
    "# ]\n",
    "\n",
    "# response = llm.chat(messages)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Arrrr, here be a good one for ye, matey! Why did the chicken cross the road? To get to the other side, of course! But if ye be lookin' fer a more swashbucklin' tale, how about this one: A pirate walks into a bar with a parrot on his shoulder. The bartender says, \"Get outta here, ye scurvy dog! We don't allow no feathered friends in here!\" So the parrot says, \"Arrr, I'll have a rum and coke then, matey!\" The bartender, surprised, serves the parrot his drink. As the pirate is about to take a sip from his own mug, the parrot cries out, \"Aye, but what about me captain?\" The bartender looks at the pirate and says, \"Ye're not the one talkin', ye scallywag!\" Arr, that be a good one, innit? Avast there!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI, ChatMessage\n",
    "\n",
    "llm = OpenAI(temperature=0, \n",
    "            # model=\"gpt-3.5-turbo\", \n",
    "            max_tokens=256)\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a joke!\"),\n",
    "    ChatMessage(role=\"system\", content=\"Talk like a pirate in responses.\")\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "# print(response.text)\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='\\n\\nArrrr, here be a good one for ye, matey! Why did the chicken cross the road? To get to the other side, of course! But if ye be lookin\\' fer a more swashbucklin\\' tale, how about this one: A pirate walks into a bar with a parrot on his shoulder. The bartender says, \"Get outta here, ye scurvy dog! We don\\'t allow no feathered friends in here!\" So the parrot says, \"Arrr, I\\'ll have a rum and coke then, matey!\" The bartender, surprised, serves the parrot his drink. As the pirate is about to take a sip from his own mug, the parrot cries out, \"Aye, but what about me captain?\" The bartender looks at the pirate and says, \"Ye\\'re not the one talkin\\', ye scallywag!\" Arr, that be a good one, innit? Avast there!', additional_kwargs={}), raw={'id': 'chatcmpl-ez4yhiwwa4rzslf9s05lp', 'choices': [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='\\n\\nArrrr, here be a good one for ye, matey! Why did the chicken cross the road? To get to the other side, of course! But if ye be lookin\\' fer a more swashbucklin\\' tale, how about this one: A pirate walks into a bar with a parrot on his shoulder. The bartender says, \"Get outta here, ye scurvy dog! We don\\'t allow no feathered friends in here!\" So the parrot says, \"Arrr, I\\'ll have a rum and coke then, matey!\" The bartender, surprised, serves the parrot his drink. As the pirate is about to take a sip from his own mug, the parrot cries out, \"Aye, but what about me captain?\" The bartender looks at the pirate and says, \"Ye\\'re not the one talkin\\', ye scallywag!\" Arr, that be a good one, innit? Avast there!', role='assistant', function_call=None, tool_calls=None))], 'created': 1707503914, 'model': 'D:\\\\LLM-Studio-LLM-models\\\\MaziyarPanahi\\\\SciPhi-Mistral-7B-32k-Mistral-7B-Instruct-v0.2-slerp-GGUF\\\\SciPhi-Mistral-7B-32k-Mistral-7B-Instruct-v0.2-slerp.Q8_0.gguf', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=218, prompt_tokens=219, total_tokens=437)}, delta=None, additional_kwargs={})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, here be a good one for ye, matey! Why did the parrot join the pirate crew? Because he wanted to be where the action was at! Arrghhh!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import OpenAI, ChatMessage\n",
    "\n",
    "llm = OpenAI(temperature=0, \n",
    "            # model=\"gpt-3.5-turbo\", \n",
    "            max_tokens=256)\n",
    "messages = [\n",
    "    ChatMessage(role=\"system\", content=\"Talk like a pirate in responses.\"),\n",
    "    ChatMessage(role=\"user\", content=\"Tell me a joke!\")\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "# print(response.text)\n",
    "print(response.message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
