{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - LLMs and Prompts\n",
    "This notebook walks through testing an LLM using the primary prompt templates used in llama-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# openai.api_key = \"YOUR_API_KEY\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Not Needed\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:1234/v1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In this section, we load a test document, create an LLM, and copy prompts from llama-index to test with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load a quick document to test with. Right now, we will just load it as plain text, but we can do other operations later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../docs/getting_started/starter_example.md\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# llm = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex uses some simple templates under the hood for answering queries -- mainly a `text_qa_template` for obtaining initial answers, and a `refine_template` for refining an existing answer when all the text does not fit into one LLM call.\n",
    "\n",
    "Let's copy the default templates, and test out our LLM with a few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Prompt\n",
    "\n",
    "text_qa_template = Prompt(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "refine_template = Prompt(\n",
    "    \"We have the opportunity to refine the original answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original answer to better \"\n",
    "    \"answer the question: {query_str}. \"\n",
    "    \"If the context isn't useful, output the original answer again.\\n\"\n",
    "    \"Original Answer: {existing_answer}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets test a few questions!\n",
    "\n",
    "## Text QA Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install llama-index, you need to follow these steps:\n",
      "1. Clone the repository using `git clone https://github.com/jerryjliu/llama_index.git`.\n",
      "2. Navigate to the cloned directory using `cd llama_index`.\n",
      "3. Install the dependencies by running `pip install -r requirements.txt` or `poetry install`.\n",
      "4. You can now use llama-index in your Python scripts or Jupyter notebooks by importing it as `from llama_index import *`.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I install llama-index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, follow these steps:\n",
      "\n",
      "1. First, make sure you have installed LlamaIndex by following the [installation](installation.md) instructions.\n",
      "2. Navigate to the `examples` folder of the LlamaIndex repository and open the desired example directory in your terminal or command prompt. For instance, if you want to use the Paul Graham essay example, go to the `examples/paul_graham_essay` directory.\n",
      "3. Create a new Python file (e.g., `create_index.py`) and import the necessary modules:\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "```\n",
      "4. Load the documents you want to index using `SimpleDirectoryReader`. In this case, it's assumed that the documents are in a `data` folder within the example directory:\n",
      "```python\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "```\n",
      "5. Create an instance of `VectorStoreIndex` from the loaded documents:\n",
      "```python\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index?\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To create an index using LlamaIndex, follow these steps:\n",
      "\n",
      "1. Import the necessary modules:\n",
      "```python\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "```\n",
      "\n",
      "2. Load the documents into memory:\n",
      "```python\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "```\n",
      "Replace `'data'` with the path to your data folder.\n",
      "\n",
      "3. Create a new index from the loaded documents:\n",
      "```python\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "```\n",
      "\n",
      "4. (Optional) Save the index to disk for later use:\n",
      "```python\n",
      "index.storage_context.persist()\n",
      "```\n",
      "\n",
      "5. To load an index from disk, first create a new `StorageContext` and then load the index using `load_index_from_storage`:\n",
      "```python\n",
      "from llama_index import StorageContext, load_index_from_storage\n",
      "\n",
      "# rebuild storage context\n",
      "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
      "# load index\n",
      "index = load_index_from_"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
    "response_gen = llm.stream_complete(prompt)\n",
    "for response in response_gen:\n",
    "    print(response.delta, end=\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Template Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create an index using LlamaIndex, follow these steps in a Python file:\n",
      "\n",
      "```python\n",
      "# Import required modules\n",
      "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
      "\n",
      "# Load documents from data folder\n",
      "documents = SimpleDirectoryReader('data').load_data()\n",
      "\n",
      "# Build the index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "# Persist the index to disk (optional)\n",
      "index.storage_context.persist()\n",
      "```\n",
      "\n",
      "This code snippet demonstrates how to create an index using LlamaIndex by loading documents from a `data` folder and building the index with `VectorStoreIndex.from_documents(documents)`. If you want to persist the index to disk, use `index.storage_context.persist()`. To reload the index from disk, use the following code:\n",
      "\n",
      "```python\n",
      "# Rebuild storage context and load index\n",
      "from llama_index import StorageContext, load_index_from_storage\n",
      "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
      "index = load_index_from_storage(storage\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I create an index? Write your answer using only code.\"\n",
    "existing_answer = \"\"\"To create an index using LlamaIndex, you need to follow these steps:\n",
    "\n",
    "1. Download the LlamaIndex repository by cloning it from GitHub.\n",
    "2. Navigate to the `examples/paul_graham_essay` folder in the cloned repository.\n",
    "3. Create a new Python file and import the necessary modules: `VectorStoreIndex` and `SimpleDirectoryReader`.\n",
    "4. Load the documents from the `data` folder using `SimpleDirectoryReader('data').load_data()`.\n",
    "5. Build the index using `VectorStoreIndex.from_documents(documents)`.\n",
    "6. To persist the index to disk, use `index.storage_context.persist()`.\n",
    "7. To reload the index from disk, use the `StorageContext` and `load_index_from_storage` functions.\n",
    "\n",
    "Note: This answer assumes that you have already installed LlamaIndex and have the necessary dependencies.\"\"\"\n",
    "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
    "response = llm.complete(prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Example\n",
    "The LLM also has a `chat` method that takes in a list of messages, to simulate a chat session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant:  To create an index using Llama Index, you'll need to follow these steps:\n",
      "\n",
      "1. Install the required packages: First, make sure you have Python and the necessary libraries installed on your system. You can install Llama Index by running `pip install llama-index` in your terminal or command prompt.\n",
      "\n",
      "2. Import the required modules: In your Python script or Jupyter Notebook, import the `GPTSimpleVectorIndex` class from the `llama_index` package. You can do this with the following line of code: `from llama_index import GPTSimpleVectorIndex`.\n",
      "\n",
      "3. Prepare your data: Llama Index works with text documents or web pages, so you'll need to convert your data into a format that it can understand. This usually involves tokenizing and encoding the text using a language model like OpenAI's GPT-2 or GPT-Neo. You can use the `LLMIndex` class from the `llama_index` package to create an index from a list of documents, where each document is a string or a dictionary with a 'text' key.\n",
      "\n",
      "4. Initialize the index: Create\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import ChatMessage\n",
    "\n",
    "chat_history = [\n",
    "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about llama-index.\"),\n",
    "    ChatMessage(role=\"user\", content=\"How do I create an index?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(chat_history)\n",
    "print(response.message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we covered the low-level LLM API, and tested out some basic prompts with out documentation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
