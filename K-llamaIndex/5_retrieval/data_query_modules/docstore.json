{"docstore/data": {"4d3efd15-341c-4b29-afa7-7953188d2938": {"__data__": {"id_": "4d3efd15-341c-4b29-afa7-7953188d2938", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "3": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "5": [{"node_id": "ed55ac15-ce09-44f2-8794-eb6f30ad4102", "node_type": null, "metadata": {}, "hash": "4262e11df5c3bbed8251c990f57e24075fea68dec6c8db7b8c6b5319d0269284"}, {"node_id": "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "node_type": null, "metadata": {}, "hash": "77f96d1de5680edd6c50e02c17bcb4d73aec29caf811c0fceff1d78500341caf"}, {"node_id": "7606d974-a75a-40e4-b749-ceee37dbf1b5", "node_type": null, "metadata": {}, "hash": "f994cfd53ce54f093d632d29c3995f2e5a7cb76c42c6249fa8ec43feb26fd0ee"}, {"node_id": "0ecd70b0-69e8-4c8d-bbde-4018da0126cb", "node_type": null, "metadata": {}, "hash": "16dc20ceedccee484801f6cd6331c075d63d21ef38feba760930abe8c6723f57"}]}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2", "text": "File Name: ../docs/core_modules/query_modules/chat_engines/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!More specifically, the `SimpleChatEngine` does not make use of a knowledge base, \nwhereas `CondenseQuestionChatEngine` and `ReActChatEngine` make use of a query engine over knowledge base.File Name: ../docs/core_modules/query_modules/chat_engines/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\n---\nmaxdepth: 1\n---\nSimple Chat Engine </examples/chat_engine/chat_engine_repl.ipynb>\nReAct Chat Engine </examples/chat_engine/chat_engine_react.ipynb>\nOpenAI Chat Engine </examples/chat_engine/chat_engine_openai.ipynb>\nCondense Question Chat Engine </examples/chat_engine/chat_engine_condense_question.ipynb>\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks:\n\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).Think ChatGPT, but augmented with your knowledge base.Conceptually, it is a **stateful** analogy of a Query Engine.By keeping track of the conversation history, it can answer questions with past context in mind.File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks:\n\nIf you want to ask standalone question over your data (i.e.without keeping track of conversation history), use Query Engine instead.File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nTo stream response:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")for token in streaming_response.response_gen:\n    print(token, end=\"\")\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: code\nHeader Path: Chat Engine/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks:\n\nBelow you can find corresponding tutorials to see the available chat engines in action.File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks:\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nBuild a chat engine from index:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine = index.as_chat_engine()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nHave a conversation with your data:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nresponse = chat_engine.chat(\"Tell me a joke.\")File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nReset chat history to start a new conversation:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine.reset()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nEnter an interactive chat REPL:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine.chat_repl()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine\nLinks:\n\nConfiguring a chat engine is very similar to configuring a query engine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6113854b-aea9-4310-bc89-6bf385c91deb": {"__data__": {"id_": "6113854b-aea9-4310-bc89-6bf385c91deb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}, "3": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "5": [{"node_id": "f4bd15e2-598d-4e37-ae15-6e03a9f08398", "node_type": null, "metadata": {}, "hash": "f997d531bcd63c9f023e50a984d05cab88c8796412bc8c6bc991a5335aea856e"}, {"node_id": "fb970878-52f4-4123-af7f-ef6a948abf39", "node_type": null, "metadata": {}, "hash": "c6253458d24ad71db59bdcd0495916aa2f8a261ef905f899e1ca3461c1c192cc"}, {"node_id": "051bf54d-d207-473e-a13b-3a0977d00377", "node_type": null, "metadata": {}, "hash": "48122b3e6a2098d31381010c56fa07d7b775eef7f1ff5ab516fcd7d8fb43e3b2"}, {"node_id": "fbf04091-9814-4c59-a2af-59cf4442e28a", "node_type": null, "metadata": {}, "hash": "e6dc2d9b6f90a2e165bf9b33f012c19e41a3164d99c104336abf3b6d508b6b18"}]}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19", "text": "File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\nYou can directly build and configure a chat engine from an index in 1 line of code:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\nchat_engine = index.as_chat_engine(\n    chat_mode='condense_question', \n    verbose=True\n)\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg.`condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`.> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.Concretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(...)`.> Note: You may need to look at API references or example notebooks.Here's an example where we configure the following:\n* configure the condense question prompt, \n* initialize the conversation with some existing history,\n* print verbose debug message.File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks:\n\nfrom llama_index.prompts  import Prompt\n\ncustom_prompt = Prompt(\"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.<Chat History> \n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\")\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/list of (human_message, ai_message) tuples\nLinks:\n\ncustom_chat_history = [\n    (\n        'Hello assistant, we are having a insightful discussion about Paul Graham today.', \n        'Okay, sounds good.')\n]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine, \n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True\n)\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks:\n\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks:\n\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag).We are working on making the behavior more consistent!File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.\")for token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\n\nSee an end-to-end tutorial\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks:\n\nUsed to remove nodes that are below a similarity score threshold.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks:\n\nUsed to ensure certain keywords are either excluded or included.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n  required_keywords=[\"word1\", \"word2\"],\n  exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43aa67f4-1213-4cfd-87bf-82baa0a7a21a": {"__data__": {"id_": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "3": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "5": [{"node_id": "9a424f32-a874-4f8e-ab1c-991d3a9dbe49", "node_type": null, "metadata": {}, "hash": "33315f4fb092f447a63428c4de8c9426c04b9d14b9066451bb8c349bb8aed600"}, {"node_id": "897ceee3-7af9-4300-a996-7069853dd3fe", "node_type": null, "metadata": {}, "hash": "415da5caa22124dcc721dee78617a51e585f7fe8dece2256478f05ea40b355e5"}, {"node_id": "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "node_type": null, "metadata": {}, "hash": "36738bb23aa0fcc746d254c0e39e574306470f8125f936803db728adca78853a"}, {"node_id": "b42d48c1-08ff-42a5-bdb4-9dd6f364a027", "node_type": null, "metadata": {}, "hash": "40e16441ab13ac84a903aa068d336b090e273e96b024890e4b0050529b834cab"}]}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819", "text": "The percentile cutoff is a measure for using the top percentage of relevant sentences.The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nfrom llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n  embed_model=service_context.embed_model,\n  percentile_cutoff=0.5,\n  # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nA full notebook guide can be found here\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nfrom llama_index.indices.postprocessor import CohereRerank\n\npostprocessor = CohereRerank(\n  top_n=2\n  model=\"rerank-english-v2.0\",\n  api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nFull notebook guide is available here.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are.Returns the top N ranked nodes.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nfrom llama_index.indices.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(\n  top_n=2\n  service_context=service_context,\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nFull notebook guide is available her for Gatsby and here for Lyft 10K documents.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes sorted by date.This assumes there is a `date` field to parse in the metadata of each node.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n  tok_k=1,\n  date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\n!A full notebook guide is available here.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n  service_context=service_context,\n  date_key=\"date\",\n  similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nA full notebook guide is available here.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node.Each time a node is retrieved, the time it was retrieved is recorded.This biases search to favor information that has not be returned in a query yet.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(\n  time_decay=0.99,\n  top_k=1\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nA full notebook guide is available here.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6dd66f92-57c2-4089-9f14-5d673d36d762": {"__data__": {"id_": "6dd66f92-57c2-4089-9f14-5d673d36d762", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "3": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "5": [{"node_id": "a4892a1f-2218-4bfe-b691-4162b1d4bf80", "node_type": null, "metadata": {}, "hash": "bbfc9e7516b4ad407a62d38a5451f7df026faabb8a4734f99b0f297b1e81a75e"}, {"node_id": "e043659e-e9a0-4c87-8095-875bfdf7221a", "node_type": null, "metadata": {}, "hash": "1cfa1181e601b4943de427f459dfd2ef90dd03581280ef97cff71cb4a0481ba2"}, {"node_id": "122d9186-a52b-4223-b806-d42a8563c93e", "node_type": null, "metadata": {}, "hash": "b89bd911f9c9274d05b41661c2651814c70623c370cecc0469df24111adebeaf"}, {"node_id": "9030c3b5-ba90-48a4-99dd-08c955bacadd", "node_type": null, "metadata": {}, "hash": "b743eeab3c848aca6a4ac34ab2b118c8bfa83196d2bb1013388391696cf3364e"}]}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84", "text": "File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor\nLinks:\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk.It does this by using NER (either with a dedicated NER model, or with a local LLM model).File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/LLM Version\n\n```python\nfrom llama_index.indices.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n  service_context=service_context,  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipline(\"ner\")`.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nfrom llama_index.indices.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nA full notebook guide for both can be found here.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.This is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n  docstore=index.docstore,\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n  mode=\"next\"   # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\n!File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n  docstore=index.docstore,\n  service_context=service_context\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nA full example notebook is available here.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/All Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/OptimizerDemo.ipynb\n/examples/node_postprocessor/CohereRerank.ipynb\n/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\n/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\n/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb\n/examples/node_postprocessor/PII.ipynb\n/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks:\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.In LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.LlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.File Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks:\n\nConfused about where node postprocessor fits in the pipeline?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35f742cf-4dde-4ace-9e1e-d3074523760f": {"__data__": {"id_": "35f742cf-4dde-4ace-9e1e-d3074523760f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "3": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}, "5": [{"node_id": "f60a00a0-3ba2-46c7-8cad-cd05df08c6af", "node_type": null, "metadata": {}, "hash": "9cffdfb14d878e50776c5de0f52510f16f4d348eb80a2cabc41f6660265fd71e"}, {"node_id": "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "node_type": null, "metadata": {}, "hash": "c0cbcb30decd9ba24c155ab3f966b8dc08e85f1ae9f99ab81a2b51457b8a64df"}, {"node_id": "edd49718-78ae-4333-94e6-2ffda9a60166", "node_type": null, "metadata": {}, "hash": "fe690d42f5aaee8838ac0f5b002f0da4509c7701e64b84ecd0bd67ae4d479cd1"}, {"node_id": "c907930f-94ca-477b-9bea-a98af3354b65", "node_type": null, "metadata": {}, "hash": "4dd43a1f5e676e1acf02041380d771bb6bf36ad7f0248e4a97292232884d160c"}]}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15", "text": "Read about high-level concepts\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks:\n\nAn example of using a node postprocessors is below:\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\nYou can find more details using post processors and how to build your own below.File Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks:\n\nBelow you can find guides for each node postprocessor.File Name: ../docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks:\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks:\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.File Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/all node post-processors will be applied during each query\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n  node_postprocessors=[\n    TimeWeightedPostprocessor(\n        time_decay=0.5, time_access_refresh=False, top_k=1\n    )\n  ]\n)\n\nresponse = query_engine.query(\"query string\")\n```\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks:\n\nOr used as a standalone object for filtering retrieved nodes:\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().query(\"query string\")\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks:\n\nAs you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.File Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple:\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "04e52538-f44c-4f5d-bbd5-41ae6bbd6372": {"__data__": {"id_": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "3": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}, "5": [{"node_id": "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb", "node_type": null, "metadata": {}, "hash": "1fb85872f4367af977decfaf90775641d425a30c48ace67ce2262bf9ee0b1aa6"}, {"node_id": "bf039726-614d-4d53-b16f-d119e7b18ae7", "node_type": null, "metadata": {}, "hash": "7daf57d6e5849bd6e98d3818c87c6d8107eed501fe1a84bc4828a688e59e1268"}, {"node_id": "bcc6f32f-9b05-4747-bcff-29b82efce87d", "node_type": null, "metadata": {}, "hash": "82e8070f7441f219bd41ec73e6a1c1eaf7f2cde7fd12fc25b5e22345ab51bab2"}]}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a", "text": "@abstractmethod\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"File Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nfrom llama_index import QueryBundle\nfrom llama_index.indices.postprocessor.base import BaseNodePostprocessor\nfrom llama_index.schema import NodeWithScore\n\nclass DummyNodePostprocessor:\n\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations\nLinks:\n\nLlamaIndex allows you to perform *query transformations* over your index structures.Query transformations are modules that will convert a query into another query.They can be **single-step**, as in the transformation is run once before the query is executed against an index.They can also be **multi-step**, as in: \n1.The query is transformed, executed against an index, \n2.The response is retrieved.3.Subsequent queries are transformed/executed in a sequential fashion.We list some of our query transformations in more detail below.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Use Cases\nLinks:\n\nQuery transformations have multiple use cases:\n- Transforming an initial query into a form that can be more easily embedded (e.g.HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own.(multi-step query decomposition)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks:\n\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first.This hypothetical document is then used for embedding lookup rather than the raw query.To use HyDE, an example code snippet is shown below.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks:\n\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks:\n\nCheck out our example notebook for a full walkthrough.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Single-Step Query Decomposition\nLinks:\n\nSome recent approaches (e.g.self-ask, ReAct) have suggested that LLM's \nperform better at answering complex questions when they break the question into smaller steps.We have found that this is true for queries that require knowledge augmentation as well.If your query is complex, different parts of your knowledge base may answer different \"subqueries\" around the overall query.Our single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.This is especially helpful over a composed graph.Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus.Query decomposition allows us to transform the query into a more suitable question over any given index.An example image is shown below.!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6920088c-2a84-4cbe-ad70-13f1be8704fb": {"__data__": {"id_": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}, "3": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "5": [{"node_id": "51c9d9e2-4889-4b34-84c8-76cc71f93b3f", "node_type": null, "metadata": {}, "hash": "d1753be1bb8e0df13dbeb1735c83414ef33647dd7c293075158ad286bd3e4dd7"}, {"node_id": "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "node_type": null, "metadata": {}, "hash": "36ac76a16f0bb5e758a3a01d9144c2f0a166db3bccf60d27529c5d7a733ed5ba"}, {"node_id": "25316757-07dd-4e6d-9f0f-6f96b9cf63e2", "node_type": null, "metadata": {}, "hash": "5f348a00cdcb3373aa777965ac3e0a504e23d589f5c11dfa236c3867a1b100bb"}]}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681", "text": "Here's a corresponding example code snippet over a composed graph.\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/llm_predictor_chatgpt corresponds to the ChatGPT LLM interface\nLinks:\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/initialize indexes and graph\nLinks:\n\n...\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/configure retrievers\nLinks:\n\nvector_query_engine = vector_index.as_query_engine()\nvector_query_engine = TransformQueryEngine(\n    vector_query_engine, \n    query_transform=decompose_transform\n    transform_extra_info={'index_summary': vector_index.index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.index_id: vector_query_engine\n}\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks:\n\nquery_str = (\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"\n)\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\nresponse = query_engine.query(query_str)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks:\n\nCheck out our example notebook for a full walkthrough.\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks:\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.\n\nGiven an initial, complex query, the query is transformed and executed against an index. The response is retrieved from the query. \nGiven the response (along with prior responses) and the query, followup questions may be asked against the index as well. This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.\n\nAn example image is shown below.\n\n!\n\n\nHere's a corresponding example code snippet.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks:\n\nfrom llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\nCheck out our example notebook for a full walkthrough.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "694198a8-926a-4f24-b121-897dd6ebd176": {"__data__": {"id_": "694198a8-926a-4f24-b121-897dd6ebd176", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}, "3": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "5": [{"node_id": "61b30575-d871-460a-af27-cd41e98c2909", "node_type": null, "metadata": {}, "hash": "16f3f7c84aec55a644fb8ea155e96dbd192b7b7d79d0b53f7bd4e4b88958fc48"}, {"node_id": "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "node_type": null, "metadata": {}, "hash": "fd9284a928c4c00541e4a594c9799b619cd2f9efcb9edc608c0c3698d3b30308"}, {"node_id": "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "node_type": null, "metadata": {}, "hash": "e0549233d2c4d811c7d20b71c081249f9cef80636d7d8dfa8466a9f4a6e2a8f7"}, {"node_id": "2a5b69df-3782-4a44-8674-fcc4a9fee8c4", "node_type": null, "metadata": {}, "hash": "e5d484c3d06c22948040a467fd5b4fcfdc6ea2991af7f0ab8e62556624a81fc7"}]}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc", "text": "File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n\nFile Name: ../docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Basic\n\n```{toctree}\n---\nmaxdepth: 1\n---\nRetriever Query Engine </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Structured & Semi-Structured Data\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/json_query_engine.ipynb\n/examples/query_engine/pandas_query_engine.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Advanced\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n/examples/query_engine/SQLRouterQueryEngine.ipynb\n/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\n/examples/query_engine/SQLJoinQueryEngine.ipynb\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\nRetry Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Source Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Guideline Query Engine </examples/evaluation/RetryQuery.ipynb>\n/examples/query_engine/citation_query_engine.ipynb\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Experimental\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/flare_query_engine.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/response_modes.md\nContent Type: text\nHeader Path: Response Modes\n\nRight now, we support the following options:\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`; \n    This makes a separate LLM call per Node.Good for more detailed answers.- `compact`: \"compact\" the prompt during each LLM call by stuffing as \n    many `Node` text chunks that can fit within the maximum prompt size.If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple prompts.- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree \n    and return the root node as the response.Good for summarization purposes.- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them.Then can be inspected by checking `response.source_nodes`.The response object is covered in more detail in Section 5.- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n    chunk while accumulating the responses into an array.Returns a concatenated string of all\n    responses.Good for when you need to run the same query separately against each text\n    chunk.See Response Synthesizer to learn more.File Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks:\n\nQuery engine is a generic interface that allows you to ask question over your data.A query engine takes in a natural language query, and returns a rich response.It is most often (but not always) built on one or many Indices via Retrievers.You can compose multiple query engines to achieve more advanced capability.File Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks:\n\nIf you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.\")File Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nTo stream response:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b24612df-85d4-4dfc-b6c9-84f0cb367ee5": {"__data__": {"id_": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "3": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}, "5": [{"node_id": "6c12ef17-17b3-47f6-87b8-c92e1391bfb7", "node_type": null, "metadata": {}, "hash": "9d8e5e06f4f0a3a3ba8f5586c4c57ef6b7486e6dc4aeb12e551086f3f3e69366"}, {"node_id": "e4737417-44d7-46eb-b775-58b673b79554", "node_type": null, "metadata": {}, "hash": "3627c85268c3748ed861ea7a144637f3b1f29c65f3967f3fb1f3ada6c758395f"}, {"node_id": "d8f3c932-6c3a-4538-9bd2-192933dc2055", "node_type": null, "metadata": {}, "hash": "91d403b091e2be9376485bfd5b7fc9ead09b5708442883ec0056483382010cd5"}, {"node_id": "4baab847-43bf-4430-a9bf-05f0c12d9b4d", "node_type": null, "metadata": {}, "hash": "0fdbefc843ac4b7186fd1cd00f56ac63f797611247df783e7a52fef6ac6e4781"}]}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2", "text": "streaming_response.print_response_stream()\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Modules\n\n```{toctree}\n---\nmaxdepth: 3\n---\nmodules.md\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Supporting Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\nsupporting_modules.md\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming\nLinks:\n\nLlamaIndex supports streaming the response as it's being generated.This allows you to start printing or processing the beginning of the response before the full response is finished.This can drastically reduce the perceived latency of queries.File Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nTo enable streaming, you need to use an LLM that supports streaming.Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).Configure query engine to use streaming:\n\nIf you are using the high-level API, set `streaming=True` when building a query engine.File Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n    similarity_top_k=1\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nfrom llama_index import get_response_synthesizer\nsynth = get_response_synthesizer(streaming=True, ...)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, ...)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.File Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nThe response is returned immediately when the LLM call *starts*, without having to wait for the full completion.> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.You can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.File Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nAlternatively, if you just want to print the text as they arrive:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nstreaming_response.print_response_stream()\n\nFile Name: ../docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\n\nSee an end-to-end example\n\nFile Name: ../docs/core_modules/query_modules/query_engine/supporting_modules.md\nContent Type: code\nHeader Path: Supporting Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\nadvanced/query_transformations.md\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nBuild a query engine from index:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nquery_engine = index.as_query_engine()\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nAsk a question over your data\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nresponse = query_engine.query('Who is Paul Graham?')", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8d3af97a-f70d-46ff-af7e-3eeb86d210a3": {"__data__": {"id_": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "3": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}, "5": [{"node_id": "a99d060e-119d-49e2-8d50-3781092ad15c", "node_type": null, "metadata": {}, "hash": "72c44d847f39678cc9526549dc74c37c8646fdf27ef648bf52f3ef754a3486ef"}, {"node_id": "78a64c63-ffca-43dc-82f0-566a375395c6", "node_type": null, "metadata": {}, "hash": "bd5cb0ac9e40b3538070334336607ee82c78c1fb9590922fa7adb1fd18d8ecec"}, {"node_id": "5171b641-9ca6-48a0-bde6-d47d021f3c54", "node_type": null, "metadata": {}, "hash": "099f46d80276e105d56c11faacbb4c32af70c9b71864369b910ac9502c08f0e8"}]}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd", "text": "File Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\nYou can directly build and configure a query engine from an index in 1 line of code:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\nquery_engine = index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.See **Response Modes** for a full list of response modes and what they do.File Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\n---\nmaxdepth: 1\nhidden:\n---\nresponse_modes.md\nstreaming.md\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.Concretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(...)`.> Note: You may need to look at API references or example notebooks.File Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks:\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/build index\nLinks:\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure retriever\nLinks:\n\nretriever = VectorIndexRetriever(\n    index=index, \n    similarity_top_k=2,\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure response synthesizer\nLinks:\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/assemble query engine\nLinks:\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/query\nLinks:\n\nresponse = query_engine.query(\"What did the author do growing up?\")print(response)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks:\n\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks:\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\nstreaming_response.print_response_stream()\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\n\n* Read the full streaming guide\n* See an end-to-end example\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide\nLinks:\n\nDetailed inputs/outputs for each response synthesizer are found below.File Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks:\n\nThe following shows the setup for utilizing all kwargs.- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4bec885-2ac4-4e51-b783-7835e4181f96": {"__data__": {"id_": "d4bec885-2ac4-4e51-b783-7835e4181f96", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}, "3": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}, "5": [{"node_id": "8316e5ec-9820-47ce-9a03-0a5902c9bd70", "node_type": null, "metadata": {}, "hash": "2cd55610a036436819ffbd3c5e95d45ea799487ed09d0e15715e7bfb7be5d491"}, {"node_id": "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "node_type": null, "metadata": {}, "hash": "8a4fde3c0393191e71e36a6a9eeb0742b89d25f6c232fed0a6bfe5489275eda1"}, {"node_id": "a1c28ad7-78bf-4afb-b187-ab49cae10939", "node_type": null, "metadata": {}, "hash": "6d3cc1150e1d1f8cf08348b84a2f68d099212f4e6633b10a4795192451c06cdf"}]}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee", "text": "File Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks:\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n  response_mode=\"refine\",\n  service_context=service_context,\n  text_qa_template=text_qa_template,\n  refine_template=refine_template,\n  use_async=False,\n  streaming=False\n)\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/synchronous\nLinks:\n\nresponse = response_synthesizer.synthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nresponse = await response_synthesizer.asynthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..], \n)\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nresponse_str = response_synthesizer.get_response(\n  \"query string\", \n  text_chunks=[\"text1\", \"text2\", ...]\n)\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: code\nHeader Path: Module Guide/Example Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/response_synthesizers/refine.ipynb\n/examples/response_synthesizers/tree_summarize.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks:\n\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks.The output of a response synthesizer is a `Response` object.The method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree.The main idea here is to simplify the process of generating a response using an LLM across your data.When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.File Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks:\n\nConfused about where response synthesizer fits in the pipeline?Read the high-level concepts\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nUse a response synthesizer on it's own:\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nfrom llama_index.schema import Node\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), ...])\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nOr in a query engine after you've created an index:\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.File Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks:\n\nBelow you can find detailed API information for each response synthesis module.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26782402-9635-482e-b626-75e58b3de6af": {"__data__": {"id_": "26782402-9635-482e-b626-75e58b3de6af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}, "3": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}, "5": [{"node_id": "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27", "node_type": null, "metadata": {}, "hash": "3ee763f8403d7ca6d8b237a566ba334f767091af21fbb5351b0a847760cd5f1b"}, {"node_id": "f2850e09-1c16-403b-93dd-9b73f989a8f5", "node_type": null, "metadata": {}, "hash": "8bd18e5f22a151b467ed1000b315084968cba810ecf92c83b7b06f70fa27f263"}, {"node_id": "17a940a6-e716-499c-9d83-542c33d3f97c", "node_type": null, "metadata": {}, "hash": "e22bb61dfa5d5ac930b7669bdade19f4ea8fd18d8cdc6dce23351b13fa86c3d7"}]}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138", "text": "File Name: ../docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks:\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\n  \"query text\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), ..]\n)\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nOr, more commonly, in a query engine after you've created an index:\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring the Response Mode\nLinks:\n\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.Several response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: \"create and refine\" an answer by sequentially going through each retrieved text chunk.This makes a separate LLM call per Node.Good for more detailed answers.- `compact` (default): \"compact\" the prompt during each LLM call by stuffing as \n    many text chunks that can fit within the maximum prompt size.If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple compact prompts.The same as `refine`, but should result in less LLM calls.- `tree_summarize`: Given a set of text chunks and the query, recursively construct a tree \n    and return the root node as the response.Good for summarization purposes.- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt.Good for quick\n    summarization purposes, but may lose detail due to truncation.- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them.Then can be inspected by checking `response.source_nodes`.- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array.Returns a concatenated string of all\n    responses.Good for when you need to run the same query separately against each text\n    chunk.- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.File Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks:\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`.The base API is extremely simple, which makes it easy to create your own response synthesizer.Maybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement.The basic requirements are to process a query and text chunks, and return a string (or string generator) response.File Name: ../docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks:\n\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"def __init__(\n        self,\n        service_context: Optional[ServiceContext] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"self._service_context = service_context or ServiceContext.from_defaults()\n        self._callback_manager = self._service_context.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25b2aa80-5f4e-4746-b48d-42097f230452": {"__data__": {"id_": "25b2aa80-5f4e-4746-b48d-42097f230452", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}, "3": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "5": [{"node_id": "05d0a4b8-9787-412e-9ab8-92bdcec12d1e", "node_type": null, "metadata": {}, "hash": "0173af05b63014a0fa9fe4fe78768f125b690c2db01ad72e4eedd1e1ff6b0a57"}, {"node_id": "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "node_type": null, "metadata": {}, "hash": "fc88305a4fedc81a3108d9b3646098eb9aa680c1aabd1c52062298497e0c4f0b"}, {"node_id": "7436654d-25ea-4058-9623-c981db612e37", "node_type": null, "metadata": {}, "hash": "557643045b9c72929610dc515e91171a9df1d09c28ba1816d5d03bf587c97f5c"}]}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4", "text": "...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"...File Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\nWe are adding more module guides soon!In the meanwhile, please take a look at the API References.File Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks:\n\n* VectorIndexRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks:\n\n---\nmaxdepth: 1\n---\nVectorIndexAutoRetriever </examples/vector_stores/chroma_auto_retriever.ipynb>\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/List Index\nLinks:\n\n* ListIndexRetriever \n* ListIndexEmbeddingRetriever \n* ListIndexLLMRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Tree Index\nLinks:\n\n* TreeSelectLeafRetriever\n* TreeSelectLeafEmbeddingRetriever\n* TreeAllLeafRetriever\n* TreeRootRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Keyword Table Index\nLinks:\n\n* KeywordTableGPTRetriever\n* KeywordTableSimpleRetriever\n* KeywordTableRAKERetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: code\nHeader Path: Module Guides/Knowledge Graph Index\n\n```{toctree}\n---\nmaxdepth: 1\n---\nCustom Retriever (KG Index and Vector Store Index) </examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb>\n```\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Knowledge Graph Index\nLinks:\n\n* KGTableRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Document Summary Index\nLinks:\n\n* DocumentSummaryIndexRetriever\n* DocumentSummaryIndexEmbeddingRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks:\n\n* TransformRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks:\n\n---\nmaxdepth: 1\n---\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n\nFile Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes\nLinks:\n\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.> Note that `retriever_mode` can mean different thing for different index classes.File Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Vector Index\nLinks:\n\nSpecifying `retriever_mode` has no effect (silently ignored).`vector_index.as_retriever(...)` always returns a VectorIndexRetriever.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aae303c7-88a8-425a-baa2-775741a5e48a": {"__data__": {"id_": "aae303c7-88a8-425a-baa2-775741a5e48a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}, "3": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}, "5": [{"node_id": "4387dad2-8b75-46de-bc1b-3ff7936eb37a", "node_type": null, "metadata": {}, "hash": "f1048bc9c7dbdd264ae25a82c1e249415c2c6f3cf9c99345e7566b331788ed0d"}, {"node_id": "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "node_type": null, "metadata": {}, "hash": "4c921f860ffb445c035ef24b476d6b31262e195905bfe401629c31da9a14741d"}, {"node_id": "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "node_type": null, "metadata": {}, "hash": "b1facabb9c90f48304de2adcd82f65164f296f3207d5ad09747ace5f84a04b3f"}, {"node_id": "a600037b-6be1-416c-be05-c752abf38ffe", "node_type": null, "metadata": {}, "hash": "c36fe57fd95ade9b737e41dbbbc8b18fe6a83c85ff9414470e7675e68947114a"}]}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d", "text": "File Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/List Index\nLinks:\n\n* `default`: ListIndexRetriever \n* `embedding`: ListIndexEmbeddingRetriever \n* `llm`: ListIndexLLMRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Tree Index\nLinks:\n\n* `select_leaf`: TreeSelectLeafRetriever\n* `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n* `all_leaf`: TreeAllLeafRetriever\n* `root`: TreeRootRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Keyword Table Index\nLinks:\n\n* `default`: KeywordTableGPTRetriever\n* `simple`: KeywordTableSimpleRetriever\n* `rake`: KeywordTableRAKERetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Knowledge Graph Index\nLinks:\n\n* `keyword`: KGTableRetriever\n* `embedding`: KGTableRetriever\n* `hybrid`: KGTableRetriever\n\nFile Name: ../docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Document Summary Index\n\n* `default`: DocumentSummaryIndexRetriever\n* `embedding`: DocumentSummaryIndexEmbeddingRetrievers\n\nFile Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks:\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).It can be built on top of Indices, but can also be defined independently.It is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.File Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks:\n\nConfused about where retriever fits in the pipeline?Read about high-level concepts\n\nFile Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks:\n\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?\")File Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: code\nHeader Path: Retriever/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: code\nHeader Path: Retriever/Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nGet a retriever from index:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nretriever = index.as_retriever()\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nRetrieve relevant context for a question:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nnodes = retriever.retrieve('Who is Paul Graham?')File Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\n> Note: To learn how to build an index, see Index\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nYou can select the index-specific retriever class via `retriever_mode`.For example, with a `ListIndex`:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n)\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nThis creates a ListIndexLLMRetriever on top of the list index.See **Retriever Modes** for a full list of (index-specific) retriever modes\nand the retriever classes they map to.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "413527a2-71f2-466f-b6c7-4b480c41b702": {"__data__": {"id_": "413527a2-71f2-466f-b6c7-4b480c41b702", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "3": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "5": [{"node_id": "8e83a6bb-1b59-4597-8b68-0af905679267", "node_type": null, "metadata": {}, "hash": "7778d12011847b5667a700609c14bc83795807abcebefad0a039a585ebe7c541"}, {"node_id": "469c53f1-d0e2-40b2-8f54-f84a22474d93", "node_type": null, "metadata": {}, "hash": "7564e066790552ce64be067ad08a95b6136078f91d5bd807be3a1ae7a150ba5b"}, {"node_id": "5c7983b9-f801-4a20-9ddc-321df1a15374", "node_type": null, "metadata": {}, "hash": "ddc83cb3fd24b86fb2da1a814ae51d5e6a42d1f8730252d5f78fa94594e42a05"}]}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960", "text": "File Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\n---\nmaxdepth: 1\nhidden:\n---\nretriever_modes.md\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks:\n\nIn the same way, you can pass kwargs to configure the selected retriever.> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.For example, if we selected the \"llm\" retriever mode, we might do the following:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks:\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n    choice_batch_size=5,\n)\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.To achieve the same outcome as above, you can directly import and construct the desired retriever class:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks:\n\nfrom llama_index.indices.list import ListIndexLLMRetriever\n\nretriever = ListIndexLLMRetriever(\n    index=list_index,\n    choice_batch_size=5,\n)\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/High-Level API/Advanced\n\n```{toctree}\n---\nmaxdepth: 1\n---\nDefine Custom Retriever </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing\nLinks:\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks.These output parsing modules can be used in the following ways:\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks:\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas.See below for a code example.File Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import GuardrailsOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define query / output spec\nLinks:\n\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "426429e8-7a33-4b53-a9f5-e586e1f65f2c": {"__data__": {"id_": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}, "3": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}, "5": [{"node_id": "a7e3546c-e36e-4229-a275-24b1eba8a3fd", "node_type": null, "metadata": {}, "hash": "16362627e4a35e5e81416ce8f903a66949c04a7e6f05809df83452c5d9a817ad"}, {"node_id": "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "node_type": null, "metadata": {}, "hash": "16c3441a8677f30247d9e5fa184585d2335284f4a5b1648bd3f59a670344279d"}, {"node_id": "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "node_type": null, "metadata": {}, "hash": "451a04f37c689d504f80a19d43848f5c94aa339b1ff4943b06555144a8f89bef"}, {"node_id": "26022177-817a-4eb3-895a-71b8837d4ebd", "node_type": null, "metadata": {}, "hash": "45a4b8a594725fd7cf65ef78af2aaf9b31b30848ff0b088a0d0f2eb75839f998"}]}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2", "text": "@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks:\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks:\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?\", \n)\nprint(response)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\nOutput:\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks:\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.File Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import LangchainOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('../paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.from_documents(documents)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output schema\nLinks:\n\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")]\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks:\n\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks:\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb266488-e2e8-4a42-b4c9-c72a9cafe32f": {"__data__": {"id_": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "3": {"node_id": "b6260184-53e4-4db7-b4a2-3259dde76196", "node_type": null, "metadata": {}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd"}, "5": [{"node_id": "f15cb451-d8da-4e4f-afcc-7c195357c7a2", "node_type": null, "metadata": {}, "hash": "87e3e0448b8585cd89ec9f82e72b3f7e88aac60e246eeb91cd49b87539d282d7"}, {"node_id": "06e1128d-3441-4bd0-81ff-61e6be9a7404", "node_type": null, "metadata": {}, "hash": "bbd130373a88f7e04a7208c061869d4860fb30730f945f20f640630086fb2a47"}, {"node_id": "99f90aec-e539-408d-b6c4-0f026081ffd5", "node_type": null, "metadata": {}, "hash": "7962e6a3aedc89e9322cbcf510865fb49119839ff70f59a7ca3b22f10fb41920"}]}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c", "text": "\", \n)\nprint(str(response))\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\nOutput:\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}File Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: code\nHeader Path: Output Parsing/Guides\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n\n/examples/output_parsing/GuardrailsDemo.ipynb\n/examples/output_parsing/LangchainOutputParserDemo.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n/examples/output_parsing/openai_pydantic_program.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program\nLinks:\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.Because this abstraction is so generic, it encompasses a broad range of LLM workflows.The programs are composable and be for more generic or specific use cases.There's a few general types of Pydantic Programs:\n- **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.- **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.File Name: ../docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program/LLM Text Completion Pydantic Programs\nLinks:\n\nTODO: Coming soon!File Name: ../docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/LLM Function Calling Pydantic Programs\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/openai_pydantic_program.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/Prepackaged Pydantic Programs\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/df_program.ipynb\n/examples/output_parsing/evaporate_program.ipynb\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: text\nHeader Path: Structured Outputs\nLinks:\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values.LlamaIndex itself also relies on structured output in the following ways.- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval.For instance, the tree index expects LLM calls to be in the format \"ANSWER: (number)\".- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g.a JSON output, a formatted SQL query, etc.)LlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format.We provide modules at different levels of abstraction:\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint.They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object.They may use function calling APIs or text completion APIs + output parsers.- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).See the sections below for an overview of output parsers and Pydantic programs.File Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: text\nHeader Path: Structured Outputs/\ud83d\udd2c Anatomy of a Structured Output Function\nLinks:\n\nHere we describe the different components of an LLM-powered structured output function.The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.!With generic completion APIs, the inputs and outputs are handled by text prompts.The output parser plays a role before and after the LLM call in ensuring structured outputs.Before the LLM call, the output parser can\nappend format instructions to the prompt.After the LLM call, the output parser can parse the output to the specified instructions.With function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object.The structured output just needs to be cast in the right object format (e.g.Pydantic).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b6260184-53e4-4db7-b4a2-3259dde76196": {"__data__": {"id_": "b6260184-53e4-4db7-b4a2-3259dde76196", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ae9ef74-42fa-406e-87af-3080e3a73233", "node_type": null, "metadata": {}, "hash": "1ad3de0545ad0ad79cb6bc5b187a6911397dc74295b7be8dee89de3db6b7d259"}, "2": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}, "5": [{"node_id": "9c03a722-9e11-431f-a445-106c1699e852", "node_type": null, "metadata": {}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd"}]}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd", "text": "File Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: code\nHeader Path: Structured Outputs/Output Parser Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\noutput_parser.md\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: code\nHeader Path: Structured Outputs/Pydantic Program Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\npydantic_program.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ed55ac15-ce09-44f2-8794-eb6f30ad4102": {"__data__": {"id_": "ed55ac15-ce09-44f2-8794-eb6f30ad4102", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}, "3": {"node_id": "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "node_type": null, "metadata": {}, "hash": "77f96d1de5680edd6c50e02c17bcb4d73aec29caf811c0fceff1d78500341caf"}, "4": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}}, "hash": "4262e11df5c3bbed8251c990f57e24075fea68dec6c8db7b8c6b5319d0269284", "text": "File Name: ./docs/core_modules/query_modules/chat_engines/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\nWe provide a few simple implementations to start, with more sophisticated modes coming soon!More specifically, the `SimpleChatEngine` does not make use of a knowledge base, \nwhereas `CondenseQuestionChatEngine` and `ReActChatEngine` make use of a query engine over knowledge base.File Name: ./docs/core_modules/query_modules/chat_engines/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\n---\nmaxdepth: 1\n---\nSimple Chat Engine </examples/chat_engine/chat_engine_repl.ipynb>\nReAct Chat Engine </examples/chat_engine/chat_engine_react.ipynb>\nOpenAI Chat Engine </examples/chat_engine/chat_engine_openai.ipynb>\nCondense Question Chat Engine </examples/chat_engine/chat_engine_condense_question.ipynb>\n\nFile Name: ./docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks:\n\nChat engine is a high-level interface for having a conversation with your data\n(multiple back-and-forth instead of a single question & answer).Think ChatGPT, but augmented with your knowledge base.Conceptually, it is a **stateful** analogy of a Query Engine.By keeping track of the conversation history, it can answer questions with past context in mind.File Name: ./docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Concept\nLinks:\n\nIf you want to ask standalone question over your data (i.e.without keeping track of conversation history), use Query Engine instead.File Name: ./docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "37d71d91-c9f9-4466-b919-a2d3f4eaff84": {"__data__": {"id_": "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}, "2": {"node_id": "ed55ac15-ce09-44f2-8794-eb6f30ad4102", "node_type": null, "metadata": {}, "hash": "4262e11df5c3bbed8251c990f57e24075fea68dec6c8db7b8c6b5319d0269284"}, "3": {"node_id": "7606d974-a75a-40e4-b749-ceee37dbf1b5", "node_type": null, "metadata": {}, "hash": "f994cfd53ce54f093d632d29c3995f2e5a7cb76c42c6249fa8ec43feb26fd0ee"}, "4": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}}, "hash": "77f96d1de5680edd6c50e02c17bcb4d73aec29caf811c0fceff1d78500341caf", "text": "/docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nchat_engine = index.as_chat_engine()\nresponse = chat_engine.chat(\"Tell me a joke.\")File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nTo stream response:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Usage Pattern\nLinks:\n\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7606d974-a75a-40e4-b749-ceee37dbf1b5": {"__data__": {"id_": "7606d974-a75a-40e4-b749-ceee37dbf1b5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}, "2": {"node_id": "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "node_type": null, "metadata": {}, "hash": "77f96d1de5680edd6c50e02c17bcb4d73aec29caf811c0fceff1d78500341caf"}, "3": {"node_id": "0ecd70b0-69e8-4c8d-bbde-4018da0126cb", "node_type": null, "metadata": {}, "hash": "16dc20ceedccee484801f6cd6331c075d63d21ef38feba760930abe8c6723f57"}, "4": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}}, "hash": "f994cfd53ce54f093d632d29c3995f2e5a7cb76c42c6249fa8ec43feb26fd0ee", "text": "\")for token in streaming_response.response_gen:\n    print(token, end=\"\")\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: code\nHeader Path: Chat Engine/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks:\n\nBelow you can find corresponding tutorials to see the available chat engines in action.File Name: ../docs/core_modules/query_modules/chat_engines/root.md\nContent Type: text\nHeader Path: Chat Engine/Modules\nLinks:\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nBuild a chat engine from index:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine = index.as_chat_engine()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nHave a conversation with your data:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nresponse = chat_engine.chat(\"Tell me a joke.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0ecd70b0-69e8-4c8d-bbde-4018da0126cb": {"__data__": {"id_": "0ecd70b0-69e8-4c8d-bbde-4018da0126cb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}, "2": {"node_id": "7606d974-a75a-40e4-b749-ceee37dbf1b5", "node_type": null, "metadata": {}, "hash": "f994cfd53ce54f093d632d29c3995f2e5a7cb76c42c6249fa8ec43feb26fd0ee"}, "4": {"node_id": "4d3efd15-341c-4b29-afa7-7953188d2938", "node_type": null, "metadata": {}, "hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2"}}, "hash": "16dc20ceedccee484801f6cd6331c075d63d21ef38feba760930abe8c6723f57", "text": "\")File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nReset chat history to start a new conversation:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine.reset()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nEnter an interactive chat REPL:\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nchat_engine.chat_repl()\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine\nLinks:\n\nConfiguring a chat engine is very similar to configuring a query engine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f4bd15e2-598d-4e37-ae15-6e03a9f08398": {"__data__": {"id_": "f4bd15e2-598d-4e37-ae15-6e03a9f08398", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "3": {"node_id": "fb970878-52f4-4123-af7f-ef6a948abf39", "node_type": null, "metadata": {}, "hash": "c6253458d24ad71db59bdcd0495916aa2f8a261ef905f899e1ca3461c1c192cc"}, "4": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}}, "hash": "f997d531bcd63c9f023e50a984d05cab88c8796412bc8c6bc991a5335aea856e", "text": "File Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\nYou can directly build and configure a chat engine from an index in 1 line of code:\n\nFile Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\nchat_engine = index.as_chat_engine(\n    chat_mode='condense_question', \n    verbose=True\n)\n\nFile Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/High-Level API\nLinks:\n\n> Note: you can access different chat engines by specifying the `chat_mode` as a kwarg.`condense_question` corresponds to `CondenseQuestionChatEngine`, `react` corresponds to `ReActChatEngine`.> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.File Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.Concretely speaking, you would explicitly construct `ChatEngine` object instead of calling `index.as_chat_engine(.)`.> Note: You may need to look at API references or example notebooks.Here's an example where we configure the following:\n* configure the condense question prompt, \n* initialize the conversation with some existing history,\n* print verbose debug message.File Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb970878-52f4-4123-af7f-ef6a948abf39": {"__data__": {"id_": "fb970878-52f4-4123-af7f-ef6a948abf39", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "2": {"node_id": "f4bd15e2-598d-4e37-ae15-6e03a9f08398", "node_type": null, "metadata": {}, "hash": "f997d531bcd63c9f023e50a984d05cab88c8796412bc8c6bc991a5335aea856e"}, "3": {"node_id": "051bf54d-d207-473e-a13b-3a0977d00377", "node_type": null, "metadata": {}, "hash": "48122b3e6a2098d31381010c56fa07d7b775eef7f1ff5ab516fcd7d8fb43e3b2"}, "4": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}}, "hash": "c6253458d24ad71db59bdcd0495916aa2f8a261ef905f899e1ca3461c1c192cc", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Low-Level Composition API\nLinks:\n\nfrom llama_index.prompts  import Prompt\n\ncustom_prompt = Prompt(\"\"\"\\\nGiven a conversation (between Human and Assistant) and a follow up message from Human, \\\nrewrite the message to be a standalone question that captures all relevant context \\\nfrom the conversation.<Chat History> \n{chat_history}\n\n<Follow Up Message>\n{question}\n\n<Standalone question>\n\"\"\")\n\nFile Name: ./docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/list of (human_message, ai_message) tuples\nLinks:\n\ncustom_chat_history = [\n    (\n        'Hello assistant, we are having a insightful discussion about Paul Graham today.', \n        'Okay, sounds good.')", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "051bf54d-d207-473e-a13b-3a0977d00377": {"__data__": {"id_": "051bf54d-d207-473e-a13b-3a0977d00377", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "2": {"node_id": "fb970878-52f4-4123-af7f-ef6a948abf39", "node_type": null, "metadata": {}, "hash": "c6253458d24ad71db59bdcd0495916aa2f8a261ef905f899e1ca3461c1c192cc"}, "3": {"node_id": "fbf04091-9814-4c59-a2af-59cf4442e28a", "node_type": null, "metadata": {}, "hash": "e6dc2d9b6f90a2e165bf9b33f012c19e41a3164d99c104336abf3b6d508b6b18"}, "4": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}}, "hash": "48122b3e6a2098d31381010c56fa07d7b775eef7f1ff5ab516fcd7d8fb43e3b2", "text": "]\n\nquery_engine = index.as_query_engine()\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine, \n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True\n)\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks:\n\nTo enable streaming, you simply need to call the `stream_chat` endpoint instead of the `chat` endpoint.File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\nLinks:\n\nThis somewhat inconsistent with query engine (where you pass in a `streaming=True` flag).We are working on making the behavior more consistent!File Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\n\n```python\nchat_engine = index.as_chat_engine()\nstreaming_response = chat_engine.stream_chat(\"Tell me a joke.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fbf04091-9814-4c59-a2af-59cf4442e28a": {"__data__": {"id_": "fbf04091-9814-4c59-a2af-59cf4442e28a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}, "2": {"node_id": "051bf54d-d207-473e-a13b-3a0977d00377", "node_type": null, "metadata": {}, "hash": "48122b3e6a2098d31381010c56fa07d7b775eef7f1ff5ab516fcd7d8fb43e3b2"}, "4": {"node_id": "6113854b-aea9-4310-bc89-6bf385c91deb", "node_type": null, "metadata": {}, "hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19"}}, "hash": "e6dc2d9b6f90a2e165bf9b33f012c19e41a3164d99c104336abf3b6d508b6b18", "text": "\")for token in streaming_response.response_gen:\n    print(token, end=\"\")\n```\n\nFile Name: ../docs/core_modules/query_modules/chat_engines/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Chat Engine/Streaming\n\nSee an end-to-end tutorial\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks:\n\nUsed to remove nodes that are below a similarity score threshold.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SimilarityPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\npostprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks:\n\nUsed to ensure certain keywords are either excluded or included.File Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/KeywordNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import KeywordNodePostprocessor\n\npostprocessor = KeywordNodePostprocessor(\n  required_keywords=[\"word1\", \"word2\"],\n  exclude_keywords=[\"word3\", \"word4\"]\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ../docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nThis postprocessor optimizes token usage by removing sentences that are not relevant to the query (this is done using embeddings).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a424f32-a874-4f8e-ab1c-991d3a9dbe49": {"__data__": {"id_": "9a424f32-a874-4f8e-ab1c-991d3a9dbe49", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "3": {"node_id": "897ceee3-7af9-4300-a996-7069853dd3fe", "node_type": null, "metadata": {}, "hash": "415da5caa22124dcc721dee78617a51e585f7fe8dece2256478f05ea40b355e5"}, "4": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}}, "hash": "33315f4fb092f447a63428c4de8c9426c04b9d14b9066451bb8c349bb8aed600", "text": "The percentile cutoff is a measure for using the top percentage of relevant sentences.The threshold cutoff can be specified instead, which uses a raw similarity cutoff for picking which sentences to keep.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nfrom llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n\npostprocessor = SentenceEmbeddingOptimizer(\n  embed_model=service_context.embed_model,\n  percentile_cutoff=0.5,\n  # threshold_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/SentenceEmbeddingOptimizer\nLinks:\n\nA full notebook guide can be found here\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nUses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N nodes.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nfrom llama_index.indices.postprocessor import CohereRerank\n\npostprocessor = CohereRerank(\n  top_n=2\n  model=\"rerank-english-v2.0\",\n  api_key=\"YOUR COHERE API KEY\"\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/CohereRerank\nLinks:\n\nFull notebook guide is available here.File Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "897ceee3-7af9-4300-a996-7069853dd3fe": {"__data__": {"id_": "897ceee3-7af9-4300-a996-7069853dd3fe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "2": {"node_id": "9a424f32-a874-4f8e-ab1c-991d3a9dbe49", "node_type": null, "metadata": {}, "hash": "33315f4fb092f447a63428c4de8c9426c04b9d14b9066451bb8c349bb8aed600"}, "3": {"node_id": "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "node_type": null, "metadata": {}, "hash": "36738bb23aa0fcc746d254c0e39e574306470f8125f936803db728adca78853a"}, "4": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}}, "hash": "415da5caa22124dcc721dee78617a51e585f7fe8dece2256478f05ea40b355e5", "text": "/docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nUses a LLM to re-order nodes by asking the LLM to return the relevant documents and a score of how relevant they are.Returns the top N ranked nodes.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nfrom llama_index.indices.postprocessor import LLMRerank\n\npostprocessor = LLMRerank(\n  top_n=2\n  service_context=service_context,\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/LLM Rerank\nLinks:\n\nFull notebook guide is available her for Gatsby and here for Lyft 10K documents.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes sorted by date.This assumes there is a `date` field to parse in the metadata of each node.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import FixedRecencyPostprocessor\n\npostprocessor = FixedRecencyPostprocessor(\n  tok_k=1,\n  date_key=\"date\"  # the key in the metadata to find the date\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c97f0f09-c85c-495b-a3a0-2edb8958a18c": {"__data__": {"id_": "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "2": {"node_id": "897ceee3-7af9-4300-a996-7069853dd3fe", "node_type": null, "metadata": {}, "hash": "415da5caa22124dcc721dee78617a51e585f7fe8dece2256478f05ea40b355e5"}, "3": {"node_id": "b42d48c1-08ff-42a5-bdb4-9dd6f364a027", "node_type": null, "metadata": {}, "hash": "40e16441ab13ac84a903aa068d336b090e273e96b024890e4b0050529b834cab"}, "4": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}}, "hash": "36738bb23aa0fcc746d254c0e39e574306470f8125f936803db728adca78853a", "text": "md\nContent Type: text\nHeader Path: Modules/FixedRecencyPostprocessor\nLinks:\n\n!A full notebook guide is available here.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes after sorting by date and removing older nodes that are too similar after measuring embedding similarity.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import EmbeddingRecencyPostprocessor\n\npostprocessor = EmbeddingRecencyPostprocessor(\n  service_context=service_context,\n  date_key=\"date\",\n  similarity_cutoff=0.7\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/EmbeddingRecencyPostprocessor\nLinks:\n\nA full notebook guide is available here.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nThis postproccesor returns the top K nodes applying a time-weighted rerank to each node.Each time a node is retrieved, the time it was retrieved is recorded.This biases search to favor information that has not be returned in a query yet.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\npostprocessor = TimeWeightedPostprocessor(\n  time_decay=0.99,\n  top_k=1\n)\n\npostprocessor.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b42d48c1-08ff-42a5-bdb4-9dd6f364a027": {"__data__": {"id_": "b42d48c1-08ff-42a5-bdb4-9dd6f364a027", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}, "2": {"node_id": "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "node_type": null, "metadata": {}, "hash": "36738bb23aa0fcc746d254c0e39e574306470f8125f936803db728adca78853a"}, "4": {"node_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "node_type": null, "metadata": {}, "hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819"}}, "hash": "40e16441ab13ac84a903aa068d336b090e273e96b024890e4b0050529b834cab", "text": "postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/TimeWeightedPostprocessor\nLinks:\n\nA full notebook guide is available here.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4892a1f-2218-4bfe-b691-4162b1d4bf80": {"__data__": {"id_": "a4892a1f-2218-4bfe-b691-4162b1d4bf80", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "3": {"node_id": "e043659e-e9a0-4c87-8095-875bfdf7221a", "node_type": null, "metadata": {}, "hash": "1cfa1181e601b4943de427f459dfd2ef90dd03581280ef97cff71cb4a0481ba2"}, "4": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}}, "hash": "bbfc9e7516b4ad407a62d38a5451f7df026faabb8a4734f99b0f297b1e81a75e", "text": "File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor\nLinks:\n\nThe PII (Personal Identifiable Information) postprocssor removes information that might be a security risk.It does this by using NER (either with a dedicated NER model, or with a local LLM model).File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/LLM Version\n\n```python\nfrom llama_index.indices.postprocessor import PIINodePostprocessor\n\npostprocessor = PIINodePostprocessor(\n  service_context=service_context,  # this should be setup with an LLM you trust\n)\n\npostprocessor.postprocess_nodes(nodes)\n```\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nThis version uses the default local model from Hugging Face that is loaded when you run `pipline(\"ner\")`.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nfrom llama_index.indices.postprocessor import NERPIINodePostprocessor\n\npostprocessor = NERPIINodePostprocessor()\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/NER Version\nLinks:\n\nA full notebook guide for both can be found here.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e043659e-e9a0-4c87-8095-875bfdf7221a": {"__data__": {"id_": "e043659e-e9a0-4c87-8095-875bfdf7221a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "2": {"node_id": "a4892a1f-2218-4bfe-b691-4162b1d4bf80", "node_type": null, "metadata": {}, "hash": "bbfc9e7516b4ad407a62d38a5451f7df026faabb8a4734f99b0f297b1e81a75e"}, "3": {"node_id": "122d9186-a52b-4223-b806-d42a8563c93e", "node_type": null, "metadata": {}, "hash": "b89bd911f9c9274d05b41661c2651814c70623c370cecc0469df24111adebeaf"}, "4": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}}, "hash": "1cfa1181e601b4943de427f459dfd2ef90dd03581280ef97cff71cb4a0481ba2", "text": "md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\nUses pre-defined settings to read the `Node` relationships and fetch either all nodes that come previously, next, or both.This is useful when you know the relationships point to important data (either before, after, or both) that should be sent to the LLM if that node is retrieved.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import PrevNextNodePostprocessor\n\npostprocessor = PrevNextNodePostprocessor(\n  docstore=index.docstore,\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards\n  mode=\"next\"   # can be either 'next', 'previous', or 'both'\n)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) PrevNextNodePostprocessor\nLinks:\n\n!File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nThe same as PrevNextNodePostprocessor, but lets the LLM decide the mode (next, previous, or both).File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nfrom llama_index.indices.postprocessor import AutoPrevNextNodePostprocessor\n\npostprocessor = AutoPrevNextNodePostprocessor(\n  docstore=index.docstore,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "122d9186-a52b-4223-b806-d42a8563c93e": {"__data__": {"id_": "122d9186-a52b-4223-b806-d42a8563c93e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "2": {"node_id": "e043659e-e9a0-4c87-8095-875bfdf7221a", "node_type": null, "metadata": {}, "hash": "1cfa1181e601b4943de427f459dfd2ef90dd03581280ef97cff71cb4a0481ba2"}, "3": {"node_id": "9030c3b5-ba90-48a4-99dd-08c955bacadd", "node_type": null, "metadata": {}, "hash": "b743eeab3c848aca6a4ac34ab2b118c8bfa83196d2bb1013388391696cf3364e"}, "4": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}}, "hash": "b89bd911f9c9274d05b41661c2651814c70623c370cecc0469df24111adebeaf", "text": "service_context=service_context\n  num_nodes=1,  # number of nodes to fetch when looking forawrds or backwards)\n\npostprocessor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: text\nHeader Path: Modules/(Beta) PIINodePostprocessor/(Beta) AutoPrevNextNodePostprocessor\nLinks:\n\nA full example notebook is available here.File Name: ./docs/core_modules/query_modules/node_postprocessors/modules.md\nContent Type: code\nHeader Path: Modules/(Beta) PIINodePostprocessor/All Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/node_postprocessor/OptimizerDemo.ipynb\n/examples/node_postprocessor/CohereRerank.ipynb\n/examples/node_postprocessor/LLMReranker-Lyft-10k.ipynb\n/examples/node_postprocessor/LLMReranker-Gatsby.ipynb\n/examples/node_postprocessor/RecencyPostprocessorDemo.ipynb\n/examples/node_postprocessor/TimeWeightedPostprocessorDemo.ipynb\n/examples/node_postprocessor/PII.ipynb\n/examples/node_postprocessor/PrevNextPostprocessorDemo.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks:\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.In LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.LlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.File Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9030c3b5-ba90-48a4-99dd-08c955bacadd": {"__data__": {"id_": "9030c3b5-ba90-48a4-99dd-08c955bacadd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}, "2": {"node_id": "122d9186-a52b-4223-b806-d42a8563c93e", "node_type": null, "metadata": {}, "hash": "b89bd911f9c9274d05b41661c2651814c70623c370cecc0469df24111adebeaf"}, "4": {"node_id": "6dd66f92-57c2-4089-9f14-5d673d36d762", "node_type": null, "metadata": {}, "hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84"}}, "hash": "b743eeab3c848aca6a4ac34ab2b118c8bfa83196d2bb1013388391696cf3364e", "text": "/docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Concept\nLinks:\n\nConfused about where node postprocessor fits in the pipeline?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f60a00a0-3ba2-46c7-8cad-cd05df08c6af": {"__data__": {"id_": "f60a00a0-3ba2-46c7-8cad-cd05df08c6af", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "3": {"node_id": "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "node_type": null, "metadata": {}, "hash": "c0cbcb30decd9ba24c155ab3f966b8dc08e85f1ae9f99ab81a2b51457b8a64df"}, "4": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}}, "hash": "9cffdfb14d878e50776c5de0f52510f16f4d348eb80a2cabc41f6660265fd71e", "text": "Read about high-level concepts\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks:\n\nAn example of using a node postprocessors is below:\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Usage Pattern\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\nYou can find more details using post processors and how to build your own below.File Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/filter nodes below 0.75 similarity score\nLinks:\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/root.md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks:\n\nBelow you can find guides for each node postprocessor.File Name: ./docs/core_modules/query_modules/node_postprocessors/root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17": {"__data__": {"id_": "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "2": {"node_id": "f60a00a0-3ba2-46c7-8cad-cd05df08c6af", "node_type": null, "metadata": {}, "hash": "9cffdfb14d878e50776c5de0f52510f16f4d348eb80a2cabc41f6660265fd71e"}, "3": {"node_id": "edd49718-78ae-4333-94e6-2ffda9a60166", "node_type": null, "metadata": {}, "hash": "fe690d42f5aaee8838ac0f5b002f0da4509c7701e64b84ecd0bd67ae4d479cd1"}, "4": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}}, "hash": "c0cbcb30decd9ba24c155ab3f966b8dc08e85f1ae9f99ab81a2b51457b8a64df", "text": "md\nContent Type: text\nHeader Path: Node Postprocessor/Modules\nLinks:\n\n---\nmaxdepth: 2\n---\nmodules.md\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern\nLinks:\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.File Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/all node post-processors will be applied during each query\n\n```python\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.postprocessor import TimeWeightedPostprocessor\n\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\nindex = VectorStoreIndex.from_documents(documents)\n\nquery_engine = index.as_query_engine(\n  node_postprocessors=[\n    TimeWeightedPostprocessor(\n        time_decay=0.5, time_access_refresh=False, top_k=1\n    )\n  ]\n)\n\nresponse = query_engine.query(\"query string\")\n```\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks:\n\nOr used as a standalone object for filtering retrieved nodes:\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with Retrieved Nodes\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\n\nnodes = index.as_retriever().query(\"query string\")\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "edd49718-78ae-4333-94e6-2ffda9a60166": {"__data__": {"id_": "edd49718-78ae-4333-94e6-2ffda9a60166", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "2": {"node_id": "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "node_type": null, "metadata": {}, "hash": "c0cbcb30decd9ba24c155ab3f966b8dc08e85f1ae9f99ab81a2b51457b8a64df"}, "3": {"node_id": "c907930f-94ca-477b-9bea-a98af3354b65", "node_type": null, "metadata": {}, "hash": "4dd43a1f5e676e1acf02041380d771bb6bf36ad7f0248e4a97292232884d160c"}, "4": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}}, "hash": "fe690d42f5aaee8838ac0f5b002f0da4509c7701e64b84ecd0bd67ae4d479cd1", "text": "/docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks:\n\nAs you may have noticed, the postprocessors take `NodeWithScore` objects as inputs, which is just a wrapper class with a `Node` and a `score` value.File Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Using with your own nodes\nLinks:\n\nfrom llama_index.indices.postprocessor import SimilarityPostprocessor\nfrom llama_index.schema import Node, NodeWithScore\n\nnodes = [\n  NodeWithScore(node=Node(text=\"text\"), score=0.7),\n  NodeWithScore(node=Node(text=\"text\"), score=0.8)\n]\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/filter nodes below 0.75 similarity score\nLinks:\n\nprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)\nfiltered_nodes = processor.postprocess_nodes(nodes)\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nThe base class is `BaseNodePostprocessor`, and the API interface is very simple:\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c907930f-94ca-477b-9bea-a98af3354b65": {"__data__": {"id_": "c907930f-94ca-477b-9bea-a98af3354b65", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}, "2": {"node_id": "edd49718-78ae-4333-94e6-2ffda9a60166", "node_type": null, "metadata": {}, "hash": "fe690d42f5aaee8838ac0f5b002f0da4509c7701e64b84ecd0bd67ae4d479cd1"}, "4": {"node_id": "35f742cf-4dde-4ace-9e1e-d3074523760f", "node_type": null, "metadata": {}, "hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15"}}, "hash": "4dd43a1f5e676e1acf02041380d771bb6bf36ad7f0248e4a97292232884d160c", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nclass BaseNodePostprocessor:\n    \"\"\"Node postprocessor.\"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb": {"__data__": {"id_": "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}, "3": {"node_id": "bf039726-614d-4d53-b16f-d119e7b18ae7", "node_type": null, "metadata": {}, "hash": "7daf57d6e5849bd6e98d3818c87c6d8107eed501fe1a84bc4828a688e59e1268"}, "4": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}}, "hash": "1fb85872f4367af977decfaf90775641d425a30c48ace67ce2262bf9ee0b1aa6", "text": "@abstractmethod\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \"\"\"Postprocess nodes.\"\"\"File Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nA dummy node-postprocessor can be implemented in just a few lines of code:\n\nFile Name: ./docs/core_modules/query_modules/node_postprocessors/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Node PostProcessor\nLinks:\n\nfrom llama_index import QueryBundle\nfrom llama_index.indices.postprocessor.base import BaseNodePostprocessor\nfrom llama_index.schema import NodeWithScore\n\nclass DummyNodePostprocessor:\n\n    def postprocess_nodes(\n        self, nodes: List[NodeWithScore], query_bundle: Optional[QueryBundle]\n    ) -> List[NodeWithScore]:\n        \n        # subtracts 1 from the score\n        for n in nodes:\n            n.score -= 1\n\n        return nodes\n\nFile Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations\nLinks:\n\nLlamaIndex allows you to perform *query transformations* over your index structures.Query transformations are modules that will convert a query into another query.They can be **single-step**, as in the transformation is run once before the query is executed against an index.They can also be **multi-step**, as in: \n1.The query is transformed, executed against an index, \n2.The response is retrieved.3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf039726-614d-4d53-b16f-d119e7b18ae7": {"__data__": {"id_": "bf039726-614d-4d53-b16f-d119e7b18ae7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}, "2": {"node_id": "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb", "node_type": null, "metadata": {}, "hash": "1fb85872f4367af977decfaf90775641d425a30c48ace67ce2262bf9ee0b1aa6"}, "3": {"node_id": "bcc6f32f-9b05-4747-bcff-29b82efce87d", "node_type": null, "metadata": {}, "hash": "82e8070f7441f219bd41ec73e6a1c1eaf7f2cde7fd12fc25b5e22345ab51bab2"}, "4": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}}, "hash": "7daf57d6e5849bd6e98d3818c87c6d8107eed501fe1a84bc4828a688e59e1268", "text": "Subsequent queries are transformed/executed in a sequential fashion.We list some of our query transformations in more detail below.File Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Use Cases\nLinks:\n\nQuery transformations have multiple use cases:\n- Transforming an initial query into a form that can be more easily embedded (e.g.HyDE)\n- Transforming an initial query into a subquestion that can be more easily answered from the data (single-step query decomposition)\n- Breaking an initial query into multiple subquestions that can be more easily answered on their own.(multi-step query decomposition)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks:\n\nHyDE is a technique where given a natural language query, a hypothetical document/answer is generated first.This hypothetical document is then used for embedding lookup rather than the raw query.To use HyDE, an example code snippet is shown below.File Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/HyDE (Hypothetical Document Embeddings)\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.indices.query.query_transform.base import HyDEQueryTransform\nfrom llama_index.query_engine.transform_query_engine import TransformQueryEngine\n\nFile Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents)\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bcc6f32f-9b05-4747-bcff-29b82efce87d": {"__data__": {"id_": "bcc6f32f-9b05-4747-bcff-29b82efce87d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}, "2": {"node_id": "bf039726-614d-4d53-b16f-d119e7b18ae7", "node_type": null, "metadata": {}, "hash": "7daf57d6e5849bd6e98d3818c87c6d8107eed501fe1a84bc4828a688e59e1268"}, "4": {"node_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "node_type": null, "metadata": {}, "hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a"}}, "hash": "82e8070f7441f219bd41ec73e6a1c1eaf7f2cde7fd12fc25b5e22345ab51bab2", "text": "/docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks:\n\nquery_str = \"what did paul graham do after going to RISD\"\nhyde = HyDEQueryTransform(include_original=True)\nquery_engine = index.as_query_engine()\nquery_engine = TransformQueryEngine(query_engine, query_transform=hyde)\nresponse = query_engine.query(query_str)\nprint(response)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/run query with HyDE query transform\nLinks:\n\nCheck out our example notebook for a full walkthrough.File Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Single-Step Query Decomposition\nLinks:\n\nSome recent approaches (e.g.self-ask, ReAct) have suggested that LLM's \nperform better at answering complex questions when they break the question into smaller steps.We have found that this is true for queries that require knowledge augmentation as well.If your query is complex, different parts of your knowledge base may answer different \"subqueries\" around the overall query.Our single-step query decomposition feature transforms a **complicated** question into a simpler one over the data collection to help provide a sub-answer to the original question.This is especially helpful over a composed graph.Within a composed graph, a query can be routed to multiple subindexes, each representing a subset of the overall knowledge corpus.Query decomposition allows us to transform the query into a more suitable question over any given index.An example image is shown below.!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51c9d9e2-4889-4b34-84c8-76cc71f93b3f": {"__data__": {"id_": "51c9d9e2-4889-4b34-84c8-76cc71f93b3f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}, "3": {"node_id": "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "node_type": null, "metadata": {}, "hash": "36ac76a16f0bb5e758a3a01d9144c2f0a166db3bccf60d27529c5d7a733ed5ba"}, "4": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}}, "hash": "d1753be1bb8e0df13dbeb1735c83414ef33647dd7c293075158ad286bd3e4dd7", "text": "Here's a corresponding example code snippet over a composed graph.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/llm_predictor_chatgpt corresponds to the ChatGPT LLM interface\nLinks:\n\nfrom llama_index.indices.query.query_transform.base import DecomposeQueryTransform\ndecompose_transform = DecomposeQueryTransform(\n    llm_predictor_chatgpt, verbose=True\n)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/initialize indexes and graph\nLinks:\n\n...File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/configure retrievers\nLinks:\n\nvector_query_engine = vector_index.as_query_engine()\nvector_query_engine = TransformQueryEngine(\n    vector_query_engine, \n    query_transform=decompose_transform\n    transform_extra_info={'index_summary': vector_index.index_struct.summary}\n)\ncustom_query_engines = {\n    vector_index.index_id: vector_query_engine\n}\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks:\n\nquery_str = (\n    \"Compare and contrast the airports in Seattle, Houston, and Toronto. \"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6a5c70d2-97d7-4b6b-8cb3-f58187568bda": {"__data__": {"id_": "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}, "2": {"node_id": "51c9d9e2-4889-4b34-84c8-76cc71f93b3f", "node_type": null, "metadata": {}, "hash": "d1753be1bb8e0df13dbeb1735c83414ef33647dd7c293075158ad286bd3e4dd7"}, "3": {"node_id": "25316757-07dd-4e6d-9f0f-6f96b9cf63e2", "node_type": null, "metadata": {}, "hash": "5f348a00cdcb3373aa777965ac3e0a504e23d589f5c11dfa236c3867a1b100bb"}, "4": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}}, "hash": "36ac76a16f0bb5e758a3a01d9144c2f0a166db3bccf60d27529c5d7a733ed5ba", "text": ")\nquery_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)\nresponse = query_engine.query(query_str)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/query\nLinks:\n\nCheck out our example notebook for a full walkthrough.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks:\n\nMulti-step query transformations are a generalization on top of existing single-step query transformation approaches.Given an initial, complex query, the query is transformed and executed against an index.The response is retrieved from the query.Given the response (along with prior responses) and the query, followup questions may be asked against the index as well.This technique allows a query to be run against a single knowledge source until that query has satisfied all questions.An example image is shown below.!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25316757-07dd-4e6d-9f0f-6f96b9cf63e2": {"__data__": {"id_": "25316757-07dd-4e6d-9f0f-6f96b9cf63e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}, "2": {"node_id": "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "node_type": null, "metadata": {}, "hash": "36ac76a16f0bb5e758a3a01d9144c2f0a166db3bccf60d27529c5d7a733ed5ba"}, "4": {"node_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb", "node_type": null, "metadata": {}, "hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681"}}, "hash": "5f348a00cdcb3373aa777965ac3e0a504e23d589f5c11dfa236c3867a1b100bb", "text": "Here's a corresponding example code snippet.File Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/Multi-Step Query Transformations\nLinks:\n\nfrom llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\nstep_decompose_transform = StepDecomposeQueryTransform(\n    llm_predictor, verbose=True\n)\n\nquery_engine = index.as_query_engine()\nquery_engine = MultiStepQueryEngine(query_engine, query_transform=step_decompose_transform)\n\nresponse = query_engine.query(\n    \"Who was in the first batch of the accelerator program the author started?\",\n)\nprint(str(response))\n\nFile Name: ../docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\nCheck out our example notebook for a full walkthrough.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61b30575-d871-460a-af27-cd41e98c2909": {"__data__": {"id_": "61b30575-d871-460a-af27-cd41e98c2909", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "3": {"node_id": "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "node_type": null, "metadata": {}, "hash": "fd9284a928c4c00541e4a594c9799b619cd2f9efcb9edc608c0c3698d3b30308"}, "4": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}}, "hash": "16f3f7c84aec55a644fb8ea155e96dbd192b7b7d79d0b53f7bd4e4b88958fc48", "text": "File Name: ./docs/core_modules/query_modules/query_engine/advanced/query_transformations.md\nContent Type: text\nHeader Path: Query Transformations/gpt-4\nLinks:\n\n---\ncaption: Examples\nmaxdepth: 1\n---\n/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n\nFile Name: ./docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Basic\n\n```{toctree}\n---\nmaxdepth: 1\n---\nRetriever Query Engine </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Structured & Semi-Structured Data\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/json_query_engine.ipynb\n/examples/query_engine/pandas_query_engine.ipynb\n/examples/query_engine/knowledge_graph_query_engine.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Advanced\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/RouterQueryEngine.ipynb\n/examples/query_engine/RetrieverRouterQueryEngine.ipynb\n/examples/query_engine/JointQASummary.ipynb\n/examples/query_engine/sub_question_query_engine.ipynb\n/examples/query_transformations/SimpleIndexDemo-multistep.ipynb\n/examples/query_engine/SQLRouterQueryEngine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "27dbd541-35f6-4db6-954f-78e6bd9c7f78": {"__data__": {"id_": "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "2": {"node_id": "61b30575-d871-460a-af27-cd41e98c2909", "node_type": null, "metadata": {}, "hash": "16f3f7c84aec55a644fb8ea155e96dbd192b7b7d79d0b53f7bd4e4b88958fc48"}, "3": {"node_id": "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "node_type": null, "metadata": {}, "hash": "e0549233d2c4d811c7d20b71c081249f9cef80636d7d8dfa8466a9f4a6e2a8f7"}, "4": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}}, "hash": "fd9284a928c4c00541e4a594c9799b619cd2f9efcb9edc608c0c3698d3b30308", "text": "ipynb\n/examples/query_engine/SQLAutoVectorQueryEngine.ipynb\n/examples/query_engine/SQLJoinQueryEngine.ipynb\n/examples/index_structs/struct_indices/duckdb_sql_query.ipynb\nRetry Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Source Query Engine </examples/evaluation/RetryQuery.ipynb>\nRetry Guideline Query Engine </examples/evaluation/RetryQuery.ipynb>\n/examples/query_engine/citation_query_engine.ipynb\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/modules.md\nContent Type: code\nHeader Path: Module Guides/Experimental\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/query_engine/flare_query_engine.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/response_modes.md\nContent Type: text\nHeader Path: Response Modes\n\nRight now, we support the following options:\n- `default`: \"create and refine\" an answer by sequentially going through each retrieved `Node`; \n    This makes a separate LLM call per Node.Good for more detailed answers.- `compact`: \"compact\" the prompt during each LLM call by stuffing as \n    many `Node` text chunks that can fit within the maximum prompt size.If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple prompts.- `tree_summarize`: Given a set of `Node` objects and the query, recursively construct a tree \n    and return the root node as the response.Good for summarization purposes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9": {"__data__": {"id_": "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "2": {"node_id": "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "node_type": null, "metadata": {}, "hash": "fd9284a928c4c00541e4a594c9799b619cd2f9efcb9edc608c0c3698d3b30308"}, "3": {"node_id": "2a5b69df-3782-4a44-8674-fcc4a9fee8c4", "node_type": null, "metadata": {}, "hash": "e5d484c3d06c22948040a467fd5b4fcfdc6ea2991af7f0ab8e62556624a81fc7"}, "4": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}}, "hash": "e0549233d2c4d811c7d20b71c081249f9cef80636d7d8dfa8466a9f4a6e2a8f7", "text": "- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them.Then can be inspected by checking `response.source_nodes`.The response object is covered in more detail in Section 5.- `accumulate`: Given a set of `Node` objects and the query, apply the query to each `Node` text\n    chunk while accumulating the responses into an array.Returns a concatenated string of all\n    responses.Good for when you need to run the same query separately against each text\n    chunk.See Response Synthesizer to learn more.File Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks:\n\nQuery engine is a generic interface that allows you to ask question over your data.A query engine takes in a natural language query, and returns a rich response.It is most often (but not always) built on one or many Indices via Retrievers.You can compose multiple query engines to achieve more advanced capability.File Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Concept\nLinks:\n\nIf you want to have a conversation with your data (multiple back-and-forth instead of a single question & answer), take a look at Chat Engine\n\nFile Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine()\nresponse = query_engine.query(\"Who is Paul Graham.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2a5b69df-3782-4a44-8674-fcc4a9fee8c4": {"__data__": {"id_": "2a5b69df-3782-4a44-8674-fcc4a9fee8c4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}, "2": {"node_id": "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "node_type": null, "metadata": {}, "hash": "e0549233d2c4d811c7d20b71c081249f9cef80636d7d8dfa8466a9f4a6e2a8f7"}, "4": {"node_id": "694198a8-926a-4f24-b121-897dd6ebd176", "node_type": null, "metadata": {}, "hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc"}}, "hash": "e5d484c3d06c22948040a467fd5b4fcfdc6ea2991af7f0ab8e62556624a81fc7", "text": "\")File Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nTo stream response:\n\nFile Name: ../docs/core_modules/query_modules/query_engine/root.md\nContent Type: text\nHeader Path: Query Engine/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine(streaming=True)\nstreaming_response = query_engine.query(\"Who is Paul Graham.\")", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c12ef17-17b3-47f6-87b8-c92e1391bfb7": {"__data__": {"id_": "6c12ef17-17b3-47f6-87b8-c92e1391bfb7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "3": {"node_id": "e4737417-44d7-46eb-b775-58b673b79554", "node_type": null, "metadata": {}, "hash": "3627c85268c3748ed861ea7a144637f3b1f29c65f3967f3fb1f3ada6c758395f"}, "4": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}}, "hash": "9d8e5e06f4f0a3a3ba8f5586c4c57ef6b7486e6dc4aeb12e551086f3f3e69366", "text": "streaming_response.print_response_stream()\n\nFile Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Modules\n\n```{toctree}\n---\nmaxdepth: 3\n---\nmodules.md\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/root.md\nContent Type: code\nHeader Path: Query Engine/Supporting Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\nsupporting_modules.md\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming\nLinks:\n\nLlamaIndex supports streaming the response as it's being generated.This allows you to start printing or processing the beginning of the response before the full response is finished.This can drastically reduce the perceived latency of queries.File Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nTo enable streaming, you need to use an LLM that supports streaming.Right now, streaming is supported by `OpenAI`, `HuggingFaceLLM`, and most LangChain LLMs (via `LangChainLLM`).Configure query engine to use streaming:\n\nIf you are using the high-level API, set `streaming=True` when building a query engine.File Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nquery_engine = index.as_query_engine(\n    streaming=True,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4737417-44d7-46eb-b775-58b673b79554": {"__data__": {"id_": "e4737417-44d7-46eb-b775-58b673b79554", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "2": {"node_id": "6c12ef17-17b3-47f6-87b8-c92e1391bfb7", "node_type": null, "metadata": {}, "hash": "9d8e5e06f4f0a3a3ba8f5586c4c57ef6b7486e6dc4aeb12e551086f3f3e69366"}, "3": {"node_id": "d8f3c932-6c3a-4538-9bd2-192933dc2055", "node_type": null, "metadata": {}, "hash": "91d403b091e2be9376485bfd5b7fc9ead09b5708442883ec0056483382010cd5"}, "4": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}}, "hash": "3627c85268c3748ed861ea7a144637f3b1f29c65f3967f3fb1f3ada6c758395f", "text": "similarity_top_k=1\n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nIf you are using the low-level API to compose the query engine,\npass `streaming=True` when constructing the `Response Synthesizer`:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Setup\nLinks:\n\nfrom llama_index import get_response_synthesizer\nsynth = get_response_synthesizer(streaming=True, .)\nquery_engine = RetrieverQueryEngine(response_synthesizer=synth, .)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nAfter properly configuring both the LLM and the query engine,\ncalling `query` now returns a `StreamingResponse` object.File Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nThe response is returned immediately when the LLM call *starts*, without having to wait for the full completion.> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.You can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8f3c932-6c3a-4538-9bd2-192933dc2055": {"__data__": {"id_": "d8f3c932-6c3a-4538-9bd2-192933dc2055", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "2": {"node_id": "e4737417-44d7-46eb-b775-58b673b79554", "node_type": null, "metadata": {}, "hash": "3627c85268c3748ed861ea7a144637f3b1f29c65f3967f3fb1f3ada6c758395f"}, "3": {"node_id": "4baab847-43bf-4430-a9bf-05f0c12d9b4d", "node_type": null, "metadata": {}, "hash": "0fdbefc843ac4b7186fd1cd00f56ac63f797611247df783e7a52fef6ac6e4781"}, "4": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}}, "hash": "91d403b091e2be9376485bfd5b7fc9ead09b5708442883ec0056483382010cd5", "text": "md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nfor text in streaming_response.response_gen:\n    # do something with text as they arrive.File Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nAlternatively, if you just want to print the text as they arrive:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\nLinks:\n\nstreaming_response.print_response_stream()\n\nFile Name: ./docs/core_modules/query_modules/query_engine/streaming.md\nContent Type: text\nHeader Path: Streaming/Streaming Response\n\nSee an end-to-end example\n\nFile Name: ./docs/core_modules/query_modules/query_engine/supporting_modules.md\nContent Type: code\nHeader Path: Supporting Modules\n\n```{toctree}\n---\nmaxdepth: 1\n---\nadvanced/query_transformations.md\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nBuild a query engine from index:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nquery_engine = index.as_query_engine()\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index, see Index\n```\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4baab847-43bf-4430-a9bf-05f0c12d9b4d": {"__data__": {"id_": "4baab847-43bf-4430-a9bf-05f0c12d9b4d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}, "2": {"node_id": "d8f3c932-6c3a-4538-9bd2-192933dc2055", "node_type": null, "metadata": {}, "hash": "91d403b091e2be9376485bfd5b7fc9ead09b5708442883ec0056483382010cd5"}, "4": {"node_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "node_type": null, "metadata": {}, "hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2"}}, "hash": "0fdbefc843ac4b7186fd1cd00f56ac63f797611247df783e7a52fef6ac6e4781", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nAsk a question over your data\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nresponse = query_engine.query('Who is Paul Graham?')", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a99d060e-119d-49e2-8d50-3781092ad15c": {"__data__": {"id_": "a99d060e-119d-49e2-8d50-3781092ad15c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}, "3": {"node_id": "78a64c63-ffca-43dc-82f0-566a375395c6", "node_type": null, "metadata": {}, "hash": "bd5cb0ac9e40b3538070334336607ee82c78c1fb9590922fa7adb1fd18d8ecec"}, "4": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}}, "hash": "72c44d847f39678cc9526549dc74c37c8646fdf27ef648bf52f3ef754a3486ef", "text": "File Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\nYou can directly build and configure a query engine from an index in 1 line of code:\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\nquery_engine = index.as_query_engine(\n    response_mode='tree_summarize',\n    verbose=True,\n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\n> Note: While the high-level API optimizes for ease-of-use, it does *NOT* expose full range of configurability.See **Response Modes** for a full list of response modes and what they do.File Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/High-Level API\nLinks:\n\n---\nmaxdepth: 1\nhidden:\n---\nresponse_modes.md\nstreaming.md\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.Concretely speaking, you would explicitly construct a `QueryEngine` object instead of calling `index.as_query_engine(.)`.> Note: You may need to look at API references or example notebooks.File Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "78a64c63-ffca-43dc-82f0-566a375395c6": {"__data__": {"id_": "78a64c63-ffca-43dc-82f0-566a375395c6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}, "2": {"node_id": "a99d060e-119d-49e2-8d50-3781092ad15c", "node_type": null, "metadata": {}, "hash": "72c44d847f39678cc9526549dc74c37c8646fdf27ef648bf52f3ef754a3486ef"}, "3": {"node_id": "5171b641-9ca6-48a0-bde6-d47d021f3c54", "node_type": null, "metadata": {}, "hash": "099f46d80276e105d56c11faacbb4c32af70c9b71864369b910ac9502c08f0e8"}, "4": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}}, "hash": "bd5cb0ac9e40b3538070334336607ee82c78c1fb9590922fa7adb1fd18d8ecec", "text": "md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Low-Level Composition API\nLinks:\n\nfrom llama_index import (\n    VectorStoreIndex,\n    get_response_synthesizer,\n)\nfrom llama_index.retrievers import VectorIndexRetriever\nfrom llama_index.query_engine import RetrieverQueryEngine\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/build index\nLinks:\n\nindex = VectorStoreIndex.from_documents(documents)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure retriever\nLinks:\n\nretriever = VectorIndexRetriever(\n    index=index, \n    similarity_top_k=2,\n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/configure response synthesizer\nLinks:\n\nresponse_synthesizer = get_response_synthesizer(\n    response_mode=\"tree_summarize\",\n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/assemble query engine\nLinks:\n\nquery_engine = RetrieverQueryEngine(\n    retriever=retriever,\n    response_synthesizer=response_synthesizer,\n)\n\nFile Name: ./docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/query\nLinks:\n\nresponse = query_engine.query(\"What did the author do growing up?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5171b641-9ca6-48a0-bde6-d47d021f3c54": {"__data__": {"id_": "5171b641-9ca6-48a0-bde6-d47d021f3c54", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}, "2": {"node_id": "78a64c63-ffca-43dc-82f0-566a375395c6", "node_type": null, "metadata": {}, "hash": "bd5cb0ac9e40b3538070334336607ee82c78c1fb9590922fa7adb1fd18d8ecec"}, "4": {"node_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "node_type": null, "metadata": {}, "hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd"}}, "hash": "099f46d80276e105d56c11faacbb4c32af70c9b71864369b910ac9502c08f0e8", "text": "\")print(response)\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks:\n\nTo enable streaming, you simply need to pass in a `streaming=True` flag\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\nLinks:\n\nquery_engine = index.as_query_engine(\n    streaming=True,\n)\nstreaming_response = query_engine.query(\n    \"What did the author do growing up?\", \n)\nstreaming_response.print_response_stream()\n\nFile Name: ../docs/core_modules/query_modules/query_engine/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring a Query Engine/Streaming\n\n* Read the full streaming guide\n* See an end-to-end example\n\nFile Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide\nLinks:\n\nDetailed inputs/outputs for each response synthesizer are found below.File Name: ../docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks:\n\nThe following shows the setup for utilizing all kwargs.- `response_mode` specifies which response synthesizer to use\n- `service_context` defines the LLM and related settings for synthesis\n- `text_qa_template` and `refine_template` are the prompts used at various stages\n- `use_async` is used for only the `tree_summarize` response mode right now, to asynchronously build the summary tree\n- `streaming` configures whether to return a streaming response object or not\n\nIn the `synthesize`/`asyntheszie` functions, you can optionally provide additional source nodes, which will be added to the `response.source_nodes` list.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8316e5ec-9820-47ce-9a03-0a5902c9bd70": {"__data__": {"id_": "8316e5ec-9820-47ce-9a03-0a5902c9bd70", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}, "3": {"node_id": "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "node_type": null, "metadata": {}, "hash": "8a4fde3c0393191e71e36a6a9eeb0742b89d25f6c232fed0a6bfe5489275eda1"}, "4": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}}, "hash": "2cd55610a036436819ffbd3c5e95d45ea799487ed09d0e15715e7bfb7be5d491", "text": "File Name: ./docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/API Example\nLinks:\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(\n  response_mode=\"refine\",\n  service_context=service_context,\n  text_qa_template=text_qa_template,\n  refine_template=refine_template,\n  use_async=False,\n  streaming=False\n)\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/synchronous\nLinks:\n\nresponse = response_synthesizer.synthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .], \n)\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nresponse = await response_synthesizer.asynthesize(\n  \"query string\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .],\n  additional_source_nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .], \n)\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nYou can also directly return a string, using the lower-level `get_response` and `aget_response` functions\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5e8bf41b-f072-4f1e-a2c9-367cee63d816": {"__data__": {"id_": "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}, "2": {"node_id": "8316e5ec-9820-47ce-9a03-0a5902c9bd70", "node_type": null, "metadata": {}, "hash": "2cd55610a036436819ffbd3c5e95d45ea799487ed09d0e15715e7bfb7be5d491"}, "3": {"node_id": "a1c28ad7-78bf-4afb-b187-ab49cae10939", "node_type": null, "metadata": {}, "hash": "6d3cc1150e1d1f8cf08348b84a2f68d099212f4e6633b10a4795192451c06cdf"}, "4": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}}, "hash": "8a4fde3c0393191e71e36a6a9eeb0742b89d25f6c232fed0a6bfe5489275eda1", "text": "/docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: text\nHeader Path: Module Guide/asynchronous\nLinks:\n\nresponse_str = response_synthesizer.get_response(\n  \"query string\", \n  text_chunks=[\"text1\", \"text2\", .]\n)\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/modules.md\nContent Type: code\nHeader Path: Module Guide/Example Notebooks\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/response_synthesizers/refine.ipynb\n/examples/response_synthesizers/tree_summarize.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks:\n\nA `Response Synthesizer` is what generates a response from an LLM, using a user query and a given set of text chunks.The output of a response synthesizer is a `Response` object.The method for doing this can take many forms, from as simple as iterating over text chunks, to as complex as building a tree.The main idea here is to simplify the process of generating a response using an LLM across your data.When used in a query engine, the response synthesizer is used after nodes are retrieved from a retriever, and after any node-postprocessors are ran.File Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Concept\nLinks:\n\nConfused about where response synthesizer fits in the pipeline?Read the high-level concepts\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nUse a response synthesizer on it's own:\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1c28ad7-78bf-4afb-b187-ab49cae10939": {"__data__": {"id_": "a1c28ad7-78bf-4afb-b187-ab49cae10939", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}, "2": {"node_id": "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "node_type": null, "metadata": {}, "hash": "8a4fde3c0393191e71e36a6a9eeb0742b89d25f6c232fed0a6bfe5489275eda1"}, "4": {"node_id": "d4bec885-2ac4-4e51-b783-7835e4181f96", "node_type": null, "metadata": {}, "hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee"}}, "hash": "6d3cc1150e1d1f8cf08348b84a2f68d099212f4e6633b10a4795192451c06cdf", "text": "/docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nfrom llama_index.schema import Node\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\"query text\", nodes=[Node(text=\"text\"), .])\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nOr in a query engine after you've created an index:\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\nYou can find more details on all available response synthesizers, modes, and how to build your own below.File Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Usage Pattern\nLinks:\n\n---\nmaxdepth: 2\n---\nusage_pattern.md\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks:\n\nBelow you can find detailed API information for each response synthesis module.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27": {"__data__": {"id_": "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}, "3": {"node_id": "f2850e09-1c16-403b-93dd-9b73f989a8f5", "node_type": null, "metadata": {}, "hash": "8bd18e5f22a151b467ed1000b315084968cba810ecf92c83b7b06f70fa27f263"}, "4": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}}, "hash": "3ee763f8403d7ca6d8b237a566ba334f767091af21fbb5351b0a847760cd5f1b", "text": "File Name: ./docs/core_modules/query_modules/response_synthesizers/root.md\nContent Type: text\nHeader Path: Response Synthesizer/Modules\nLinks:\n\n---\nmaxdepth: 1\n---\nmodules.md\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nConfiguring the response synthesizer for a query engine using `response_mode`:\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nfrom llama_index.schema import Node, NodeWithScore\nfrom llama_index.response_synthesizers import get_response_synthesizer\n\nresponse_synthesizer = get_response_synthesizer(response_mode='compact')\n\nresponse = response_synthesizer.synthesize(\n  \"query text\", \n  nodes=[NodeWithScore(node=Node(text=\"text\"), score=1.0), .]\n)\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nOr, more commonly, in a query engine after you've created an index:\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nquery_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\nresponse = query_engine.query(\"query_text\")\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/Get Started\n\n```{tip}\nTo learn how to build an index,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f2850e09-1c16-403b-93dd-9b73f989a8f5": {"__data__": {"id_": "f2850e09-1c16-403b-93dd-9b73f989a8f5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}, "2": {"node_id": "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27", "node_type": null, "metadata": {}, "hash": "3ee763f8403d7ca6d8b237a566ba334f767091af21fbb5351b0a847760cd5f1b"}, "3": {"node_id": "17a940a6-e716-499c-9d83-542c33d3f97c", "node_type": null, "metadata": {}, "hash": "e22bb61dfa5d5ac930b7669bdade19f4ea8fd18d8cdc6dce23351b13fa86c3d7"}, "4": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}}, "hash": "8bd18e5f22a151b467ed1000b315084968cba810ecf92c83b7b06f70fa27f263", "text": "see Index\n```\n\nFile Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Configuring the Response Mode\nLinks:\n\nResponse synthesizers are typically specified through a `response_mode` kwarg setting.Several response synthesizers are implemented already in LlamaIndex:\n\n- `refine`: \"create and refine\" an answer by sequentially going through each retrieved text chunk.This makes a separate LLM call per Node.Good for more detailed answers.- `compact` (default): \"compact\" the prompt during each LLM call by stuffing as \n    many text chunks that can fit within the maximum prompt size.If there are \n    too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n    multiple compact prompts.The same as `refine`, but should result in less LLM calls.- `tree_summarize`: Given a set of text chunks and the query, recursively construct a tree \n    and return the root node as the response.Good for summarization purposes.- `simple_summarize`: Truncates all text chunks to fit into a single LLM prompt.Good for quick\n    summarization purposes, but may lose detail due to truncation.- `no_text`: Only runs the retriever to fetch the nodes that would have been sent to the LLM, \n    without actually sending them.Then can be inspected by checking `response.source_nodes`.- `accumulate`: Given a set of text chunks and the query, apply the query to each text\n    chunk while accumulating the responses into an array.Returns a concatenated string of all\n    responses.Good for when you need to run the same query separately against each text\n    chunk.- `compact_accumulate`: The same as accumulate, but will \"compact\" each LLM prompt similar to\n    `compact`, and run the same query against each text chunk.File Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17a940a6-e716-499c-9d83-542c33d3f97c": {"__data__": {"id_": "17a940a6-e716-499c-9d83-542c33d3f97c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}, "2": {"node_id": "f2850e09-1c16-403b-93dd-9b73f989a8f5", "node_type": null, "metadata": {}, "hash": "8bd18e5f22a151b467ed1000b315084968cba810ecf92c83b7b06f70fa27f263"}, "4": {"node_id": "26782402-9635-482e-b626-75e58b3de6af", "node_type": null, "metadata": {}, "hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138"}}, "hash": "e22bb61dfa5d5ac930b7669bdade19f4ea8fd18d8cdc6dce23351b13fa86c3d7", "text": "/docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks:\n\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`.The base API is extremely simple, which makes it easy to create your own response synthesizer.Maybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it's own.Below we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement.The basic requirements are to process a query and text chunks, and return a string (or string generator) response.File Name: ./docs/core_modules/query_modules/response_synthesizers/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Custom Response Synthesizers\nLinks:\n\nclass BaseSynthesizer(ABC):\n    \"\"\"Response builder class.\"\"\"def __init__(\n        self,\n        service_context: Optional[ServiceContext] = None,\n        streaming: bool = False,\n    ) -> None:\n        \"\"\"Init params.\"\"\"self._service_context = service_context or ServiceContext.from_defaults()\n        self._callback_manager = self._service_context.callback_manager\n        self._streaming = streaming\n\n    @abstractmethod\n    def get_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "05d0a4b8-9787-412e-9ab8-92bdcec12d1e": {"__data__": {"id_": "05d0a4b8-9787-412e-9ab8-92bdcec12d1e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}, "3": {"node_id": "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "node_type": null, "metadata": {}, "hash": "fc88305a4fedc81a3108d9b3646098eb9aa680c1aabd1c52062298497e0c4f0b"}, "4": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}}, "hash": "0173af05b63014a0fa9fe4fe78768f125b690c2db01ad72e4eedd1e1ff6b0a57", "text": "...\n\n    @abstractmethod\n    async def aget_response(\n        self,\n        query_str: str,\n        text_chunks: Sequence[str],\n        **response_kwargs: Any,\n    ) -> RESPONSE_TEXT_TYPE:\n        \"\"\"Get response.\"\"\".File Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides\nLinks:\n\nWe are adding more module guides soon!In the meanwhile, please take a look at the API References.File Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks:\n\n* VectorIndexRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Vector Index Retrievers\nLinks:\n\n---\nmaxdepth: 1\n---\nVectorIndexAutoRetriever </examples/vector_stores/chroma_auto_retriever.ipynb>\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/List Index\nLinks:\n\n* ListIndexRetriever \n* ListIndexEmbeddingRetriever \n* ListIndexLLMRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Tree Index\nLinks:\n\n* TreeSelectLeafRetriever\n* TreeSelectLeafEmbeddingRetriever\n* TreeAllLeafRetriever\n* TreeRootRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a066a9ec-d70b-45c3-9f1a-445c9a86aede": {"__data__": {"id_": "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}, "2": {"node_id": "05d0a4b8-9787-412e-9ab8-92bdcec12d1e", "node_type": null, "metadata": {}, "hash": "0173af05b63014a0fa9fe4fe78768f125b690c2db01ad72e4eedd1e1ff6b0a57"}, "3": {"node_id": "7436654d-25ea-4058-9623-c981db612e37", "node_type": null, "metadata": {}, "hash": "557643045b9c72929610dc515e91171a9df1d09c28ba1816d5d03bf587c97f5c"}, "4": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}}, "hash": "fc88305a4fedc81a3108d9b3646098eb9aa680c1aabd1c52062298497e0c4f0b", "text": "md\nContent Type: text\nHeader Path: Module Guides/Keyword Table Index\nLinks:\n\n* KeywordTableGPTRetriever\n* KeywordTableSimpleRetriever\n* KeywordTableRAKERetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: code\nHeader Path: Module Guides/Knowledge Graph Index\n\n```{toctree}\n---\nmaxdepth: 1\n---\nCustom Retriever (KG Index and Vector Store Index) </examples/index_structs/knowledge_graph/KnowledgeGraphIndex_vs_VectorStoreIndex_vs_CustomIndex_combined.ipynb>\n```\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Knowledge Graph Index\nLinks:\n\n* KGTableRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Document Summary Index\nLinks:\n\n* DocumentSummaryIndexRetriever\n* DocumentSummaryIndexEmbeddingRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks:\n\n* TransformRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/modules.md\nContent Type: text\nHeader Path: Module Guides/Composed Retrievers\nLinks:\n\n---\nmaxdepth: 1\n---\n/examples/query_engine/pdf_tables/recursive_retriever.ipynb\n\nFile Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes\nLinks:\n\nHere we show the mapping from `retriever_mode` configuration to the selected retriever class.> Note that `retriever_mode` can mean different thing for different index classes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7436654d-25ea-4058-9623-c981db612e37": {"__data__": {"id_": "7436654d-25ea-4058-9623-c981db612e37", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}, "2": {"node_id": "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "node_type": null, "metadata": {}, "hash": "fc88305a4fedc81a3108d9b3646098eb9aa680c1aabd1c52062298497e0c4f0b"}, "4": {"node_id": "25b2aa80-5f4e-4746-b48d-42097f230452", "node_type": null, "metadata": {}, "hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4"}}, "hash": "557643045b9c72929610dc515e91171a9df1d09c28ba1816d5d03bf587c97f5c", "text": "File Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Vector Index\nLinks:\n\nSpecifying `retriever_mode` has no effect (silently ignored).`vector_index.as_retriever(.)` always returns a VectorIndexRetriever.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4387dad2-8b75-46de-bc1b-3ff7936eb37a": {"__data__": {"id_": "4387dad2-8b75-46de-bc1b-3ff7936eb37a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "3": {"node_id": "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "node_type": null, "metadata": {}, "hash": "4c921f860ffb445c035ef24b476d6b31262e195905bfe401629c31da9a14741d"}, "4": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}}, "hash": "f1048bc9c7dbdd264ae25a82c1e249415c2c6f3cf9c99345e7566b331788ed0d", "text": "File Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/List Index\nLinks:\n\n* `default`: ListIndexRetriever \n* `embedding`: ListIndexEmbeddingRetriever \n* `llm`: ListIndexLLMRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Tree Index\nLinks:\n\n* `select_leaf`: TreeSelectLeafRetriever\n* `select_leaf_embedding`: TreeSelectLeafEmbeddingRetriever\n* `all_leaf`: TreeAllLeafRetriever\n* `root`: TreeRootRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Keyword Table Index\nLinks:\n\n* `default`: KeywordTableGPTRetriever\n* `simple`: KeywordTableSimpleRetriever\n* `rake`: KeywordTableRAKERetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Knowledge Graph Index\nLinks:\n\n* `keyword`: KGTableRetriever\n* `embedding`: KGTableRetriever\n* `hybrid`: KGTableRetriever\n\nFile Name: ./docs/core_modules/query_modules/retriever/retriever_modes.md\nContent Type: text\nHeader Path: Retriever Modes/Document Summary Index\n\n* `default`: DocumentSummaryIndexRetriever\n* `embedding`: DocumentSummaryIndexEmbeddingRetrievers\n\nFile Name: ./docs/core_modules/query_modules/retriever/root.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71": {"__data__": {"id_": "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "2": {"node_id": "4387dad2-8b75-46de-bc1b-3ff7936eb37a", "node_type": null, "metadata": {}, "hash": "f1048bc9c7dbdd264ae25a82c1e249415c2c6f3cf9c99345e7566b331788ed0d"}, "3": {"node_id": "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "node_type": null, "metadata": {}, "hash": "b1facabb9c90f48304de2adcd82f65164f296f3207d5ad09747ace5f84a04b3f"}, "4": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}}, "hash": "4c921f860ffb445c035ef24b476d6b31262e195905bfe401629c31da9a14741d", "text": "md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks:\n\nRetrievers are responsible for fetching the most relevant context given a user query (or chat message).It can be built on top of Indices, but can also be defined independently.It is used as a key building block in Query Engines (and Chat Engines) for retrieving relevant context.File Name: ./docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Concept\nLinks:\n\nConfused about where retriever fits in the pipeline?Read about high-level concepts\n\nFile Name: ./docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks:\n\nGet started with:\n\nFile Name: ./docs/core_modules/query_modules/retriever/root.md\nContent Type: text\nHeader Path: Retriever/Usage Pattern\nLinks:\n\nretriever = index.as_retriever()\nnodes = retriever.retrieve(\"Who is Paul Graham?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7c88a07-1068-4305-aa31-ce7d2ece27ab": {"__data__": {"id_": "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "2": {"node_id": "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "node_type": null, "metadata": {}, "hash": "4c921f860ffb445c035ef24b476d6b31262e195905bfe401629c31da9a14741d"}, "3": {"node_id": "a600037b-6be1-416c-be05-c752abf38ffe", "node_type": null, "metadata": {}, "hash": "c36fe57fd95ade9b737e41dbbbc8b18fe6a83c85ff9414470e7675e68947114a"}, "4": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}}, "hash": "b1facabb9c90f48304de2adcd82f65164f296f3207d5ad09747ace5f84a04b3f", "text": "\")File Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: code\nHeader Path: Retriever/Usage Pattern\n\n```{toctree}\n---\nmaxdepth: 2\n---\nusage_pattern.md\n```\n\nFile Name: ../docs/core_modules/query_modules/retriever/root.md\nContent Type: code\nHeader Path: Retriever/Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\nmodules.md\n```\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nGet a retriever from index:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nretriever = index.as_retriever()\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nRetrieve relevant context for a question:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\nnodes = retriever.retrieve('Who is Paul Graham?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a600037b-6be1-416c-be05-c752abf38ffe": {"__data__": {"id_": "a600037b-6be1-416c-be05-c752abf38ffe", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}, "2": {"node_id": "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "node_type": null, "metadata": {}, "hash": "b1facabb9c90f48304de2adcd82f65164f296f3207d5ad09747ace5f84a04b3f"}, "4": {"node_id": "aae303c7-88a8-425a-baa2-775741a5e48a", "node_type": null, "metadata": {}, "hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d"}}, "hash": "c36fe57fd95ade9b737e41dbbbc8b18fe6a83c85ff9414470e7675e68947114a", "text": "')File Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/Get Started\nLinks:\n\n> Note: To learn how to build an index, see Index\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nYou can select the index-specific retriever class via `retriever_mode`.For example, with a `ListIndex`:\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n)\n\nFile Name: ../docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\nThis creates a ListIndexLLMRetriever on top of the list index.See **Retriever Modes** for a full list of (index-specific) retriever modes\nand the retriever classes they map to.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e83a6bb-1b59-4597-8b68-0af905679267": {"__data__": {"id_": "8e83a6bb-1b59-4597-8b68-0af905679267", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}, "3": {"node_id": "469c53f1-d0e2-40b2-8f54-f84a22474d93", "node_type": null, "metadata": {}, "hash": "7564e066790552ce64be067ad08a95b6136078f91d5bd807be3a1ae7a150ba5b"}, "4": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}}, "hash": "7778d12011847b5667a700609c14bc83795807abcebefad0a039a585ebe7c541", "text": "File Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Selecting a Retriever\nLinks:\n\n---\nmaxdepth: 1\nhidden:\n---\nretriever_modes.md\n\nFile Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks:\n\nIn the same way, you can pass kwargs to configure the selected retriever.> Note: take a look at the API reference for the selected retriever class' constructor parameters for a list of valid kwargs.For example, if we selected the \"llm\" retriever mode, we might do the following:\n\nFile Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Configuring a Retriever\nLinks:\n\nretriever = list_index.as_retriever(\n    retriever_mode='llm',\n    choice_batch_size=5,\n)\n\nFile Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks:\n\nYou can use the low-level composition API if you need more granular control.To achieve the same outcome as above, you can directly import and construct the desired retriever class:\n\nFile Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: text\nHeader Path: Usage Pattern/High-Level API/Low-Level Composition API\nLinks:\n\nfrom llama_index.indices.list import ListIndexLLMRetriever\n\nretriever = ListIndexLLMRetriever(\n    index=list_index,\n    choice_batch_size=5,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "469c53f1-d0e2-40b2-8f54-f84a22474d93": {"__data__": {"id_": "469c53f1-d0e2-40b2-8f54-f84a22474d93", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}, "2": {"node_id": "8e83a6bb-1b59-4597-8b68-0af905679267", "node_type": null, "metadata": {}, "hash": "7778d12011847b5667a700609c14bc83795807abcebefad0a039a585ebe7c541"}, "3": {"node_id": "5c7983b9-f801-4a20-9ddc-321df1a15374", "node_type": null, "metadata": {}, "hash": "ddc83cb3fd24b86fb2da1a814ae51d5e6a42d1f8730252d5f78fa94594e42a05"}, "4": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}}, "hash": "7564e066790552ce64be067ad08a95b6136078f91d5bd807be3a1ae7a150ba5b", "text": ")\n\nFile Name: ./docs/core_modules/query_modules/retriever/usage_pattern.md\nContent Type: code\nHeader Path: Usage Pattern/High-Level API/Advanced\n\n```{toctree}\n---\nmaxdepth: 1\n---\nDefine Custom Retriever </examples/query_engine/CustomRetrievers.ipynb>\n```\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing\nLinks:\n\nLlamaIndex supports integrations with output parsing modules offered\nby other frameworks.These output parsing modules can be used in the following ways:\n- To provide formatting instructions for any prompt / query (through `output_parser.format`)\n- To provide \"parsing\" for LLM outputs (through `output_parser.parse`)\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks:\n\nGuardrails is an open-source Python package for specification/validation/correction of output schemas.See below for a code example.File Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Guardrails\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import GuardrailsOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5c7983b9-f801-4a20-9ddc-321df1a15374": {"__data__": {"id_": "5c7983b9-f801-4a20-9ddc-321df1a15374", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}, "2": {"node_id": "469c53f1-d0e2-40b2-8f54-f84a22474d93", "node_type": null, "metadata": {}, "hash": "7564e066790552ce64be067ad08a95b6136078f91d5bd807be3a1ae7a150ba5b"}, "4": {"node_id": "413527a2-71f2-466f-b6c7-4b480c41b702", "node_type": null, "metadata": {}, "hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960"}}, "hash": "ddc83cb3fd24b86fb2da1a814ae51d5e6a42d1f8730252d5f78fa94594e42a05", "text": "md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\nindex = VectorStoreIndex(documents, chunk_size=512)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define query / output spec\nLinks:\n\nrail_spec = (\"\"\"\n<rail version=\"0.1\">\n\n<output>\n    <list name=\"points\" description=\"Bullet points regarding events in the author's life.\">\n        <object>\n            <string name=\"explanation\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation2\" format=\"one-line\" on-fail-one-line=\"noop\" />\n            <string name=\"explanation3\" format=\"one-line\" on-fail-one-line=\"noop\" />\n        </object>\n    </list>\n</output>\n\n<prompt>\n\nQuery string here.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7e3546c-e36e-4229-a275-24b1eba8a3fd": {"__data__": {"id_": "a7e3546c-e36e-4229-a275-24b1eba8a3fd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "3": {"node_id": "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "node_type": null, "metadata": {}, "hash": "16c3441a8677f30247d9e5fa184585d2335284f4a5b1648bd3f59a670344279d"}, "4": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}}, "hash": "16362627e4a35e5e81416ce8f903a66949c04a7e6f05809df83452c5d9a817ad", "text": "@xml_prefix_prompt\n\n{output_schema}\n\n@json_suffix_prompt_v2_wo_none\n</prompt>\n</rail>\n\"\"\")\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks:\n\noutput_parser = GuardrailsOutputParser.from_rail_string(rail_spec, llm=llm_predictor.llm)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks:\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\n\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are the three items the author did growing up?\", \n)\nprint(response)\n\nFile Name: .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e3d798c1-4d01-4fd8-9725-59dcc73c5320": {"__data__": {"id_": "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "2": {"node_id": "a7e3546c-e36e-4229-a275-24b1eba8a3fd", "node_type": null, "metadata": {}, "hash": "16362627e4a35e5e81416ce8f903a66949c04a7e6f05809df83452c5d9a817ad"}, "3": {"node_id": "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "node_type": null, "metadata": {}, "hash": "451a04f37c689d504f80a19d43848f5c94aa339b1ff4943b06555144a8f89bef"}, "4": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}}, "hash": "16c3441a8677f30247d9e5fa184585d2335284f4a5b1648bd3f59a670344279d", "text": "/docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\nOutput:\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/obtain a structured response\nLinks:\n\n{'points': [{'explanation': 'Writing short stories', 'explanation2': 'Programming on an IBM 1401', 'explanation3': 'Using microcomputers'}]}\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks:\n\nLangchain also offers output parsing modules that you can use within LlamaIndex.File Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/Langchain\nLinks:\n\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.output_parsers import LangchainOutputParser\nfrom llama_index.llm_predictor import StructuredLLMPredictor\nfrom llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt\nfrom llama_index.prompts.default_prompts import DEFAULT_TEXT_QA_PROMPT_TMPL, DEFAULT_REFINE_PROMPT_TMPL\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/load documents, build index\nLinks:\n\ndocuments = SimpleDirectoryReader('./paul_graham_essay/data').load_data()\nindex = VectorStoreIndex.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3b581203-bfa0-4565-9929-ea9e4a9b68a1": {"__data__": {"id_": "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "2": {"node_id": "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "node_type": null, "metadata": {}, "hash": "16c3441a8677f30247d9e5fa184585d2335284f4a5b1648bd3f59a670344279d"}, "3": {"node_id": "26022177-817a-4eb3-895a-71b8837d4ebd", "node_type": null, "metadata": {}, "hash": "45a4b8a594725fd7cf65ef78af2aaf9b31b30848ff0b088a0d0f2eb75839f998"}, "4": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}}, "hash": "451a04f37c689d504f80a19d43848f5c94aa339b1ff4943b06555144a8f89bef", "text": "from_documents(documents)\nllm_predictor = StructuredLLMPredictor()\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output schema\nLinks:\n\nresponse_schemas = [\n    ResponseSchema(name=\"Education\", description=\"Describes the author's educational experience/background.\"),\n    ResponseSchema(name=\"Work\", description=\"Describes the author's work experience/background.\")]", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26022177-817a-4eb3-895a-71b8837d4ebd": {"__data__": {"id_": "26022177-817a-4eb3-895a-71b8837d4ebd", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}, "2": {"node_id": "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "node_type": null, "metadata": {}, "hash": "451a04f37c689d504f80a19d43848f5c94aa339b1ff4943b06555144a8f89bef"}, "4": {"node_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "node_type": null, "metadata": {}, "hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2"}}, "hash": "45a4b8a594725fd7cf65ef78af2aaf9b31b30848ff0b088a0d0f2eb75839f998", "text": "File Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/define output parser\nLinks:\n\nlc_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\noutput_parser = LangchainOutputParser(lc_output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/format each prompt with output parser instructions\nLinks:\n\nfmt_qa_tmpl = output_parser.format(DEFAULT_TEXT_QA_PROMPT_TMPL)\nfmt_refine_tmpl = output_parser.format(DEFAULT_REFINE_PROMPT_TMPL)\nqa_prompt = QuestionAnswerPrompt(fmt_qa_tmpl, output_parser=output_parser)\nrefine_prompt = RefinePrompt(fmt_refine_tmpl, output_parser=output_parser)\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\nquery_engine = index.as_query_engine(\n    service_context=ServiceContext.from_defaults(\n        llm_predictor=llm_predictor\n    ),\n    text_qa_template=qa_prompt, \n    refine_template=refine_prompt, \n)\nresponse = query_engine.query(\n    \"What are a few things the author did growing up?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f15cb451-d8da-4e4f-afcc-7c195357c7a2": {"__data__": {"id_": "f15cb451-d8da-4e4f-afcc-7c195357c7a2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}, "3": {"node_id": "06e1128d-3441-4bd0-81ff-61e6be9a7404", "node_type": null, "metadata": {}, "hash": "bbd130373a88f7e04a7208c061869d4860fb30730f945f20f640630086fb2a47"}, "4": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}}, "hash": "87e3e0448b8585cd89ec9f82e72b3f7e88aac60e246eeb91cd49b87539d282d7", "text": "\", \n)\nprint(str(response))\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\nOutput:\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: text\nHeader Path: Output Parsing/query index\nLinks:\n\n{'Education': 'Before college, the author wrote short stories and experimented with programming on an IBM 1401.', 'Work': 'The author worked on writing and programming outside of school.'}File Name: ./docs/core_modules/query_modules/structured_outputs/output_parser.md\nContent Type: code\nHeader Path: Output Parsing/Guides\n\n```{toctree}\n---\ncaption: Examples\nmaxdepth: 1\n---\n\n/examples/output_parsing/GuardrailsDemo.ipynb\n/examples/output_parsing/LangchainOutputParserDemo.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n/examples/output_parsing/openai_pydantic_program.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program\nLinks:\n\nA pydantic program is a generic abstraction that takes in an input string and converts it to a structured Pydantic object type.Because this abstraction is so generic, it encompasses a broad range of LLM workflows.The programs are composable and be for more generic or specific use cases.There's a few general types of Pydantic Programs:\n- **LLM Text Completion Pydantic Programs**: These convert input text into a user-specified structured object through a text completion API + output parsing.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "06e1128d-3441-4bd0-81ff-61e6be9a7404": {"__data__": {"id_": "06e1128d-3441-4bd0-81ff-61e6be9a7404", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}, "2": {"node_id": "f15cb451-d8da-4e4f-afcc-7c195357c7a2", "node_type": null, "metadata": {}, "hash": "87e3e0448b8585cd89ec9f82e72b3f7e88aac60e246eeb91cd49b87539d282d7"}, "3": {"node_id": "99f90aec-e539-408d-b6c4-0f026081ffd5", "node_type": null, "metadata": {}, "hash": "7962e6a3aedc89e9322cbcf510865fb49119839ff70f59a7ca3b22f10fb41920"}, "4": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}}, "hash": "bbd130373a88f7e04a7208c061869d4860fb30730f945f20f640630086fb2a47", "text": "- **LLM Function Calling Pydantic Program**: These convert input text into a user-specified structured object through an LLM function calling API.- **Prepackaged Pydantic Programs**: These convert input text into prespecified structured objects.File Name: ./docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: text\nHeader Path: Pydantic Program/LLM Text Completion Pydantic Programs\nLinks:\n\nTODO: Coming soon!File Name: ./docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/LLM Function Calling Pydantic Programs\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/openai_pydantic_program.ipynb\n/examples/output_parsing/guidance_pydantic_program.ipynb\n/examples/output_parsing/guidance_sub_question.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/pydantic_program.md\nContent Type: code\nHeader Path: Pydantic Program/Prepackaged Pydantic Programs\n\n```{toctree}\n---\nmaxdepth: 1\n---\n/examples/output_parsing/df_program.ipynb\n/examples/output_parsing/evaporate_program.ipynb\n```\n\nFile Name: ./docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: text\nHeader Path: Structured Outputs\nLinks:\n\nThe ability of LLMs to produce structured outputs are important for downstream applications that rely on reliably parsing output values.LlamaIndex itself also relies on structured output in the following ways.- **Document retrieval**: Many data structures within LlamaIndex rely on LLM calls with a specific schema for Document retrieval.For instance,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "99f90aec-e539-408d-b6c4-0f026081ffd5": {"__data__": {"id_": "99f90aec-e539-408d-b6c4-0f026081ffd5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}, "2": {"node_id": "06e1128d-3441-4bd0-81ff-61e6be9a7404", "node_type": null, "metadata": {}, "hash": "bbd130373a88f7e04a7208c061869d4860fb30730f945f20f640630086fb2a47"}, "4": {"node_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "node_type": null, "metadata": {}, "hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c"}}, "hash": "7962e6a3aedc89e9322cbcf510865fb49119839ff70f59a7ca3b22f10fb41920", "text": "the tree index expects LLM calls to be in the format \"ANSWER: (number)\".- **Response synthesis**: Users may expect that the final response contains some degree of structure (e.g.a JSON output, a formatted SQL query, etc.)LlamaIndex provides a variety of modules enabling LLMs to produce outputs in a structured format.We provide modules at different levels of abstraction:\n- **Output Parsers**: These are modules that operate before and after an LLM text completion endpoint.They are not used with LLM function calling endpoints (since those contain structured outputs out of the box).- **Pydantic Programs**: These are generic modules that map an input prompt to a structured output, represented by a Pydantic object.They may use function calling APIs or text completion APIs + output parsers.- **Pre-defined Pydantic Program**: We have pre-defined Pydantic programs that map inputs to specific output types (like dataframes).See the sections below for an overview of output parsers and Pydantic programs.File Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: text\nHeader Path: Structured Outputs/\ud83d\udd2c Anatomy of a Structured Output Function\nLinks:\n\nHere we describe the different components of an LLM-powered structured output function.The pipeline depends on whether you're using a **generic LLM text completion API** or an **LLM function calling API**.!With generic completion APIs, the inputs and outputs are handled by text prompts.The output parser plays a role before and after the LLM call in ensuring structured outputs.Before the LLM call, the output parser can\nappend format instructions to the prompt.After the LLM call, the output parser can parse the output to the specified instructions.With function calling APIs, the output is inherently in a structured format, and the input can take in the signature of the desired object.The structured output just needs to be cast in the right object format (e.g.Pydantic).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9c03a722-9e11-431f-a445-106c1699e852": {"__data__": {"id_": "9c03a722-9e11-431f-a445-106c1699e852", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6260184-53e4-4db7-b4a2-3259dde76196", "node_type": null, "metadata": {}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd"}, "4": {"node_id": "b6260184-53e4-4db7-b4a2-3259dde76196", "node_type": null, "metadata": {}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd"}}, "hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd", "text": "File Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: code\nHeader Path: Structured Outputs/Output Parser Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\noutput_parser.md\n```\n\nFile Name: ../docs/core_modules/query_modules/structured_outputs/root.md\nContent Type: code\nHeader Path: Structured Outputs/Pydantic Program Modules\n\n```{toctree}\n---\nmaxdepth: 2\n---\npydantic_program.md\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"4ae9ef74-42fa-406e-87af-3080e3a73233": {"node_ids": ["4d3efd15-341c-4b29-afa7-7953188d2938", "6113854b-aea9-4310-bc89-6bf385c91deb", "43aa67f4-1213-4cfd-87bf-82baa0a7a21a", "6dd66f92-57c2-4089-9f14-5d673d36d762", "35f742cf-4dde-4ace-9e1e-d3074523760f", "04e52538-f44c-4f5d-bbd5-41ae6bbd6372", "6920088c-2a84-4cbe-ad70-13f1be8704fb", "694198a8-926a-4f24-b121-897dd6ebd176", "b24612df-85d4-4dfc-b6c9-84f0cb367ee5", "8d3af97a-f70d-46ff-af7e-3eeb86d210a3", "d4bec885-2ac4-4e51-b783-7835e4181f96", "26782402-9635-482e-b626-75e58b3de6af", "25b2aa80-5f4e-4746-b48d-42097f230452", "aae303c7-88a8-425a-baa2-775741a5e48a", "413527a2-71f2-466f-b6c7-4b480c41b702", "426429e8-7a33-4b53-a9f5-e586e1f65f2c", "eb266488-e2e8-4a42-b4c9-c72a9cafe32f", "b6260184-53e4-4db7-b4a2-3259dde76196"], "metadata": {}}, "4d3efd15-341c-4b29-afa7-7953188d2938": {"node_ids": ["ed55ac15-ce09-44f2-8794-eb6f30ad4102", "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "7606d974-a75a-40e4-b749-ceee37dbf1b5", "0ecd70b0-69e8-4c8d-bbde-4018da0126cb", "ed55ac15-ce09-44f2-8794-eb6f30ad4102", "37d71d91-c9f9-4466-b919-a2d3f4eaff84", "7606d974-a75a-40e4-b749-ceee37dbf1b5", "0ecd70b0-69e8-4c8d-bbde-4018da0126cb"], "metadata": {}}, "6113854b-aea9-4310-bc89-6bf385c91deb": {"node_ids": ["f4bd15e2-598d-4e37-ae15-6e03a9f08398", "fb970878-52f4-4123-af7f-ef6a948abf39", "051bf54d-d207-473e-a13b-3a0977d00377", "fbf04091-9814-4c59-a2af-59cf4442e28a", "f4bd15e2-598d-4e37-ae15-6e03a9f08398", "fb970878-52f4-4123-af7f-ef6a948abf39", "051bf54d-d207-473e-a13b-3a0977d00377", "fbf04091-9814-4c59-a2af-59cf4442e28a"], "metadata": {}}, "43aa67f4-1213-4cfd-87bf-82baa0a7a21a": {"node_ids": ["9a424f32-a874-4f8e-ab1c-991d3a9dbe49", "897ceee3-7af9-4300-a996-7069853dd3fe", "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "b42d48c1-08ff-42a5-bdb4-9dd6f364a027", "9a424f32-a874-4f8e-ab1c-991d3a9dbe49", "897ceee3-7af9-4300-a996-7069853dd3fe", "c97f0f09-c85c-495b-a3a0-2edb8958a18c", "b42d48c1-08ff-42a5-bdb4-9dd6f364a027"], "metadata": {}}, "6dd66f92-57c2-4089-9f14-5d673d36d762": {"node_ids": ["a4892a1f-2218-4bfe-b691-4162b1d4bf80", "e043659e-e9a0-4c87-8095-875bfdf7221a", "122d9186-a52b-4223-b806-d42a8563c93e", "9030c3b5-ba90-48a4-99dd-08c955bacadd", "a4892a1f-2218-4bfe-b691-4162b1d4bf80", "e043659e-e9a0-4c87-8095-875bfdf7221a", "122d9186-a52b-4223-b806-d42a8563c93e", "9030c3b5-ba90-48a4-99dd-08c955bacadd"], "metadata": {}}, "35f742cf-4dde-4ace-9e1e-d3074523760f": {"node_ids": ["f60a00a0-3ba2-46c7-8cad-cd05df08c6af", "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "edd49718-78ae-4333-94e6-2ffda9a60166", "c907930f-94ca-477b-9bea-a98af3354b65", "f60a00a0-3ba2-46c7-8cad-cd05df08c6af", "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17", "edd49718-78ae-4333-94e6-2ffda9a60166", "c907930f-94ca-477b-9bea-a98af3354b65"], "metadata": {}}, "04e52538-f44c-4f5d-bbd5-41ae6bbd6372": {"node_ids": ["a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb", "bf039726-614d-4d53-b16f-d119e7b18ae7", "bcc6f32f-9b05-4747-bcff-29b82efce87d", "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb", "bf039726-614d-4d53-b16f-d119e7b18ae7", "bcc6f32f-9b05-4747-bcff-29b82efce87d"], "metadata": {}}, "6920088c-2a84-4cbe-ad70-13f1be8704fb": {"node_ids": ["51c9d9e2-4889-4b34-84c8-76cc71f93b3f", "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "25316757-07dd-4e6d-9f0f-6f96b9cf63e2", "51c9d9e2-4889-4b34-84c8-76cc71f93b3f", "6a5c70d2-97d7-4b6b-8cb3-f58187568bda", "25316757-07dd-4e6d-9f0f-6f96b9cf63e2"], "metadata": {}}, "694198a8-926a-4f24-b121-897dd6ebd176": {"node_ids": ["61b30575-d871-460a-af27-cd41e98c2909", "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "2a5b69df-3782-4a44-8674-fcc4a9fee8c4", "61b30575-d871-460a-af27-cd41e98c2909", "27dbd541-35f6-4db6-954f-78e6bd9c7f78", "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9", "2a5b69df-3782-4a44-8674-fcc4a9fee8c4"], "metadata": {}}, "b24612df-85d4-4dfc-b6c9-84f0cb367ee5": {"node_ids": ["6c12ef17-17b3-47f6-87b8-c92e1391bfb7", "e4737417-44d7-46eb-b775-58b673b79554", "d8f3c932-6c3a-4538-9bd2-192933dc2055", "4baab847-43bf-4430-a9bf-05f0c12d9b4d", "6c12ef17-17b3-47f6-87b8-c92e1391bfb7", "e4737417-44d7-46eb-b775-58b673b79554", "d8f3c932-6c3a-4538-9bd2-192933dc2055", "4baab847-43bf-4430-a9bf-05f0c12d9b4d"], "metadata": {}}, "8d3af97a-f70d-46ff-af7e-3eeb86d210a3": {"node_ids": ["a99d060e-119d-49e2-8d50-3781092ad15c", "78a64c63-ffca-43dc-82f0-566a375395c6", "5171b641-9ca6-48a0-bde6-d47d021f3c54", "a99d060e-119d-49e2-8d50-3781092ad15c", "78a64c63-ffca-43dc-82f0-566a375395c6", "5171b641-9ca6-48a0-bde6-d47d021f3c54"], "metadata": {}}, "d4bec885-2ac4-4e51-b783-7835e4181f96": {"node_ids": ["8316e5ec-9820-47ce-9a03-0a5902c9bd70", "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "a1c28ad7-78bf-4afb-b187-ab49cae10939", "8316e5ec-9820-47ce-9a03-0a5902c9bd70", "5e8bf41b-f072-4f1e-a2c9-367cee63d816", "a1c28ad7-78bf-4afb-b187-ab49cae10939"], "metadata": {}}, "26782402-9635-482e-b626-75e58b3de6af": {"node_ids": ["26bbaf3d-a5b3-4759-a25d-b7df0f43ef27", "f2850e09-1c16-403b-93dd-9b73f989a8f5", "17a940a6-e716-499c-9d83-542c33d3f97c", "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27", "f2850e09-1c16-403b-93dd-9b73f989a8f5", "17a940a6-e716-499c-9d83-542c33d3f97c"], "metadata": {}}, "25b2aa80-5f4e-4746-b48d-42097f230452": {"node_ids": ["05d0a4b8-9787-412e-9ab8-92bdcec12d1e", "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "7436654d-25ea-4058-9623-c981db612e37", "05d0a4b8-9787-412e-9ab8-92bdcec12d1e", "a066a9ec-d70b-45c3-9f1a-445c9a86aede", "7436654d-25ea-4058-9623-c981db612e37"], "metadata": {}}, "aae303c7-88a8-425a-baa2-775741a5e48a": {"node_ids": ["4387dad2-8b75-46de-bc1b-3ff7936eb37a", "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "a600037b-6be1-416c-be05-c752abf38ffe", "4387dad2-8b75-46de-bc1b-3ff7936eb37a", "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71", "a7c88a07-1068-4305-aa31-ce7d2ece27ab", "a600037b-6be1-416c-be05-c752abf38ffe"], "metadata": {}}, "413527a2-71f2-466f-b6c7-4b480c41b702": {"node_ids": ["8e83a6bb-1b59-4597-8b68-0af905679267", "469c53f1-d0e2-40b2-8f54-f84a22474d93", "5c7983b9-f801-4a20-9ddc-321df1a15374", "8e83a6bb-1b59-4597-8b68-0af905679267", "469c53f1-d0e2-40b2-8f54-f84a22474d93", "5c7983b9-f801-4a20-9ddc-321df1a15374"], "metadata": {}}, "426429e8-7a33-4b53-a9f5-e586e1f65f2c": {"node_ids": ["a7e3546c-e36e-4229-a275-24b1eba8a3fd", "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "26022177-817a-4eb3-895a-71b8837d4ebd", "a7e3546c-e36e-4229-a275-24b1eba8a3fd", "e3d798c1-4d01-4fd8-9725-59dcc73c5320", "3b581203-bfa0-4565-9929-ea9e4a9b68a1", "26022177-817a-4eb3-895a-71b8837d4ebd"], "metadata": {}}, "eb266488-e2e8-4a42-b4c9-c72a9cafe32f": {"node_ids": ["f15cb451-d8da-4e4f-afcc-7c195357c7a2", "06e1128d-3441-4bd0-81ff-61e6be9a7404", "99f90aec-e539-408d-b6c4-0f026081ffd5", "f15cb451-d8da-4e4f-afcc-7c195357c7a2", "06e1128d-3441-4bd0-81ff-61e6be9a7404", "99f90aec-e539-408d-b6c4-0f026081ffd5"], "metadata": {}}, "b6260184-53e4-4db7-b4a2-3259dde76196": {"node_ids": ["9c03a722-9e11-431f-a445-106c1699e852", "9c03a722-9e11-431f-a445-106c1699e852"], "metadata": {}}}, "docstore/metadata": {"4d3efd15-341c-4b29-afa7-7953188d2938": {"doc_hash": "6f8ec28aabfcd722e8db20a1c6f0eb6da161969d6b25af5c28538c67cebb5cf2", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "6113854b-aea9-4310-bc89-6bf385c91deb": {"doc_hash": "23ece345e5f586c10b97690c83deeebe6f66a795e799e1af8fa93db5617b0c19", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "43aa67f4-1213-4cfd-87bf-82baa0a7a21a": {"doc_hash": "a0d1f179a217cf7fc1c4fd1d22fca457eeaccd5067fc576576d6c574833c9819", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "6dd66f92-57c2-4089-9f14-5d673d36d762": {"doc_hash": "57b3a9d1ed2161800ca8bac022fcab50e962e36982501f1eb585e8bd1d7f2b84", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "35f742cf-4dde-4ace-9e1e-d3074523760f": {"doc_hash": "6a6bda7efa03e7cc02c0e7680f8c5733a6189c644cb105b834a87a752f78ff15", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "04e52538-f44c-4f5d-bbd5-41ae6bbd6372": {"doc_hash": "bc0161262c1dc2aad3ca4cf6b88ad30099437cd12c9874915d62d90c08c4303a", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "6920088c-2a84-4cbe-ad70-13f1be8704fb": {"doc_hash": "3e02e38d063a3a8abfc1c3a7d2de32be785acef71e2cc5bc7fc0caaf18bfe681", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "694198a8-926a-4f24-b121-897dd6ebd176": {"doc_hash": "29c21ff640a6cd9da6e4df4d3c0168a3b0aab94f96744cc1adc2b43a18aac0cc", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "b24612df-85d4-4dfc-b6c9-84f0cb367ee5": {"doc_hash": "077623c29831c1ed704d254f398a3ce87e9d3a6486ede1e70631bcd3eca322d2", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "8d3af97a-f70d-46ff-af7e-3eeb86d210a3": {"doc_hash": "7f00adc6844033a8e93e275648dda6e79d21262691c1df107c9193f2fd37cbdd", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "d4bec885-2ac4-4e51-b783-7835e4181f96": {"doc_hash": "efdc6d67c28004e0f4404c272b12c3595f51988b5ea627b7af2e791a9027c8ee", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "26782402-9635-482e-b626-75e58b3de6af": {"doc_hash": "176caf3999e0eb5643f52cd884bdae32faeda9d8a60407b388f5ea9ac762d138", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "25b2aa80-5f4e-4746-b48d-42097f230452": {"doc_hash": "fa1db5008bdc963023cefafbd02496a9a0f487eea05875bfa8ff7a7dda51e7e4", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "aae303c7-88a8-425a-baa2-775741a5e48a": {"doc_hash": "a869b8ae5d018206e19c4dd9bc50c9a1080186c82fc4274d558476dd12687e3d", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "413527a2-71f2-466f-b6c7-4b480c41b702": {"doc_hash": "86738b20de788a0f4749a15508f7b237bfff9aa6006d20a4c2d2ce2715cd8960", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "426429e8-7a33-4b53-a9f5-e586e1f65f2c": {"doc_hash": "6de32bdd828b5b5b5db524c58b86abbd80fe85c04ed6f9e420160450346e52d2", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "eb266488-e2e8-4a42-b4c9-c72a9cafe32f": {"doc_hash": "3318422c3746f1ffee8bf8ae68857b4955ceacf7cd1d0a17bb85f972b4eea56c", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "b6260184-53e4-4db7-b4a2-3259dde76196": {"doc_hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd", "ref_doc_id": "4ae9ef74-42fa-406e-87af-3080e3a73233"}, "ed55ac15-ce09-44f2-8794-eb6f30ad4102": {"doc_hash": "4262e11df5c3bbed8251c990f57e24075fea68dec6c8db7b8c6b5319d0269284", "ref_doc_id": "4d3efd15-341c-4b29-afa7-7953188d2938"}, "37d71d91-c9f9-4466-b919-a2d3f4eaff84": {"doc_hash": "77f96d1de5680edd6c50e02c17bcb4d73aec29caf811c0fceff1d78500341caf", "ref_doc_id": "4d3efd15-341c-4b29-afa7-7953188d2938"}, "7606d974-a75a-40e4-b749-ceee37dbf1b5": {"doc_hash": "f994cfd53ce54f093d632d29c3995f2e5a7cb76c42c6249fa8ec43feb26fd0ee", "ref_doc_id": "4d3efd15-341c-4b29-afa7-7953188d2938"}, "0ecd70b0-69e8-4c8d-bbde-4018da0126cb": {"doc_hash": "16dc20ceedccee484801f6cd6331c075d63d21ef38feba760930abe8c6723f57", "ref_doc_id": "4d3efd15-341c-4b29-afa7-7953188d2938"}, "f4bd15e2-598d-4e37-ae15-6e03a9f08398": {"doc_hash": "f997d531bcd63c9f023e50a984d05cab88c8796412bc8c6bc991a5335aea856e", "ref_doc_id": "6113854b-aea9-4310-bc89-6bf385c91deb"}, "fb970878-52f4-4123-af7f-ef6a948abf39": {"doc_hash": "c6253458d24ad71db59bdcd0495916aa2f8a261ef905f899e1ca3461c1c192cc", "ref_doc_id": "6113854b-aea9-4310-bc89-6bf385c91deb"}, "051bf54d-d207-473e-a13b-3a0977d00377": {"doc_hash": "48122b3e6a2098d31381010c56fa07d7b775eef7f1ff5ab516fcd7d8fb43e3b2", "ref_doc_id": "6113854b-aea9-4310-bc89-6bf385c91deb"}, "fbf04091-9814-4c59-a2af-59cf4442e28a": {"doc_hash": "e6dc2d9b6f90a2e165bf9b33f012c19e41a3164d99c104336abf3b6d508b6b18", "ref_doc_id": "6113854b-aea9-4310-bc89-6bf385c91deb"}, "9a424f32-a874-4f8e-ab1c-991d3a9dbe49": {"doc_hash": "33315f4fb092f447a63428c4de8c9426c04b9d14b9066451bb8c349bb8aed600", "ref_doc_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a"}, "897ceee3-7af9-4300-a996-7069853dd3fe": {"doc_hash": "415da5caa22124dcc721dee78617a51e585f7fe8dece2256478f05ea40b355e5", "ref_doc_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a"}, "c97f0f09-c85c-495b-a3a0-2edb8958a18c": {"doc_hash": "36738bb23aa0fcc746d254c0e39e574306470f8125f936803db728adca78853a", "ref_doc_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a"}, "b42d48c1-08ff-42a5-bdb4-9dd6f364a027": {"doc_hash": "40e16441ab13ac84a903aa068d336b090e273e96b024890e4b0050529b834cab", "ref_doc_id": "43aa67f4-1213-4cfd-87bf-82baa0a7a21a"}, "a4892a1f-2218-4bfe-b691-4162b1d4bf80": {"doc_hash": "bbfc9e7516b4ad407a62d38a5451f7df026faabb8a4734f99b0f297b1e81a75e", "ref_doc_id": "6dd66f92-57c2-4089-9f14-5d673d36d762"}, "e043659e-e9a0-4c87-8095-875bfdf7221a": {"doc_hash": "1cfa1181e601b4943de427f459dfd2ef90dd03581280ef97cff71cb4a0481ba2", "ref_doc_id": "6dd66f92-57c2-4089-9f14-5d673d36d762"}, "122d9186-a52b-4223-b806-d42a8563c93e": {"doc_hash": "b89bd911f9c9274d05b41661c2651814c70623c370cecc0469df24111adebeaf", "ref_doc_id": "6dd66f92-57c2-4089-9f14-5d673d36d762"}, "9030c3b5-ba90-48a4-99dd-08c955bacadd": {"doc_hash": "b743eeab3c848aca6a4ac34ab2b118c8bfa83196d2bb1013388391696cf3364e", "ref_doc_id": "6dd66f92-57c2-4089-9f14-5d673d36d762"}, "f60a00a0-3ba2-46c7-8cad-cd05df08c6af": {"doc_hash": "9cffdfb14d878e50776c5de0f52510f16f4d348eb80a2cabc41f6660265fd71e", "ref_doc_id": "35f742cf-4dde-4ace-9e1e-d3074523760f"}, "032f4ec0-4f4e-4f3a-9bdd-36d1fb4a3c17": {"doc_hash": "c0cbcb30decd9ba24c155ab3f966b8dc08e85f1ae9f99ab81a2b51457b8a64df", "ref_doc_id": "35f742cf-4dde-4ace-9e1e-d3074523760f"}, "edd49718-78ae-4333-94e6-2ffda9a60166": {"doc_hash": "fe690d42f5aaee8838ac0f5b002f0da4509c7701e64b84ecd0bd67ae4d479cd1", "ref_doc_id": "35f742cf-4dde-4ace-9e1e-d3074523760f"}, "c907930f-94ca-477b-9bea-a98af3354b65": {"doc_hash": "4dd43a1f5e676e1acf02041380d771bb6bf36ad7f0248e4a97292232884d160c", "ref_doc_id": "35f742cf-4dde-4ace-9e1e-d3074523760f"}, "a2fa9582-e7af-4d7d-a913-d43ec2bc1eeb": {"doc_hash": "1fb85872f4367af977decfaf90775641d425a30c48ace67ce2262bf9ee0b1aa6", "ref_doc_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372"}, "bf039726-614d-4d53-b16f-d119e7b18ae7": {"doc_hash": "7daf57d6e5849bd6e98d3818c87c6d8107eed501fe1a84bc4828a688e59e1268", "ref_doc_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372"}, "bcc6f32f-9b05-4747-bcff-29b82efce87d": {"doc_hash": "82e8070f7441f219bd41ec73e6a1c1eaf7f2cde7fd12fc25b5e22345ab51bab2", "ref_doc_id": "04e52538-f44c-4f5d-bbd5-41ae6bbd6372"}, "51c9d9e2-4889-4b34-84c8-76cc71f93b3f": {"doc_hash": "d1753be1bb8e0df13dbeb1735c83414ef33647dd7c293075158ad286bd3e4dd7", "ref_doc_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb"}, "6a5c70d2-97d7-4b6b-8cb3-f58187568bda": {"doc_hash": "36ac76a16f0bb5e758a3a01d9144c2f0a166db3bccf60d27529c5d7a733ed5ba", "ref_doc_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb"}, "25316757-07dd-4e6d-9f0f-6f96b9cf63e2": {"doc_hash": "5f348a00cdcb3373aa777965ac3e0a504e23d589f5c11dfa236c3867a1b100bb", "ref_doc_id": "6920088c-2a84-4cbe-ad70-13f1be8704fb"}, "61b30575-d871-460a-af27-cd41e98c2909": {"doc_hash": "16f3f7c84aec55a644fb8ea155e96dbd192b7b7d79d0b53f7bd4e4b88958fc48", "ref_doc_id": "694198a8-926a-4f24-b121-897dd6ebd176"}, "27dbd541-35f6-4db6-954f-78e6bd9c7f78": {"doc_hash": "fd9284a928c4c00541e4a594c9799b619cd2f9efcb9edc608c0c3698d3b30308", "ref_doc_id": "694198a8-926a-4f24-b121-897dd6ebd176"}, "2eb6f6ed-93f1-4d5a-bc86-a1cae2532af9": {"doc_hash": "e0549233d2c4d811c7d20b71c081249f9cef80636d7d8dfa8466a9f4a6e2a8f7", "ref_doc_id": "694198a8-926a-4f24-b121-897dd6ebd176"}, "2a5b69df-3782-4a44-8674-fcc4a9fee8c4": {"doc_hash": "e5d484c3d06c22948040a467fd5b4fcfdc6ea2991af7f0ab8e62556624a81fc7", "ref_doc_id": "694198a8-926a-4f24-b121-897dd6ebd176"}, "6c12ef17-17b3-47f6-87b8-c92e1391bfb7": {"doc_hash": "9d8e5e06f4f0a3a3ba8f5586c4c57ef6b7486e6dc4aeb12e551086f3f3e69366", "ref_doc_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5"}, "e4737417-44d7-46eb-b775-58b673b79554": {"doc_hash": "3627c85268c3748ed861ea7a144637f3b1f29c65f3967f3fb1f3ada6c758395f", "ref_doc_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5"}, "d8f3c932-6c3a-4538-9bd2-192933dc2055": {"doc_hash": "91d403b091e2be9376485bfd5b7fc9ead09b5708442883ec0056483382010cd5", "ref_doc_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5"}, "4baab847-43bf-4430-a9bf-05f0c12d9b4d": {"doc_hash": "0fdbefc843ac4b7186fd1cd00f56ac63f797611247df783e7a52fef6ac6e4781", "ref_doc_id": "b24612df-85d4-4dfc-b6c9-84f0cb367ee5"}, "a99d060e-119d-49e2-8d50-3781092ad15c": {"doc_hash": "72c44d847f39678cc9526549dc74c37c8646fdf27ef648bf52f3ef754a3486ef", "ref_doc_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3"}, "78a64c63-ffca-43dc-82f0-566a375395c6": {"doc_hash": "bd5cb0ac9e40b3538070334336607ee82c78c1fb9590922fa7adb1fd18d8ecec", "ref_doc_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3"}, "5171b641-9ca6-48a0-bde6-d47d021f3c54": {"doc_hash": "099f46d80276e105d56c11faacbb4c32af70c9b71864369b910ac9502c08f0e8", "ref_doc_id": "8d3af97a-f70d-46ff-af7e-3eeb86d210a3"}, "8316e5ec-9820-47ce-9a03-0a5902c9bd70": {"doc_hash": "2cd55610a036436819ffbd3c5e95d45ea799487ed09d0e15715e7bfb7be5d491", "ref_doc_id": "d4bec885-2ac4-4e51-b783-7835e4181f96"}, "5e8bf41b-f072-4f1e-a2c9-367cee63d816": {"doc_hash": "8a4fde3c0393191e71e36a6a9eeb0742b89d25f6c232fed0a6bfe5489275eda1", "ref_doc_id": "d4bec885-2ac4-4e51-b783-7835e4181f96"}, "a1c28ad7-78bf-4afb-b187-ab49cae10939": {"doc_hash": "6d3cc1150e1d1f8cf08348b84a2f68d099212f4e6633b10a4795192451c06cdf", "ref_doc_id": "d4bec885-2ac4-4e51-b783-7835e4181f96"}, "26bbaf3d-a5b3-4759-a25d-b7df0f43ef27": {"doc_hash": "3ee763f8403d7ca6d8b237a566ba334f767091af21fbb5351b0a847760cd5f1b", "ref_doc_id": "26782402-9635-482e-b626-75e58b3de6af"}, "f2850e09-1c16-403b-93dd-9b73f989a8f5": {"doc_hash": "8bd18e5f22a151b467ed1000b315084968cba810ecf92c83b7b06f70fa27f263", "ref_doc_id": "26782402-9635-482e-b626-75e58b3de6af"}, "17a940a6-e716-499c-9d83-542c33d3f97c": {"doc_hash": "e22bb61dfa5d5ac930b7669bdade19f4ea8fd18d8cdc6dce23351b13fa86c3d7", "ref_doc_id": "26782402-9635-482e-b626-75e58b3de6af"}, "05d0a4b8-9787-412e-9ab8-92bdcec12d1e": {"doc_hash": "0173af05b63014a0fa9fe4fe78768f125b690c2db01ad72e4eedd1e1ff6b0a57", "ref_doc_id": "25b2aa80-5f4e-4746-b48d-42097f230452"}, "a066a9ec-d70b-45c3-9f1a-445c9a86aede": {"doc_hash": "fc88305a4fedc81a3108d9b3646098eb9aa680c1aabd1c52062298497e0c4f0b", "ref_doc_id": "25b2aa80-5f4e-4746-b48d-42097f230452"}, "7436654d-25ea-4058-9623-c981db612e37": {"doc_hash": "557643045b9c72929610dc515e91171a9df1d09c28ba1816d5d03bf587c97f5c", "ref_doc_id": "25b2aa80-5f4e-4746-b48d-42097f230452"}, "4387dad2-8b75-46de-bc1b-3ff7936eb37a": {"doc_hash": "f1048bc9c7dbdd264ae25a82c1e249415c2c6f3cf9c99345e7566b331788ed0d", "ref_doc_id": "aae303c7-88a8-425a-baa2-775741a5e48a"}, "2cc83fb2-2ea4-4cfb-a584-867fa8eb3c71": {"doc_hash": "4c921f860ffb445c035ef24b476d6b31262e195905bfe401629c31da9a14741d", "ref_doc_id": "aae303c7-88a8-425a-baa2-775741a5e48a"}, "a7c88a07-1068-4305-aa31-ce7d2ece27ab": {"doc_hash": "b1facabb9c90f48304de2adcd82f65164f296f3207d5ad09747ace5f84a04b3f", "ref_doc_id": "aae303c7-88a8-425a-baa2-775741a5e48a"}, "a600037b-6be1-416c-be05-c752abf38ffe": {"doc_hash": "c36fe57fd95ade9b737e41dbbbc8b18fe6a83c85ff9414470e7675e68947114a", "ref_doc_id": "aae303c7-88a8-425a-baa2-775741a5e48a"}, "8e83a6bb-1b59-4597-8b68-0af905679267": {"doc_hash": "7778d12011847b5667a700609c14bc83795807abcebefad0a039a585ebe7c541", "ref_doc_id": "413527a2-71f2-466f-b6c7-4b480c41b702"}, "469c53f1-d0e2-40b2-8f54-f84a22474d93": {"doc_hash": "7564e066790552ce64be067ad08a95b6136078f91d5bd807be3a1ae7a150ba5b", "ref_doc_id": "413527a2-71f2-466f-b6c7-4b480c41b702"}, "5c7983b9-f801-4a20-9ddc-321df1a15374": {"doc_hash": "ddc83cb3fd24b86fb2da1a814ae51d5e6a42d1f8730252d5f78fa94594e42a05", "ref_doc_id": "413527a2-71f2-466f-b6c7-4b480c41b702"}, "a7e3546c-e36e-4229-a275-24b1eba8a3fd": {"doc_hash": "16362627e4a35e5e81416ce8f903a66949c04a7e6f05809df83452c5d9a817ad", "ref_doc_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c"}, "e3d798c1-4d01-4fd8-9725-59dcc73c5320": {"doc_hash": "16c3441a8677f30247d9e5fa184585d2335284f4a5b1648bd3f59a670344279d", "ref_doc_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c"}, "3b581203-bfa0-4565-9929-ea9e4a9b68a1": {"doc_hash": "451a04f37c689d504f80a19d43848f5c94aa339b1ff4943b06555144a8f89bef", "ref_doc_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c"}, "26022177-817a-4eb3-895a-71b8837d4ebd": {"doc_hash": "45a4b8a594725fd7cf65ef78af2aaf9b31b30848ff0b088a0d0f2eb75839f998", "ref_doc_id": "426429e8-7a33-4b53-a9f5-e586e1f65f2c"}, "f15cb451-d8da-4e4f-afcc-7c195357c7a2": {"doc_hash": "87e3e0448b8585cd89ec9f82e72b3f7e88aac60e246eeb91cd49b87539d282d7", "ref_doc_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f"}, "06e1128d-3441-4bd0-81ff-61e6be9a7404": {"doc_hash": "bbd130373a88f7e04a7208c061869d4860fb30730f945f20f640630086fb2a47", "ref_doc_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f"}, "99f90aec-e539-408d-b6c4-0f026081ffd5": {"doc_hash": "7962e6a3aedc89e9322cbcf510865fb49119839ff70f59a7ca3b22f10fb41920", "ref_doc_id": "eb266488-e2e8-4a42-b4c9-c72a9cafe32f"}, "9c03a722-9e11-431f-a445-106c1699e852": {"doc_hash": "aec735ffcc16723e9832e9ffdf74f1c8401a5cefe7fb8b862bf19e5230f1dbfd", "ref_doc_id": "b6260184-53e4-4db7-b4a2-3259dde76196"}}}