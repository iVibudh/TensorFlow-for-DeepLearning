{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GWvKKFRdxvP"
      },
      "source": [
        "# Retrieval Augmented FuctionCallingAgent\n",
        "\n",
        "\n",
        "In this tutorial, we demonstrate how to utilize our `FunctionCallingAgent `implementation alongside a tool retriever.\n",
        "\n",
        "The goal is to construct an agent that leverages the function-calling API of LLMs (such as OpenAI, Anthropic, or Mistral) to manage and index an arbitrary number of tools.\n",
        "\n",
        "Our indexing and retrieval modules are designed to simplify the process, addressing the challenge of accommodating numerous functions within a prompt.\n",
        "\n",
        "`FunctionCallingAgent` uses LLM function calling capabilities to complete the task.\n",
        "\n",
        "[LlamaIndex YT Ref](https://www.youtube.com/watch?v=K7h17Jjtbzg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w1rb_v0OfIPS"
      },
      "outputs": [],
      "source": [
        "# !pip install llama-index\n",
        "# !pip install llama-index-llms-anthropic\n",
        "# !pip install llama-index-llms-mistralai\n",
        "# !pip install llama-index-llms-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH47QWjodBbK"
      },
      "source": [
        "## Simple Calculator Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dd8MzkGuLhtq"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import ReActAgent\n",
        "# from llama_index.llms.openai import OpenAI\n",
        "# from llama_index.llms.anthropic import Anthropic\n",
        "# from llama_index.llms.mistralai import MistralAI\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool, FunctionTool\n",
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiPWsI6PdHRr"
      },
      "source": [
        "### Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fQW2ccGlLrg7"
      },
      "outputs": [],
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers with and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two integers with and returns the result integer\"\"\"\n",
        "    return a - b\n",
        "\n",
        "def useless(a: int, b: int) -> int:\n",
        "    \"\"\"Toy useless function.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Bp4NnpPkLycF"
      },
      "outputs": [],
      "source": [
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
        "\n",
        "\n",
        "# Adding useless tools 50 times \n",
        "useless_tools = [\n",
        "    FunctionTool.from_defaults(fn=useless, name=f\"useless_{str(idx)}\")\n",
        "    for idx in range(50)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2aY0239ts2cY"
      },
      "outputs": [],
      "source": [
        "all_tools = [multiply_tool] + [add_tool] + [subtract_tool] + useless_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Cnh9qKX8kN"
      },
      "source": [
        "### Setup API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MZtnSRlBXFvC"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J8ar0Q8u1Qs"
      },
      "source": [
        "### Create ObjectIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GwYtvzLas7sb"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:59\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     58\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m OpenAIEmbedding()\n\u001b[1;32m---> 59\u001b[0m     validate_openai_api_key(embed_model\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai\\utils.py:104\u001b[0m, in \u001b[0;36mvalidate_openai_api_key\u001b[1;34m(api_key)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m openai_api_key:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(MISSING_API_KEY_ERROR_MESSAGE)\n",
            "\u001b[1;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobjects\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObjectIndex\n\u001b[1;32m----> 5\u001b[0m obj_index \u001b[38;5;241m=\u001b[39m ObjectIndex\u001b[38;5;241m.\u001b[39mfrom_objects(\n\u001b[0;32m      6\u001b[0m     all_tools,\n\u001b[0;32m      7\u001b[0m     index_cls\u001b[38;5;241m=\u001b[39mVectorStoreIndex,\n\u001b[0;32m      8\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\objects\\base.py:174\u001b[0m, in \u001b[0;36mObjectIndex.from_objects\u001b[1;34m(cls, objects, object_mapping, from_node_fn, to_node_fn, index_cls, **index_kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     object_mapping \u001b[38;5;241m=\u001b[39m get_object_mapping(\n\u001b[0;32m    168\u001b[0m         objects,\n\u001b[0;32m    169\u001b[0m         from_node_fn\u001b[38;5;241m=\u001b[39mfrom_node_fn,\n\u001b[0;32m    170\u001b[0m         to_node_fn\u001b[38;5;241m=\u001b[39mto_node_fn,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m nodes \u001b[38;5;241m=\u001b[39m object_mapping\u001b[38;5;241m.\u001b[39mto_nodes(objects)\n\u001b[1;32m--> 174\u001b[0m index \u001b[38;5;241m=\u001b[39m index_cls(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mindex_kwargs)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(index, object_mapping)\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:71\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     85\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\settings.py:274\u001b[0m, in \u001b[0;36membed_model_from_settings_or_context\u001b[1;34m(settings, context)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39membed_model\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m settings\u001b[38;5;241m.\u001b[39membed_model\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\settings.py:67\u001b[0m, in \u001b[0;36m_Settings.embed_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get the embedding model.\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m resolve_embed_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model\u001b[38;5;241m.\u001b[39mcallback_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\n",
            "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\embeddings\\utils.py:66\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[1;34m(embed_model, callback_manager)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`llama-index-embeddings-openai` package not found, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease run `pip install llama-index-embeddings-openai`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 66\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load OpenAI embedding model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you intended to use OpenAI, please check your OPENAI_API_KEY.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConsider using embed_model=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisit our documentation for more embedding options: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings.html#modules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m******\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# for image multi-modal embeddings\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m embed_model\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "\u001b[1;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
          ]
        }
      ],
      "source": [
        "# define an \"object\" index over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCk_5CGqL-zQ"
      },
      "source": [
        "####  OpenAI GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dpG5vDrOtQyd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'obj_index' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHermes2Pro\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      2\u001b[0m             request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30.0\u001b[39m)\n\u001b[0;32m      4\u001b[0m agent_worker \u001b[38;5;241m=\u001b[39m FunctionCallingAgentWorker\u001b[38;5;241m.\u001b[39mfrom_tools(\n\u001b[1;32m----> 5\u001b[0m     tool_retriever\u001b[38;5;241m=\u001b[39mobj_index\u001b[38;5;241m.\u001b[39mas_retriever(similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      6\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m      7\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     allow_parallel_tool_calls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m agent \u001b[38;5;241m=\u001b[39m AgentRunner(agent_worker)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'obj_index' is not defined"
          ]
        }
      ],
      "source": [
        "llm = Ollama(model=\"Hermes2Pro\", \n",
        "            request_timeout=30.0)\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=2),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=False,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9JTUm8ltCGU",
        "outputId": "df4cd14d-7c17-4459-d0e7-fd7e3a0ed051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10?\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== LLM Response ===\n",
            "500 multiplied by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrNyoUOhtFBb",
        "outputId": "c778b1a5-7eaa-451b-8b59-935db91c8e0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: 500 multiplied by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3oXEzTVMca7",
        "outputId": "ceec0569-5a5e-4fb2-9583-34a357b2936e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10 and then subtract 1000?\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== Calling Function ===\n",
            "Calling function: subtract with args: {\"a\": 5000, \"b\": 1000}\n",
            "=== Function Output ===\n",
            "4000\n",
            "=== LLM Response ===\n",
            "500 multiplied by 10 and then subtracting 1000 gives 4000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10 and then subtract 1000?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsWkUzx7u9bk",
        "outputId": "949fce42-f586-4538-c09e-e0483525fd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: 500 multiplied by 10 and then subtracting 1000 gives 4000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Acucy7MEku"
      },
      "source": [
        "#### Anthropic Sonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK3rHgyMMjyn"
      },
      "outputs": [],
      "source": [
        "os.environ['ANTHROPIC_API_KEY'] = 'YOUR ANTHROPIC API KEY'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iPdYMJ4MDLm"
      },
      "outputs": [],
      "source": [
        "llm = Anthropic(model=\"claude-3-sonnet-20240229\")\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=2),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NZjCVgQRWs4",
        "outputId": "feb907a3-d08e-4349-b0fe-1495cfe04f36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10?\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== LLM Response ===\n",
            "So 500 multiplied by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA0xh4n3Radt",
        "outputId": "fc1fe918-7905-4868-f71e-e1727b4e2591"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: So 500 multiplied by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOmCX2bpRp71",
        "outputId": "2a797cc3-8404-4e93-beae-20daadb79df7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10 and then subtract 1000?\n",
            "=== LLM Response ===\n",
            "Okay, let's break this down into steps:\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== LLM Response ===\n",
            "First, 500 multiplied by 10 is 5000.\n",
            "=== Calling Function ===\n",
            "Calling function: subtract with args: {\"a\": 5000, \"b\": 1000}\n",
            "=== Function Output ===\n",
            "4000\n",
            "=== LLM Response ===\n",
            "Then, subtracting 1000 from 5000 gives 4000.\n",
            "\n",
            "Therefore, the result of 500 multiplied by 10 and then subtracting 1000 is 4000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10 and then subtract 1000?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCQdzp4BRrSF",
        "outputId": "55e88716-5336-4d6c-8a0c-3406f2564c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: Then, subtracting 1000 from 5000 gives 4000.\n",
            "\n",
            "Therefore, the result of 500 multiplied by 10 and then subtracting 1000 is 4000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MuNIhZzMHcm"
      },
      "source": [
        "#### MistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A4Fgl5fRZiF"
      },
      "outputs": [],
      "source": [
        "os.environ['MISTRAL_API_KEY'] = 'YOUR MISTRALAI API KEY'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMOq_oraMDfc"
      },
      "outputs": [],
      "source": [
        "llm = MistralAI(model=\"mistral-large-latest\")\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=2),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=False,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPU4KUHMVu1Z",
        "outputId": "43512606-a57e-4ce0-9d39-5408c18141f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10?\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== LLM Response ===\n",
            "The result of multiplying 500 by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Oubu48XVw81",
        "outputId": "6fc11448-4b66-4ec6-db0c-07f17efd3f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The result of multiplying 500 by 10 is 5000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u_sLM7gVx91",
        "outputId": "ae56659c-a759-43bd-fbe0-8e1e91f19070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What's 500 multiplied by 10 and then subtract 1000?\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 500, \"b\": 10}\n",
            "=== Function Output ===\n",
            "5000\n",
            "=== Calling Function ===\n",
            "Calling function: subtract with args: {\"a\": 5000, \"b\": 1000}\n",
            "=== Function Output ===\n",
            "4000\n",
            "=== LLM Response ===\n",
            "The result of multiplying 500 by 10 and then subtracting 1000 is 4000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What's 500 multiplied by 10 and then subtract 1000?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4niNrj5OV0es",
        "outputId": "725af6c1-e5ae-449b-8880-6c5d60f27d2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The result of multiplying 500 by 10 and then subtracting 1000 is 4000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMlBJu2vdUU2"
      },
      "source": [
        "## RAG QueryEngine Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEY6G0JtXuMo"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7x3grUzbL5w"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        persist_dir=\"./storage/lyft\"\n",
        "    )\n",
        "    lyft_index = load_index_from_storage(storage_context)\n",
        "\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        persist_dir=\"./storage/uber\"\n",
        "    )\n",
        "    uber_index = load_index_from_storage(storage_context)\n",
        "\n",
        "    index_loaded = True\n",
        "except:\n",
        "    index_loaded = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUnO5ZX8dYJA"
      },
      "source": [
        "### Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA0rmS5IbOhv",
        "outputId": "1ac43f07-6634-4037-f143-70fad59c165c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-04-12 22:32:15--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1880483 (1.8M) [application/octet-stream]\n",
            "Saving to: ‘data/10k/uber_2021.pdf’\n",
            "\n",
            "\rdata/10k/uber_2021.   0%[                    ]       0  --.-KB/s               \rdata/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-12 22:32:16 (40.5 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n",
            "\n",
            "--2024-04-12 22:32:16--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1440303 (1.4M) [application/octet-stream]\n",
            "Saving to: ‘data/10k/lyft_2021.pdf’\n",
            "\n",
            "data/10k/lyft_2021. 100%[===================>]   1.37M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-04-12 22:32:16 (38.3 MB/s) - ‘data/10k/lyft_2021.pdf’ saved [1440303/1440303]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/10k/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf' -O 'data/10k/lyft_2021.pdf'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlhxgDAhdanb"
      },
      "source": [
        "### Load Data And Create Index\n",
        "\n",
        "We will by default use OpenAI Embeddings for building index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3kycd7CbRDe"
      },
      "outputs": [],
      "source": [
        "if not index_loaded:\n",
        "    # load data\n",
        "    lyft_docs = SimpleDirectoryReader(\n",
        "        input_files=[\"./data/10k/lyft_2021.pdf\"]\n",
        "    ).load_data()\n",
        "    uber_docs = SimpleDirectoryReader(\n",
        "        input_files=[\"./data/10k/uber_2021.pdf\"]\n",
        "    ).load_data()\n",
        "\n",
        "    # build index\n",
        "    lyft_index = VectorStoreIndex.from_documents(lyft_docs)\n",
        "    uber_index = VectorStoreIndex.from_documents(uber_docs)\n",
        "\n",
        "    # persist index\n",
        "    lyft_index.storage_context.persist(persist_dir=\"./storage/lyft\")\n",
        "    uber_index.storage_context.persist(persist_dir=\"./storage/uber\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsNt_zZhdcld"
      },
      "source": [
        "### Create QueryEngines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hibIsja5bUlb"
      },
      "outputs": [],
      "source": [
        "lyft_engine = lyft_index.as_query_engine(similarity_top_k=3)\n",
        "uber_engine = uber_index.as_query_engine(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZsjutZYde9R"
      },
      "source": [
        "### Define Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jidXn8xCbXHF"
      },
      "outputs": [],
      "source": [
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=lyft_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"lyft_10k\",\n",
        "            description=(\n",
        "                \"Provides information about Lyft financials for year 2021. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=uber_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"uber_10k\",\n",
        "            description=(\n",
        "                \"Provides information about Uber financials for year 2021. \"\n",
        "                \"Use a detailed plain text question as input to the tool.\"\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zASbANOhv6AN"
      },
      "outputs": [],
      "source": [
        "obj_index = ObjectIndex.from_objects(\n",
        "    query_engine_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju5JLvh9dhXb"
      },
      "source": [
        "#### OpenAI GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KML03j15bbKl"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model=\"gpt-4\")\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVtJB-4wbq-E",
        "outputId": "1f412929-0036-4c7b-bd1e-87f8473b2ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Uber in 2021?\n",
            "=== Calling Function ===\n",
            "Calling function: uber_10k with args: {\"input\": \"What is the revenue of Uber in 2021?\"}\n",
            "=== Function Output ===\n",
            "$17,455\n",
            "=== LLM Response ===\n",
            "The revenue of Uber in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Uber in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PypQerhAbreq",
        "outputId": "6b70eb98-42df-422a-e702-17b03d4c084c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The revenue of Uber in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSCyB3J1wbnD",
        "outputId": "bb75bf33-23d2-4313-f444-b5142c3c7734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Lyft in 2021?\n",
            "=== Calling Function ===\n",
            "Calling function: lyft_10k with args: {\"input\": \"What is the revenue of Lyft in 2021?\"}\n",
            "=== Function Output ===\n",
            "$3,208,323\n",
            "=== LLM Response ===\n",
            "The revenue of Lyft in 2021 was $3,208,323,000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Lyft in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW4ukYc1we8U",
        "outputId": "d00df400-9126-46f8-e12d-9e97385ae732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The revenue of Lyft in 2021 was $3,208,323,000.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFf1kpWudklG"
      },
      "source": [
        "#### Anthropic Sonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kVH4Sx5bw2q"
      },
      "outputs": [],
      "source": [
        "llm = Anthropic(model=\"claude-3-sonnet-20240229\")\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=True,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOfSijMb2t1",
        "outputId": "a7b4e447-4d0e-46ac-e838-73fd5de9599c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Uber in 2021?\n",
            "=== Calling Function ===\n",
            "Calling function: uber_10k with args: {\"input\": \"What was Uber's revenue in 2021?\"}\n",
            "=== Function Output ===\n",
            "Uber's revenue in 2021 was $17,455 million.\n",
            "=== LLM Response ===\n",
            "So Uber's revenue in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Uber in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wyJFAQCb3zZ",
        "outputId": "a893d687-e075-41ab-b788-1ff0352fc005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: So Uber's revenue in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xld82HwQwmqs",
        "outputId": "aa52fdaf-9583-45d4-f42a-be31931f5930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Lyft in 2021?\n",
            "=== LLM Response ===\n",
            "Okay, let me check Lyft's 2021 revenue using the provided tool:\n",
            "=== Calling Function ===\n",
            "Calling function: lyft_10k with args: {\"input\": \"What was Lyft's revenue in 2021?\"}\n",
            "=== Function Output ===\n",
            "Lyft's revenue in 2021 was $3,208,323.\n",
            "=== LLM Response ===\n",
            "So according to Lyft's 10-K filing, their revenue in 2021 was $3,208,323.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Lyft in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPoozUldwqAB",
        "outputId": "f69c1aec-af16-48c2-93b3-3b0f327d6909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: So according to Lyft's 10-K filing, their revenue in 2021 was $3,208,323.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AET5ffzlr88w"
      },
      "source": [
        "### MistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ1Ad-LscFHS"
      },
      "outputs": [],
      "source": [
        "llm = MistralAI(model=\"mistral-large-latest\")\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    allow_parallel_tool_calls=False,\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtOEyVCkr2iq",
        "outputId": "2c5d1338-f772-43c8-fa1a-1bc031dd1936"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Uber in 2021?\n",
            "=== Calling Function ===\n",
            "Calling function: uber_10k with args: {\"input\": \"What is the revenue of Uber in 2021?\"}\n",
            "=== Function Output ===\n",
            "The revenue of Uber in 2021 was $17,455 million.\n",
            "=== LLM Response ===\n",
            "The revenue of Uber in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Uber in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igwWqCeOr303",
        "outputId": "0d575ce0-3108-429d-f6d9-568072feaee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The revenue of Uber in 2021 was $17,455 million.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FXUKH34r69R",
        "outputId": "c836d0f6-d473-46ef-ff94-477f3482a583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What is the revenue of Lyft in 2021?\n",
            "=== Calling Function ===\n",
            "Calling function: lyft_10k with args: {\"input\": \"What is the revenue of Lyft in 2021?\"}\n",
            "=== Function Output ===\n",
            "The revenue of Lyft in 2021 was $3,208,323,000.\n",
            "=== LLM Response ===\n",
            "The revenue of Lyft in 2021 was $3,208,323,000.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\n",
        "    \"What is the revenue of Lyft in 2021?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M969QUxawyNF",
        "outputId": "dd95b01c-78a6-407d-f9f3-8a0374a74893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "assistant: The revenue of Lyft in 2021 was $3,208,323,000.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhSfavCswzrh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
