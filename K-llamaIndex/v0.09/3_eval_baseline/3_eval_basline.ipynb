{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LlamaIndex Bottoms-Up Development - Evaluation Baseline\n",
    "\n",
    "LlamaIndex provides some basic evaluation of query engines! We can setup an evaluator that will measure both hallucinations, as well as if the query was actually answered!\n",
    "\n",
    "This is provided by two main evaluations:\n",
    "\n",
    "- `ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection!\n",
    "- `QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered!\n",
    "\n",
    "You may have noticed that we are using an LLM for this task. That means we will want to pick a powerful LLM, like GPT-4 or Claude-2.\n",
    "\n",
    "Lastly, using these methods, we can also use the LLM to generate syntheic questions to evaluate with!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Baseline Query Engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_docs_bot.markdown_docs_reader import MarkdownDocsReader\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "def load_markdown_docs(filepath):\n",
    "    \"\"\"Load markdown docs from a directory, excluding all other file types.\"\"\"\n",
    "    loader = SimpleDirectoryReader(\n",
    "        input_dir=filepath, \n",
    "        required_exts=[\".md\"],\n",
    "        file_extractor={\".md\": MarkdownDocsReader()},\n",
    "        recursive=True\n",
    "    )\n",
    "\n",
    "    documents = loader.load_data()\n",
    "\n",
    "    # exclude some metadata from the LLM\n",
    "    for doc in documents:\n",
    "        doc.excluded_llm_metadata_keys = [\"File Name\", \"Content Type\", \"Header Path\"]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our documents from each folder.\n",
    "# we keep them seperate for now, in order to create seperate indexes later\n",
    "getting_started_docs = load_markdown_docs(\"../docs/getting_started\")\n",
    "community_docs = load_markdown_docs(\"../docs/community\")\n",
    "data_docs = load_markdown_docs(\"../docs/core_modules/data_modules\")\n",
    "agent_docs = load_markdown_docs(\"../docs/core_modules/agent_modules\")\n",
    "model_docs = load_markdown_docs(\"../docs/core_modules/model_modules\")\n",
    "query_docs = load_markdown_docs(\"../docs/core_modules/query_modules\")\n",
    "supporting_docs = load_markdown_docs(\"../docs/core_modules/supporting_modules\")\n",
    "tutorials_docs = load_markdown_docs(\"../docs/end_to_end_tutorials\")\n",
    "contributing_docs = load_markdown_docs(\"../docs/development\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Not Needed\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://localhost:1234/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext, set_global_service_context\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# create a global service context\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0))\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: API_KEY_HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     contributing_index \u001b[38;5;241m=\u001b[39m load_index_from_storage(StorageContext\u001b[38;5;241m.\u001b[39mfrom_defaults(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./contributing_index\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     getting_started_index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(getting_started_docs)\n\u001b[0;32m     16\u001b[0m     getting_started_index\u001b[38;5;241m.\u001b[39mstorage_context\u001b[38;5;241m.\u001b[39mpersist(persist_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./getting_started_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     community_index \u001b[38;5;241m=\u001b[39m VectorStoreIndex\u001b[38;5;241m.\u001b[39mfrom_documents(community_docs)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\base.py:112\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[1;34m(cls, documents, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[0;32m    105\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[0;32m    106\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     service_context\u001b[38;5;241m.\u001b[39mtransformations,\n\u001b[0;32m    108\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    110\u001b[0m )\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    113\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m    114\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m    115\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m    116\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    118\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:53\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     54\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     55\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     56\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     57\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     58\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     59\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     61\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\base.py:75\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m---> 75\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_index_from_nodes(\n\u001b[0;32m     76\u001b[0m         nodes \u001b[38;5;241m+\u001b[39m objects  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:274\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    267\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    268\u001b[0m ):\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    272\u001b[0m     )\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_from_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:246\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_nodes_to_index(\n\u001b[0;32m    247\u001b[0m         index_struct,\n\u001b[0;32m    248\u001b[0m         nodes,\n\u001b[0;32m    249\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs,\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:199\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m--> 199\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[0;32m    200\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:107\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[0;32m     99\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m embed_nodes(\n\u001b[0;32m    108\u001b[0m         nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39membed_model, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    109\u001b[0m     )\n\u001b[0;32m    111\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\indices\\utils.py:137\u001b[0m, in \u001b[0;36membed_nodes\u001b[1;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m--> 137\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mget_text_embedding_batch(\n\u001b[0;32m    138\u001b[0m     texts_to_embed, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[0;32m    142\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\core\\embeddings\\base.py:256\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[1;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cur_batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch_size:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# flush\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    253\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[0;32m    254\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[0;32m    255\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 256\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embeddings(cur_batch)\n\u001b[0;32m    257\u001b[0m         result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[0;32m    258\u001b[0m         event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    259\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    260\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[0;32m    261\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[0;32m    262\u001b[0m             },\n\u001b[0;32m    263\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai.py:413\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    412\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    414\u001b[0m     client,\n\u001b[0;32m    415\u001b[0m     texts,\n\u001b[0;32m    416\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    418\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai.py:178\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[1;32m--> 178\u001b[0m data \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_of_text, model\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:105\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m     99\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    100\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    108\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    109\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    110\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    111\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    112\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    113\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    114\u001b[0m     ),\n\u001b[0;32m    115\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    835\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    836\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    837\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    838\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    839\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    840\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vibud\\miniconda3\\Lib\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: API_KEY_HERE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "\n",
    "# create a vector store index for each folder\n",
    "try:\n",
    "    getting_started_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./getting_started_index\"))\n",
    "    community_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./community_index\"))\n",
    "    data_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./data_index\"))\n",
    "    agent_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./agent_index\"))\n",
    "    model_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./model_index\"))\n",
    "    query_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./query_index\"))\n",
    "    supporting_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./supporting_index\"))\n",
    "    tutorials_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./tutorials_index\"))\n",
    "    contributing_index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./contributing_index\"))\n",
    "except:\n",
    "    getting_started_index = VectorStoreIndex.from_documents(getting_started_docs)\n",
    "    getting_started_index.storage_context.persist(persist_dir=\"./getting_started_index\")\n",
    "\n",
    "    community_index = VectorStoreIndex.from_documents(community_docs)\n",
    "    community_index.storage_context.persist(persist_dir=\"./community_index\")\n",
    "\n",
    "    data_index = VectorStoreIndex.from_documents(data_docs)\n",
    "    data_index.storage_context.persist(persist_dir=\"./data_index\")\n",
    "\n",
    "    agent_index = VectorStoreIndex.from_documents(agent_docs)\n",
    "    agent_index.storage_context.persist(persist_dir=\"./agent_index\")\n",
    "\n",
    "    model_index = VectorStoreIndex.from_documents(model_docs)\n",
    "    model_index.storage_context.persist(persist_dir=\"./model_index\")\n",
    "\n",
    "    query_index = VectorStoreIndex.from_documents(query_docs)\n",
    "    query_index.storage_context.persist(persist_dir=\"./query_index\")    \n",
    "\n",
    "    supporting_index = VectorStoreIndex.from_documents(supporting_docs)\n",
    "    supporting_index.storage_context.persist(persist_dir=\"./supporting_index\")\n",
    "\n",
    "    tutorials_index = VectorStoreIndex.from_documents(tutorials_docs)\n",
    "    tutorials_index.storage_context.persist(persist_dir=\"./tutorials_index\")\n",
    "\n",
    "    contributing_index = VectorStoreIndex.from_documents(contributing_docs)\n",
    "    contributing_index.storage_context.persist(persist_dir=\"./contributing_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Query Engine Tools\n",
    "\n",
    "Since we have so many indicies, we can create a query engine tool for each and then use them in a single query engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool\n",
    "\n",
    "# create a query engine tool for each folder\n",
    "getting_started_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=getting_started_index.as_query_engine(), \n",
    "    name=\"Getting Started\", \n",
    "    description=\"Useful for answering questions about installing and running llama index, as well as basic explanations of how llama index works.\"\n",
    ")\n",
    "\n",
    "community_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=community_index.as_query_engine(),\n",
    "    name=\"Community\",\n",
    "    description=\"Useful for answering questions about integrations and other apps built by the community.\"\n",
    ")\n",
    "\n",
    "data_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=data_index.as_query_engine(),\n",
    "    name=\"Data Modules\",\n",
    "    description=\"Useful for answering questions about data loaders, documents, nodes, and index structures.\"\n",
    ")\n",
    "\n",
    "agent_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=agent_index.as_query_engine(),\n",
    "    name=\"Agent Modules\",\n",
    "    description=\"Useful for answering questions about data agents, agent configurations, and tools.\"\n",
    ")\n",
    "\n",
    "model_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=model_index.as_query_engine(),\n",
    "    name=\"Model Modules\",\n",
    "    description=\"Useful for answering questions about using and configuring LLMs, embedding modles, and prompts.\"\n",
    ")\n",
    "\n",
    "query_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_index.as_query_engine(),\n",
    "    name=\"Query Modules\",\n",
    "    description=\"Useful for answering questions about query engines, query configurations, and using various parts of the query engine pipeline.\"\n",
    ")\n",
    "\n",
    "supporting_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=supporting_index.as_query_engine(),\n",
    "    name=\"Supporting Modules\",\n",
    "    description=\"Useful for answering questions about supporting modules, such as callbacks, service context, and avaluation.\"\n",
    ")\n",
    "\n",
    "tutorials_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=tutorials_index.as_query_engine(),\n",
    "    name=\"Tutorials\",\n",
    "    description=\"Useful for answering questions about end-to-end tutorials and giving examples of specific use-cases.\"\n",
    ")\n",
    "\n",
    "contributing_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=contributing_index.as_query_engine(),\n",
    "    name=\"Contributing\",\n",
    "    description=\"Useful for answering questions about contributing to llama index, including how to contribute to the codebase and how to build documentation.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Unified Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[\n",
    "        getting_started_tool,\n",
    "        community_tool,\n",
    "        data_tool,\n",
    "        agent_tool,\n",
    "        model_tool,\n",
    "        query_tool,\n",
    "        supporting_tool,\n",
    "        tutorials_tool,\n",
    "        contributing_tool\n",
    "    ],\n",
    "    # enable this for streaming\n",
    "    # response_synthesizer=get_response_synthesizer(streaming=True),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Query Engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install LlamaIndex, you can follow these steps:\n",
      "\n",
      "1. Install the package using pip by running the command: `pip install llama-index`.\n",
      "2. Clone the repository using Git by running the command: `git clone https://github.com/jerryjliu/llama_index.git`.\n",
      "3. If you want to do an editable install of just the package itself, run the command: `pip install -e .`.\n",
      "4. If you want to install optional dependencies and dependencies used for development, run the command: `pip install -r requirements.txt`.\n",
      "\n",
      "Please note that these steps assume you have pip and Git installed on your system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"How do I install llama index?\")\n",
    "print(str(response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Basline!\n",
    "\n",
    "Now that we have our baseline query engine created, we can create a basic evaluation pipeline!\n",
    "\n",
    "Our pipeline will:\n",
    "\n",
    "- Generate a small dataset of questions\n",
    "- Save/cache these questions (so we can properly compare performance later!)\n",
    "- Evaluate both response quality and hallucination\n",
    "\n",
    "To do this reliably, we need to use an LLM smarter than `gpt-3.5-turbo`, so we will setup `gpt-4` for the evaluation process!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Dataset\n",
    "\n",
    "In order to make the question generation more effecient, we can remove small documents and combine all documents into a giant single docoument.\n",
    "\n",
    "I also modify the question generation prompt, to generate a single question for each chunk, along with extra context for what it is reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\"../docs\", recursive=True, required_exts=[\".md\"]).load_data()\n",
    "\n",
    "all_text = \"\"\n",
    "\n",
    "for doc in documents:\n",
    "    all_text += doc.text\n",
    "\n",
    "giant_document = Document(text=all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.prompts import Prompt\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "\n",
    "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(llm=\"gpt-4\", temperature=0))\n",
    "\n",
    "question_dataset = []\n",
    "if os.path.exists(\"question_dataset.txt\"):\n",
    "    with open(\"question_dataset.txt\", \"r\") as f:\n",
    "        for line in f:\n",
    "            question_dataset.append(line.strip())\n",
    "else:\n",
    "    # generate questions\n",
    "    data_generator = DatasetGenerator.from_documents(\n",
    "        [giant_document],\n",
    "        text_question_template=Prompt(\n",
    "            \"A sample from the LlamaIndex documentation is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Using the documentation sample, carefully follow the instructions below:\\n\"\n",
    "            \"{query_str}\"\n",
    "        ),\n",
    "        question_gen_query=(\n",
    "            \"You are an evaluator for a search pipeline. Your task is to write a single question \"\n",
    "            \"using the provided documentation sample above to test the search pipeline. The question should \"\n",
    "            \"reference specific names, functions, and terms. Restrict the question to the \"\n",
    "            \"context information provided.\\n\"\n",
    "            \"Question: \"\n",
    "        ),\n",
    "        # set this to be low, so we can generate more questions\n",
    "        service_context=gpt4_service_context\n",
    "    )\n",
    "    generated_questions = data_generator.generate_questions_from_nodes()\n",
    "\n",
    "    # randomly pick 40 questions from each dataset\n",
    "    generated_questions = random.sample(generated_questions, 40)\n",
    "    question_dataset.extend(generated_questions)\n",
    "\n",
    "    print(f\"Generated {len(question_dataset)} questions.\")\n",
    "\n",
    "    # save the questions!\n",
    "    with open(\"question_dataset.txt\", \"w\") as f:\n",
    "        for question in question_dataset:\n",
    "            f.write(f\"{question.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the function used to specify the metadata visible to the embedding model and how can it be customized?', 'How can I convert tools to LangChain tools using the provided documentation sample?', 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?', 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?', 'What is the default number of LLM calls required for the ListIndex?']\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(question_dataset, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with the Dataset\n",
    "\n",
    "Now that we have our dataset, let's measure performance!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for response in responses:\n",
    "            eval_result = 1 if \"YES\" in evaluator.evaluate(response) else 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Hallucination? Scored 29 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import ResponseEvaluator\n",
    "\n",
    "# gpt-4 evaluator!\n",
    "evaluator = ResponseEvaluator(service_context=gpt4_service_context)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Hallucination? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?'\n",
      " 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?'\n",
      " 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?'\n",
      " 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the purpose of the `insert_terms` function in the LlamaIndex app?'\n",
      " 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?'\n",
      " 'What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hallucinated_questions = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(hallucinated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context information, there is no specific mention of the \"GuidancePydanticProgram\" class in the LlamaIndex documentation. Therefore, it is not possible to determine the purpose of this class without further information.\n",
      "-----------------\n",
      "> Source (Doc id: bbf2dedc-90fb-49e8-be57-28a2783ea7c1): Sub question: What is the purpose of the GuidancePydanticProgram class?\n",
      "Response: Based on the given context information, there is no specific mention of the \"GuidancePydanticProgram\" class. Therefore, it is not possible to determine the purpose of this class without further information....\n",
      "\n",
      "> Source (Doc id: b393e1a2-75d2-4017-bd12-d3b536aa3c4a): Sub question: How do I install and run LlamaIndex?\n",
      "Response: To install LlamaIndex, you can use the pip package manager by running the command \"pip install llama-index\" in your terminal or command prompt.\n",
      "\n",
      "After installing LlamaIndex, you can use the following starter example to run it:\n",
      "\n",
      "1. Import the LlamaIndex module in your Python script:\n",
      "   ```python\n",
      "   import llama_index\n",
      "   ```\n",
      "\n",
      "2. Use the LlamaIndex functions and classes as needed in your script.\n",
      "\n",
      "Make sure you have Python and pip installed on your system before running the installation command....\n",
      "\n",
      "> Source (Doc id: a9c75753-964e-4fb3-9798-b6b8ba6c044b): Sub question: What are the basic explanations of how LlamaIndex works?\n",
      "Response: LlamaIndex is a platform that allows you to create LLM-powered applications using custom data. The platform follows the retrieval augmented generation (RAG) paradigm, which combines LLM (Language Model) with custom data. \n",
      "\n",
      "The basic explanation of how LlamaIndex works is by using LLM to generate responses based on the given input. It does this by retrieving relevant information from the custom data and then using LLM to generate a response that is relevant to the input.\n",
      "\n",
      "To compose your own RAG pipeline in LlamaIndex, you need to understand key concepts and modules. These include understanding how to retrieve information from the custom data, how to use LLM for generation, and how to combine these components effectively to create your desired application.\n",
      "\n",
      "Overall, LlamaIndex provides a framework for building LLM-powered applications by combining LLM with custom data, allowing you to create applications...\n",
      "\n",
      "> Source (Doc id: 60e1c688-4acc-428b-9282-240d0c7092df): Sub question: Are there any integrations or other apps built by the community for LlamaIndex?\n",
      "Response: Yes, there are integrations and other apps built by the community for LlamaIndex....\n",
      "\n",
      "> Source (Doc id: 8880918b-13f3-4695-b805-bb2d99df393b): Sub question: What are data loaders, documents, nodes, and index structures in LlamaIndex?\n",
      "Response: In LlamaIndex, data loaders are components that are responsible for ingesting and loading external data into the system. They handle the process of converting the external data into LlamaIndex's internal representation, which consists of documents and nodes.\n",
      "\n",
      "Documents in LlamaIndex refer to the ingested data, which can be text or any other form of content. These documents are internally parsed and chunked into smaller units called nodes. Nodes represent chunks of text from the documents and serve as the basic building blocks for indexing and querying.\n",
      "\n",
      "Index structures in LlamaIndex are the data structures used to organize and store the index metadata. The index metadata includes information about the nodes, such as their location and properties, which enables efficient retrieval and querying of the data. LlamaIndex supports customizable index stores, allowing users to choose the st...\n",
      "\n",
      "> Source (Doc id: adb3de98-42f7-4ea9-8c53-67c14aa30000): Sub question: How do I work with data agents, agent configurations, and tools in LlamaIndex?\n",
      "Response: To work with data agents, agent configurations, and tools in LlamaIndex, you need to understand the core components and functionalities involved.\n",
      "\n",
      "Data agents in LlamaIndex are knowledge workers powered by LLM (Llama Language Model). They can perform various tasks on your data, both in a \"read\" and \"write\" function. They are capable of automated search and retrieval of different types of data, including unstructured, semi-structured, and structured data. Additionally, they can call external service APIs in a structured manner and process the response, storing it for later use.\n",
      "\n",
      "To build a data agent, you need two core components: a reasoning loop and tool abstractions. The reasoning loop helps the agent decide which tools to use, in what sequence, and with what parameters, based on the input task. The tool abstractions represent the APIs or tools that the agent can interact with. T...\n",
      "\n",
      "> Source (Doc id: e274a384-c44f-429a-9f4b-044b7f80ed2f): Sub question: What is the purpose of LLMs, embedding models, and prompts in LlamaIndex?\n",
      "Response: The purpose of LLMs (Large Language Models) in LlamaIndex is to provide expressive power and enhance the functionality of the framework. LLMs can be used as standalone modules or integrated into other core LlamaIndex components. They are primarily used during the response synthesis step, but depending on the type of index being used, they may also be utilized during index construction, insertion, and query traversal.\n",
      "\n",
      "Embedding models are used in LlamaIndex to generate embeddings or representations of text data. These embeddings can be used for various tasks such as similarity matching, clustering, or classification. Embedding models help in organizing and structuring the data within LlamaIndex.\n",
      "\n",
      "Prompts play a crucial role in LlamaIndex as they are used for various purposes. They are used to build the index, perform insertion, traverse queries, and synthesize the final answer. LlamaInd...\n",
      "\n",
      "> Source (Doc id: 5c3e397e-95e1-4a9a-8bf6-0c6a1d7f3589): Sub question: How do I use query engines, query configurations, and the query engine pipeline in LlamaIndex?\n",
      "Response: To use query engines, query configurations, and the query engine pipeline in LlamaIndex, you need to follow these steps:\n",
      "\n",
      "1. Import the necessary modules from the LlamaIndex library. In this case, you need to import the `VectorStoreIndex`, `get_response_synthesizer`, `VectorIndexRetriever`, and `RetrieverQueryEngine` modules.\n",
      "\n",
      "2. Create an instance of the `VectorStoreIndex` class. This class represents the index structure that you want to perform queries on.\n",
      "\n",
      "3. Create an instance of the `VectorIndexRetriever` class, passing the `VectorStoreIndex` instance as a parameter. This class is responsible for retrieving responses from the index.\n",
      "\n",
      "4. Create an instance of the `RetrieverQueryEngine` class, passing the `VectorIndexRetriever` instance as a parameter. This class represents the query engine that will execute the queries and retrieve the responses.\n",
      "\n",
      "5. Configure t...\n",
      "\n",
      "> Source (Doc id: 5c1412b2-165a-45f8-aa66-019bbae04d35): Sub question: What are the supporting modules in LlamaIndex, such as callbacks, service context, and evaluation?\n",
      "Response: The supporting modules in LlamaIndex include callbacks, service context, and evaluation....\n",
      "\n",
      "> Source (Doc id: 66e6e99d-20d9-48f5-b41c-2ab714af20a1): Sub question: Are there any end-to-end tutorials or examples of specific use-cases for LlamaIndex?\n",
      "Response: Based on the given context information, it is not explicitly mentioned whether there are any end-to-end tutorials or examples of specific use-cases for LlamaIndex....\n",
      "\n",
      "> Source (Doc id: bae0667b-f1e5-4303-906a-2ff1c823181a): Sub question: How can I contribute to LlamaIndex, including contributing to the codebase and building documentation?\n",
      "Response: To contribute to LLamaIndex, you can start by contributing to its codebase. This can be done by forking the LLamaIndex repository on a platform like GitHub, making your desired changes or additions to the code, and then submitting a pull request to the main LLamaIndex repository. The LLamaIndex team will review your changes and, if approved, merge them into the codebase.\n",
      "\n",
      "In addition to code contributions, you can also contribute to LLamaIndex by building documentation. This can involve creating or updating documentation that helps users understand how to use LLamaIndex, its features, and its configuration options. You can contribute to the documentation by submitting pull requests to the LLamaIndex documentation repository.\n",
      "\n",
      "By contributing to the codebase and building documentation, you can help improve LLamaIndex and make it more accessible and useful for...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `RouterQueryEngine` in LlamaIndex is to perform query transformations over index structures. It is responsible for converting a query into another query, either in a single-step or multi-step process. Additionally, the `RouterQueryEngine` supports streaming the response as it is being generated, allowing for the processing or printing of the beginning of the response before the full response is finished, thereby reducing the perceived latency of queries.\n",
      "\n",
      "As for how the `RouterQueryEngine` can be used in the search pipeline, the given context information does not provide any specific details. Therefore, without prior knowledge, it is not possible to determine the exact usage of the `RouterQueryEngine` in the search pipeline.\n",
      "-----------------\n",
      "> Source (Doc id: b1fba297-c258-4d6f-9f18-205177456c59): Sub question: What is the purpose of the `RouterQueryEngine` in LlamaIndex?\n",
      "Response: Based on the given context information, the purpose of the `RouterQueryEngine` in LlamaIndex is to perform query transformations over index structures. It is responsible for converting a query into another query, either in a single-step or multi-step process. The `RouterQueryEngine` also supports streaming the response as it is being generated, allowing for the processing or printing of the beginning of the response before the full response is finished, thereby reducing the perceived latency of queries....\n",
      "\n",
      "> Source (Doc id: 8cefb2af-73d6-4edf-baab-13651453ea56): Sub question: How can the `RouterQueryEngine` be used in the search pipeline?\n",
      "Response: The given context information does not provide any specific details about the `RouterQueryEngine`. Therefore, without prior knowledge, it is not possible to determine how the `RouterQueryEngine` can be used in the search pipeline....\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `RouterQueryEngine` in LlamaIndex and how can it be used in the search pipeline?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Response for Answer Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from llama_index import Response\n",
    "\n",
    "def evaluate_query_engine(evaluator, query_engine, questions):\n",
    "    async def run_query(query_engine, q):\n",
    "        try:\n",
    "            return await query_engine.aquery(q)\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "\n",
    "    total_correct = 0\n",
    "    all_results = []\n",
    "    for batch_size in range(0, len(questions), 5):\n",
    "        batch_qs = questions[batch_size:batch_size+5]\n",
    "\n",
    "        tasks = [run_query(query_engine, q) for q in batch_qs]\n",
    "        responses = asyncio.run(asyncio.gather(*tasks))\n",
    "        print(f\"finished batch {(batch_size // 5) + 1} out of {len(questions) // 5}\")\n",
    "\n",
    "        for question, response in zip(batch_qs, responses):\n",
    "            eval_result = 1 if \"YES\" in evaluator.evaluate(question, response) else 0\n",
    "            total_correct += eval_result\n",
    "            all_results.append(eval_result)\n",
    "        \n",
    "        # helps avoid rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return total_correct, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished batch 1 out of 8\n",
      "finished batch 2 out of 8\n",
      "finished batch 3 out of 8\n",
      "finished batch 4 out of 8\n",
      "finished batch 5 out of 8\n",
      "finished batch 6 out of 8\n",
      "finished batch 7 out of 8\n",
      "finished batch 8 out of 8\n",
      "Response satisfies the query? Scored 19 out of 40 questions correctly.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.evaluation import QueryResponseEvaluator\n",
    "\n",
    "evaluator = QueryResponseEvaluator(service_context=gpt4_service_context)\n",
    "\n",
    "total_correct, all_results = evaluate_query_engine(evaluator, query_engine, question_dataset)\n",
    "\n",
    "print(f\"Response satisfies the query? Scored {total_correct} out of {len(question_dataset)} questions correctly.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating Incorrect Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the purpose of the `GuidancePydanticProgram` class in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SubQuestionQueryEngine class in LlamaIndex?'\n",
      " 'What are the available options for the storage backend of the index store in LlamaIndex?'\n",
      " 'What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?'\n",
      " \"What is the purpose of the `CollectionQueryConsumer` class in the Delphic application's WebSocket handling?\"\n",
      " 'What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?'\n",
      " 'How can I create a Django superuser using the Delphic application?'\n",
      " 'What is the default number of LLM calls required for the ListIndex?'\n",
      " 'What are the different vector stores supported by LlamaIndex for use as the storage backend for `VectorStoreIndex`?'\n",
      " 'What is the purpose of the \"router query engine\" in the LlamaIndex framework?'\n",
      " 'What storage backends are supported by LlamaIndex for persisting data?'\n",
      " 'What is the purpose of the `fetchDocuments` function in the `fetchDocuments.tsx` file in the React frontend?'\n",
      " 'What is the purpose of the `RefinePrompt` class in the LlamaIndex documentation?'\n",
      " \"What is the function used to retrieve the collections for the logged-in user in the Delphic project's frontend?\"\n",
      " 'What is the purpose of the ResponseEvaluator class in the LlamaIndex library?'\n",
      " 'What is the purpose of the Algovera tool built on top of LlamaIndex?'\n",
      " 'What is the purpose of the HyDE query transform in the LlamaIndex?'\n",
      " 'What are the three primary sections within the layout of the ChatView component?'\n",
      " 'What is the purpose of the `insert_terms` function in the LlamaIndex app?'\n",
      " 'What is the purpose of the `load_collection_model` function in the LlamaIndex documentation?'\n",
      " 'What is the purpose of the SQLTableNodeMapping object in the LlamaIndex documentation sample?']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unanswered_queries = np.array(question_dataset)[np.array(all_results) == 0]\n",
    "print(unanswered_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `ReActAgent` is to instantiate an agent from a set of Tools. It can be initialized with other agents as tools by passing them as parameters to the `from_tools()` method.\n",
      "-----------------\n",
      "> Source (Doc id: 809486e9-ee91-42cb-a620-66c534c6890f): Sub question: What is the purpose of the ReActAgent?\n",
      "Response: The purpose of the ReActAgent is to instantiate an agent from a set of Tools....\n",
      "\n",
      "> Source (Doc id: 581d674e-6550-4a40-aa78-8936b53867e8): Sub question: How can the ReActAgent be initialized with other agents as tools?\n",
      "Response: Based on the given context information, it is not possible to initialize the ReActAgent with other agents as tools. The ReActAgent can only be initialized with a s...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the `ReActAgent` and how can it be initialized with other agents as tools?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a high-level interface for ingesting, indexing, and querying external data. It allows users to customize storage components such as document stores, index stores, and vector stores. The LoadAndSearchToolSpec also supports persisting data to various storage backends and offers different ways of querying a list index, including embedding-based queries and keyword filters.\n",
      "-----------------\n",
      "> Source (Doc id: 8ad62670-cacc-4a34-bb9b-00504904b04c): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information....\n",
      "\n",
      "> Source (Doc id: f5974d32-081a-4366-9e8a-ea9b4bccbdaf): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a high-level interface for ingesting, indexing, and querying...\n",
      "\n",
      "> Source (Doc id: 69d8cee2-a4eb-4183-acbe-34047f3b2649): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a tool specification that can be used to load and search dat...\n",
      "\n",
      "> Source (Doc id: be322260-e24b-4dc3-9540-8b973b1afecf): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, it is not possible to determine the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation. T...\n",
      "\n",
      "> Source (Doc id: f0f035ac-2c2d-4ba6-b84b-77d88bb7aaed): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, there is no mention of the \"LoadAndSearchToolSpec\" in the LlamaIndex documentation. Therefore, it is not po...\n",
      "\n",
      "> Source (Doc id: b0384b51-bb28-48dd-98fd-63f57d1c4791): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: Based on the given context information, there is no mention of the \"LoadAndSearchToolSpec\" in the LlamaIndex documentation. Therefore, the purpose ...\n",
      "\n",
      "> Source (Doc id: 9d0c096e-9049-4c36-9f1a-5b166c93a635): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is not mentioned in the given context information....\n",
      "\n",
      "> Source (Doc id: 65259c15-1ad2-4211-bd11-7c7b6f182cf8): Sub question: What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?\n",
      "Response: The purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation is to provide a tool that allows users to load and search for embeddings a...\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('What is the purpose of the LoadAndSearchToolSpec in the LlamaIndex documentation?')\n",
    "print(str(response))\n",
    "print(\"-----------------\")\n",
    "print(response.get_formatted_sources(length=256))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we covered several key topics!\n",
    "\n",
    "- setting up a sub-question query engine\n",
    "- generating a dataset of evaluation questions\n",
    "- evaluating responses for hallucination\n",
    "- evaluating responses for answer quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
