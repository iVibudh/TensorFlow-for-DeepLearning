# Notes

These notes are from the DeepLearning.ai course on [Building Systems with the ChatGPT API](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)

The topics covered in the lectures are as follows:
- Introduction 
- Language Modles, the Chat Format and Tokens
- Classification (Evaluate Inputs)
- Moderation 
- Chain of Thought Reasoning 
- Chaining Prompts
- Check Outputs
- Build an End-to-End System
- Evaluation Part 1 
- Evaluation Part 2 
- Summary 

## 1. Introduction 
 Best practices for building a complex application in LLM. More internal steps than visible to a user. 
- Evaluate the input to check if there is any problematic content, such as, hateful speech 
- Next, it to process the input 
evaluate what type of querry is it - complaint, product inquiry, 
- Retrieve the relevant information 
- Use LLM to build a helpful response
- Check the optput to understand if the output is problematic, such as, inacurate or inappropriate
- Best practices for evaluating and improving your system over time

## 2. Language Modles, the Chat Format and Tokens
- How does LLM model work 
- Two types of LLM models 
    - Base LLM 
    - Instruction Tuned LLM 
- Getting from a Base LLM to an instruction tuned LLM:
    - Train a Base LLM on a lot of data.
    - Further train the model:
        - Fine-tune on examples of where the input/output data
        - Obtain human-ratings of the quality of different LLM outputs - helpful, honest and harmless
        - Tune LLM to increase probability that it generates the more highly rated outputs (using RLHF: Reinforcement Learning from Human Feedback)
- Limitations due to Tokens 
- Helper function - chat format 
    - usage of system messages
- Responsible use of Open AI API key in .env file


## 3. Classification (Evaluate Inputs)
- Evaluate inputs to ensure safety and security of the system.

- For tasks in which a lot of different sets of instructions are needed top handle different cases, it can be beneficial to first classify the type of querry and then determine the type of instructions to use. Use fixed categories and hard coded instructions for handling tasks

## 4. Moderation 
- If you are building a system where the users can input information. It is important to check if the users are using the system responsibly.

 -Commpon types of system abuse:
    - Hateful speech (Use Moderation API to avoid)
 - Prompt injections (usage of prompts to detect prompt injections)
 
## 5. Chain of Thought Reasoning 
- Usually, this chain of thought reasoning often gives more useful and better results as model can reason in detail before answering a specific question.

- If we can reframe the query to ask a series of relevant steps before the model provides it's final answer, the model think longer and more methodically about the problem.

- **Inner Monologue**
    - provide the instruction about following the **reasoning steps as a system message**
    - the reasoning sections are asked to be presented in a structured format
    - this allows the developer to parse out the irrelevant sections of the model response (in this case, reasoning)

## 6. Chaining Prompts
- It is more tedious to break a large complex prompt into a chain of simpler prompts. However, chaining propts usually help us in getting the correect reasoning at each step.
- An example of a Chatbot which can give information on products is given in the notebook. We can build on top of the instructions to help it to make it do more tasks.


Why use chain prompting?
- More focused (breaks down a complex task)
- Context Limitations (Max tokens for input and output response)
- Reduced Costs (pay per token) dynamically allowing relevant information to be taken in


There are more advanced ways to retrieve contextual information 
- Text embeddings (allows you to retrieve relevant information more effectively)
    - it enables fuzzy or semantics search which allows us to retrive information more effectively, without using the exact keywords. 

## 7. Check Outputs 
Here we focus on the checking the outputs generated by the system. This step can be important to ensure the quality, relevance and safety of the responses provided. We can also use automation flows. 

- We will learn how to use the moderation API (this time for Outputs)
- How to use additional propmpts to the model to evaluate output quality before displying them. 


## 8. Build an End-to-End System

This puts together the chain of prompts that you saw throughout the course. We will build a customer service assistant following these steps
- Step 1: Input passed moderation check.
    - if it doesn't we send an appropriate response, such as, "Sorry, we cannot process this request."
- Step 2: If it passes, Extracted list of products.
- Step 3: Looked up product information.
- Step 4: Generated response to user question.
- Step 5: Response passed moderation check.
- Step 6: Model evaluated the response.
- Step 7: Model approved the response.

## 9. Evaluation Part 1 (there is a single "right answer")
- Some best practices to evaluate the output of an LLM.
- How does it feel to build such a system? How is it different from a traditional supervised Machine Learning workflow?

**Process of building an application**
- Tune prompts on handful of examples
    - based on the first few explamples make sure that your prompts work.
- Add additional "tricky" examples opportunistically 
    - Once the model is build, you undergo additional testing on your system
    - You keep these test cases handy to see how newer versions of the model perform on them
- Develop metrices to measure performance in examples
    - Once you have a considerable number of examples that you know could be tricky 
    - You can develop a way to test the performance of the LLM system autumatically, rather than testing each use case individually. 
    - Maybe have "Average Accuracy" to measure the performance of the model. 
- Collect randomly sampled set of examples to tune to (development set/hold-out cross validation set)
    - In case the examples that you have explored are not giving you enough confidence, then you can collect randomly sampled examples to continue to **"tune your prompt"**
- Collect and use a hand-out test set.
    - Only if you need a even higher fidelity estimate of the performance of your system, then you can collect a hold out test set that you don't even look at yourself when you tune he model. 
    
    
When you are building high stakes applications, where there is risk of bias or an inappropriate output causing harm to someone, they as developers we have the responsibility to collect the test set. We need to rigourously evaluate the performance of the system to make sure that it is doing the right thing.

## 10. Evaluation Part 2 (there isn't a single "right answer.")

# L10: Evaluation Part II

Evaluate LLM responses where there isn't a single "right answer."

In cases when there isn't a single "right answer", there are 2 ways by which you can evaluate the performance of the model:
- **Evaluation Framework 1:** You can define a rubric, where you assess the performance of a model. Here is a sample rubric:
    - Is the Assistant response based only on the context provided? (Y or N)
    - Does the answer include information that is not provided in the context? (Y or N)
    - Is there any disagreement between the response and the context? (Y or N)
    - Count how many questions the user asked. (output a number)
    - For each question that the user asked, is there a corresponding answer to it?
    - Of the number of questions asked, how many of these questions were addressed by the answer? (output a number)
    
**Note:** It might be useful to deploy a cheaper model (eg. ChatGPT-3.5-Turbo) and use a more advanced and expensive model (eg. ChatGPT-4) for the evaluation.

- **Evaluation Framework 2:** Evaluate the LLM's answer to the user based on an "ideal" / "expert" response. It will be unreasonable for any LLM to generate this exact "ideal answer word by word. However, we can use one of the 2 approaches:
    1. Calculate the similarity of the LLM response with the "ideal" response. You can use BLEU score (BiLingual Evaluation Understudy) for this task. 
    2. **(Best Way)** You can use a propmt to ask the LLM, to compare how well the automated generated response compare to the "ideal" response. You can use the **OpenAI's open scource eval framework** ([OpenAI evals](https://github.com/openai/evals/blob/main/evals/registry/modelgraded/fact.yaml)). 
    The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
        - **(Best Case)** (A) The submitted answer is a subset of the expert answer and is fully consistent with it. 
        - **(Maybe some hallucination)** (B) The submitted answer is a superset of the expert answer and is fully consistent with it. 
        - **(Good enough)** (C) The submitted answer contains all the same details as the expert answer. 
        - **(Worst case)** (D) There is a disagreement between the submitted answer and the expert answer. 
        - **(Good enough)** (E) The answers differ, but these differences don't matter from the perspective of factuality. <br>
  choice_strings: ABCDE

## 11. Summary 
- Details of how an LLM works. 
- Details covered - tokenizer, reverse a string
- Methods to evaluate the user input to ensure quality and safety of the system 
- Processing inputs using 
    - chain of though reasoning 
    - splitting tasks into sub tasks with chain prompts
- Checking outputs before showing them to the user. 
- Methods to evaluate the system over time to monitor and improve its performance
- Building responsibly with these tools, ensuring that the model is safe and provides appropriate responses that are accurate, relevant and in the tone that you want. 




