# Generative AI with Large Language Models

This course will take a deep dive with you into how LLM technology actually works including going through many of the technical details, like model training, instruction tuning, fine-tuning, etc.

- Week 1: examine the transformer architecture that powers large language models, explore how these models are trained, and understand the compute resources required to develop these powerful LLMs. You'll also learn about a technique called in-context learning. How to guide the model to output at inference time with prompt engineering, and how to tune the most important generation parameters of LLMs for tuning your model output. 
- Lab 1: construct a compare different prompts and inputs for a given generative task, in this case, dialogue summarization. You'll also explore different inference parameters and sampling strategies to gain intuition on how to further improve the generative model of responses.
- Week 2: explore options for adapting pre-trained models to specific tasks and datasets via a process called instruction fine tuning. 
- Lab 2: fine tune it existing large language model from Hugging Face, a popular open-source model hub. You'll play with both full fine-tuning and parameter efficient fine tuning or PEFT for short. You'll see how PEFT lets you make your workflow much more efficient.
- Week 3: to align the output of language models with human values in order to increase helpfulness and decrease potential harm and toxicity. 
- Lab 3: hands-on with reinforcement learning from human feedback or RLHF, you'll build a reward model classifier to label model responses as either toxic or non-toxic.

### Week 1

#### Introduction to LLMs and generative AI projects
