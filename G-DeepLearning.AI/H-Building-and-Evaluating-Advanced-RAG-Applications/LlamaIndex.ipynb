{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xil8_DI8gIyK",
        "outputId": "9ef981c2-8c40-4eec-8b19-d1b248a5af1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llama_index'...\n",
            "remote: Enumerating objects: 16817, done.\u001b[K\n",
            "remote: Counting objects: 100% (3393/3393), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1316/1316), done.\u001b[K\n",
            "remote: Total 16817 (delta 2628), reused 2268 (delta 2070), pack-reused 13424\u001b[K\n",
            "Receiving objects: 100% (16817/16817), 45.34 MiB | 18.07 MiB/s, done.\n",
            "Resolving deltas: 100% (11017/11017), done.\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "YT Ref -> https://www.youtube.com/watch?v=cNMYeW2mpBs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/jerryjliu/llama_index.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_UJ1jdIaBsw",
        "outputId": "aac70082-5c5e-41f7-f827-deee32e8c064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.6.28-py3-none-any.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.5/515.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama_hub\n",
            "  Downloading llama_hub-0.0.3-py3-none-any.whl (335 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.0/335.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses-json (from llama-index)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchain>=0.0.154 (from llama-index)\n",
            "  Downloading langchain-0.0.205-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy>=2.0.15 (from llama-index)\n",
            "  Downloading SQLAlchemy-2.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.2)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.26.15)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index)\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect==0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting tiktoken (from llama-index)\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect==0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting atlassian-python-api (from llama_hub)\n",
            "  Downloading atlassian-python-api-3.39.0.tar.gz (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.5/157.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting html2text (from llama_hub)\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from llama_hub) (5.9.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.154->llama-index) (6.0)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain>=0.0.154->llama-index)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain>=0.0.154->llama-index)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting langchainplus-sdk>=0.0.9 (from langchain>=0.0.154->llama-index)\n",
            "  Downloading langchainplus_sdk-0.0.13-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.154->llama-index) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain>=0.0.154->llama-index)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.154->llama-index) (1.10.7)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json->llama-index)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json->llama-index)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.26.4->llama-index) (4.65.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index) (2.0.2)\n",
            "Collecting deprecated (from atlassian-python-api->llama_hub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api->llama_hub) (1.16.0)\n",
            "Requirement already satisfied: oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api->llama_hub) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api->llama_hub) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2022.7.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->llama-index) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama-index)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama-index) (23.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->atlassian-python-api->llama_hub) (1.14.1)\n",
            "Building wheels for collected packages: wikipedia, atlassian-python-api\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=a9298ad27c35bc0a3fda75dfa8248bb5d06a25e40726c72746400850d833bba4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for atlassian-python-api (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for atlassian-python-api: filename=atlassian_python_api-3.39.0-py3-none-any.whl size=163450 sha256=dd2ed206b6eda98ef8656b39cd689ee570ff7423c4a851b9742bb77f4ef5f930\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/11/7b/591be4ed711f64315323e034c010408771c9c0cf88d1f08db2\n",
            "Successfully built wikipedia atlassian-python-api\n",
            "Installing collected packages: sqlalchemy, mypy-extensions, multidict, marshmallow, html2text, fsspec, frozenlist, deprecated, async-timeout, yarl, wikipedia, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, atlassian-python-api, aiohttp, openai, langchain, llama-index, llama_hub\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 atlassian-python-api-3.39.0 dataclasses-json-0.5.8 deprecated-1.2.14 frozenlist-1.3.3 fsspec-2023.6.0 html2text-2020.1.16 langchain-0.0.205 langchainplus-sdk-0.0.13 llama-index-0.6.28 llama_hub-0.0.3 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 sqlalchemy-2.0.16 tiktoken-0.4.0 typing-inspect-0.8.0 wikipedia-1.4.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama_hub wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPFmLmrWhsb1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"COPY AND PASTE YOUR OPENAI API HERE\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL1wjI7StrFg"
      },
      "source": [
        "# Data connectors (LlamaHub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgNV_zwtbO-C"
      },
      "outputs": [],
      "source": [
        "from llama_hub.wikipedia.base import WikipediaReader\n",
        "\n",
        "loader = WikipediaReader()\n",
        "documents = loader.load_data(pages=['Berlin', 'Rome', 'Tokyo', 'Canberra', 'Santiago'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8hSbUjUT4VK"
      },
      "source": [
        "# Basic query functionalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Goo4yvAnTN_5"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "# build an index over these Document objects.\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "# you can query an index with the default QueryEngine\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"How many people live in Berlin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzDGa1dATtyX",
        "outputId": "85965939-8f51-4202-c71e-baf3e7921393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "According to the context information, Berlin has a population of approximately 3.7 million inhabitants.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed8v3NF990f6"
      },
      "source": [
        "# Query Multiple Documents:\n",
        "Source: https://gpt-index.readthedocs.io/en/latest/examples/usecases/10q_sub_question.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk9PnvfwCyUz"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vogXg-_Y-DHS"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader, LLMPredictor, ServiceContext, VectorStoreIndex\n",
        "from llama_index.response.pprint_utils import pprint_response\n",
        "from langchain import OpenAI\n",
        "\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obFzvbnF95mX"
      },
      "outputs": [],
      "source": [
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\", max_tokens=-1, streaming=True))\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZyv2CR-MzJ",
        "outputId": "ffb3bce3-230c-4a10-d225-09b27474c1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.10.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.9/255.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvF4mfiyES6W",
        "outputId": "a8764a9f-c158-4ce7-9a54-4db2a5e93ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama_index/docs/examples/data/10q/uber_10q_sept_2022.pdf\n"
          ]
        }
      ],
      "source": [
        "ls llama_index/docs/examples/data/10q/uber_10q_sept_2022.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtAeoSQF97Rx"
      },
      "outputs": [],
      "source": [
        "march_2022 = SimpleDirectoryReader(input_files=[\"llama_index/docs/examples/data/10q/uber_10q_march_2022.pdf\"]).load_data()\n",
        "june_2022 = SimpleDirectoryReader(input_files=[\"llama_index/docs/examples/data/10q/uber_10q_june_2022.pdf\"]).load_data()\n",
        "sept_2022 = SimpleDirectoryReader(input_files=[\"llama_index/docs/examples/data/10q/uber_10q_sept_2022.pdf\"]).load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw28Y6fu-3wx"
      },
      "outputs": [],
      "source": [
        "march_index = VectorStoreIndex.from_documents(march_2022)\n",
        "june_index = VectorStoreIndex.from_documents(june_2022)\n",
        "sept_index = VectorStoreIndex.from_documents(sept_2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HlBCpeB-4Dt"
      },
      "outputs": [],
      "source": [
        "march_engine = march_index.as_query_engine(similarity_top_k=3)\n",
        "june_engine = june_index.as_query_engine(similarity_top_k=3)\n",
        "sept_engine = sept_index.as_query_engine(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd_bZl8E-5j8"
      },
      "outputs": [],
      "source": [
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=sept_engine,\n",
        "        metadata=ToolMetadata(name='sept_22', description='Provides information about Uber quarterly financials ending September 2022')\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=june_engine,\n",
        "        metadata=ToolMetadata(name='june_22', description='Provides information about Uber quarterly financials ending June 2022')\n",
        "    ),\n",
        "    QueryEngineTool(\n",
        "        query_engine=march_engine,\n",
        "        metadata=ToolMetadata(name='march_22', description='Provides information about Uber quarterly financials ending March 2022')\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-6Ro7FN-7q4"
      },
      "outputs": [],
      "source": [
        "# Given a query, this query engine `SubQuestionQueryEngine ` will generate a “query plan”\n",
        "# containing sub-queries against sub-documents before synthesizing the final answer.\n",
        "s_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87T7bQRd-9lA",
        "outputId": "690e99b3-8976-4eb5-fb39-cd2e3f493fb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 2 sub questions.\n",
            "\u001b[36;1m\u001b[1;3m[sept_22] Q: What is the revenue growth of Uber for the quarter ending September 2022\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m[sept_22] A: compared to the same period in 2021?\n",
            "\n",
            "The revenue growth of Uber for the quarter ending September 2022 compared to the same period in 2021 is 72%.\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[june_22] Q: What is the revenue growth of Uber for the quarter ending June 2022\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3m[june_22] A: compared to the same period in 2021?\n",
            "\n",
            "Revenue increased $4.1 billion, or 105%, for the three months ended June 30, 2022 compared to the same period in 2021.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response = s_engine.query('Analyze Uber revenue growth over the latest two quarter filings')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxDRYtcK--nb",
        "outputId": "42811164-677f-4f6b-fac6-e47b4e80e752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uber's revenue growth over the latest two quarter filings has been strong, with a 72% increase for the quarter ending September 2022 compared to the same period in 2021, and a 105% increase for the quarter ending June 2022 compared to the same period in 2021.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQaqiYCeIPLP"
      },
      "source": [
        "# Router\n",
        "\n",
        "define a custom router query engine that can route to either a SQL database or a vector database.\n",
        "\n",
        "Source: https://gpt-index.readthedocs.io/en/latest/examples/query_engine/SQLRouterQueryEngine.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NM8XlJ5SL4_"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        "    SQLStructStoreIndex,\n",
        "    SQLDatabase,\n",
        "    WikipediaReader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7eXvL2-SXH6",
        "outputId": "552f3a7f-2cec-4d31-9ac6-95da7a4a5149"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['city_stats'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create Database Schema + Test Data\n",
        "# Here we introduce a toy scenario where there are 100 tables (too big to fit into the prompt)\n",
        "\n",
        "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n",
        "engine = create_engine(\"sqlite:///:memory:\", future=True)\n",
        "metadata_obj = MetaData()\n",
        "# create city SQL table\n",
        "table_name = \"city_stats\"\n",
        "city_stats_table = Table(\n",
        "    table_name,\n",
        "    metadata_obj,\n",
        "    Column(\"city_name\", String(16), primary_key=True),\n",
        "    Column(\"population\", Integer),\n",
        "    Column(\"country\", String(16), nullable=False),\n",
        ")\n",
        "\n",
        "metadata_obj.create_all(engine)\n",
        "# print tables\n",
        "metadata_obj.tables.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCyK-KwNSfKu"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import insert\n",
        "rows = [\n",
        "    {\"city_name\": \"Toronto\", \"population\": 2930000, \"country\": \"Canada\"},\n",
        "    {\"city_name\": \"Tokyo\", \"population\": 13960000, \"country\": \"Japan\"},\n",
        "    {\"city_name\": \"Berlin\", \"population\": 3645000, \"country\": \"Germany\"},\n",
        "]\n",
        "for row in rows:\n",
        "    stmt = insert(city_stats_table).values(**row)\n",
        "    with engine.connect() as connection:\n",
        "        cursor = connection.execute(stmt)\n",
        "        connection.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csoWL2frSnGH",
        "outputId": "b8a8b12d-e884-432d-f05a-6b107088c8c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Toronto', 2930000, 'Canada'), ('Tokyo', 13960000, 'Japan'), ('Berlin', 3645000, 'Germany')]\n"
          ]
        }
      ],
      "source": [
        "with engine.connect() as connection:\n",
        "    cursor = connection.exec_driver_sql(\"SELECT * FROM city_stats\")\n",
        "    print(cursor.fetchall())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJLGZCHISn33"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "# We first show how to convert a Document into a set of Nodes, and insert into a DocumentStore.\n",
        "cities = ['Toronto', 'Berlin', 'Tokyo']\n",
        "wiki_docs = WikipediaReader().load_data(pages=cities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SSuSCx8Sz5k"
      },
      "outputs": [],
      "source": [
        "# Build SQL Index\n",
        "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
        "sql_index = SQLStructStoreIndex.from_documents(\n",
        "    [],\n",
        "    sql_database=sql_database,\n",
        "    table_name=\"city_stats\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip_GdJv-S3ah"
      },
      "outputs": [],
      "source": [
        "# Build Vector Index\n",
        "# build a separate vector index per city\n",
        "# You could also choose to define a single vector index across all docs, and annotate each chunk by metadata\n",
        "vector_indices = []\n",
        "for wiki_doc in wiki_docs:\n",
        "    vector_index = VectorStoreIndex.from_documents([wiki_doc])\n",
        "    vector_indices.append(vector_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pGh81hKS4n4"
      },
      "outputs": [],
      "source": [
        "# Define Query Engines, Set as Tools\n",
        "sql_query_engine = sql_index.as_query_engine()\n",
        "vector_query_engines = [index.as_query_engine() for index in vector_indices]\n",
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    description=(\n",
        "        'Useful for translating a natural language query into a SQL query over a table containing: '\n",
        "        'city_stats, containing the population/country of each city'\n",
        "    )\n",
        ")\n",
        "vector_tools = []\n",
        "for city, query_engine in zip(cities, vector_query_engines):\n",
        "    vector_tool = QueryEngineTool.from_defaults(\n",
        "        query_engine=query_engine,\n",
        "        description=f'Useful for answering semantic questions about {city}',\n",
        "    )\n",
        "    vector_tools.append(vector_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8DbPGr7S9BL"
      },
      "outputs": [],
      "source": [
        "# Define Router Query Engine\n",
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.selectors.llm_selectors import LLMSingleSelector\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=([sql_tool] + vector_tools)\n",
        ")\n",
        "response = query_engine.query('Which city has the highest population?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7U9dbyLTFOD",
        "outputId": "245bc043-4a39-48a4-b246-528de3d41b5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Response(response=' Tokyo has the highest population, with 13,960,000 people.', source_nodes=[], extra_info={'result': [('Tokyo', 13960000)], 'sql_query': 'SELECT city_name, population FROM city_stats ORDER BY population DESC LIMIT 1;'})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vucFYIDwTHvJ",
        "outputId": "e3e60e75-6572-4b1f-86e5-6060062d485a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Tokyo has the highest population, with 13,960,000 people.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecuSETa6TJpI"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query('Tell me about the historical museums in Berlin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgnn29Q4TLi6",
        "outputId": "41fb877f-c407-4704-f7c6-de9a4e64f888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Response(response=\"\\nBerlin is home to many historical museums, including the Museum Island on the River Spree, which houses five museums built between 1830 and 1930 and is a UNESCO World Heritage site. The Altes Museum, Neues Museum, Alte Nationalgalerie, Pergamon Museum, and Bode Museum are all located on the Museum Island. The German Museum of Technology in Kreuzberg has a large collection of historical technical artifacts, and the Museum für Naturkunde (Berlin's natural history museum) exhibits natural history near Berlin Hauptbahnhof. The Jewish Museum has a standing exhibition on two millennia of German-Jewish history, and the Beate Uhse Erotic Museum was once the largest erotic museum in the world. The Brücke Museum features one of the largest collections of works by artists of the early 20th-century expressionist movement, and the Stasi Museum is located on the grounds of the former East German Ministry for State Security. The site of Checkpoint Charlie, one of the most renowned crossing points of the Berlin Wall, is also preserved.\", source_nodes=[NodeWithScore(node=Node(text='art galleries. The ensemble on the Museum Island is a UNESCO World Heritage Site and is in the northern part of the Spree Island between the Spree and the Kupfergraben. As early as 1841 it was designated a \"district dedicated to art and antiquities\" by a royal decree. Subsequently, the Altes Museum was built in the Lustgarten. The Neues Museum, which displays the bust of Queen Nefertiti, Alte Nationalgalerie, Pergamon Museum, and Bode Museum were built there.\\nApart from the Museum Island, there are many additional museums in the city. The Gemäldegalerie (Painting Gallery) focuses on the paintings of the \"old masters\" from the 13th to the 18th centuries, while the Neue Nationalgalerie (New National Gallery, built by Ludwig Mies van der Rohe) specializes in 20th-century European painting. The Hamburger Bahnhof, in Moabit, exhibits a major collection of modern and contemporary art. The expanded Deutsches Historisches Museum reopened in the Zeughaus with an overview of German history spanning more than a millennium. The Bauhaus Archive is a museum of 20th-century design from the famous Bauhaus school. Museum Berggruen houses the collection of noted 20th century collector Heinz Berggruen, and features an extensive assortment of works by Picasso, Matisse, Cézanne, and Giacometti, among others. The Kupferstichkabinett Berlin (Museum of Prints and Drawings) is part of the Staatlichen Museen zu Berlin (Berlin State Museums) and the Kulturforum at Potsdamer Platz in the Tiergarten district of Berlin\\'s Mitte district. It is the largest museum of the graphic arts in Germany and at the same time one of the four most important collections of its kind in the world. The collection includes Friedrich Gilly\\'s design for the monument to Frederick II of Prussia.\\n\\nThe Jewish Museum has a standing exhibition on two millennia of German-Jewish history. The German Museum of Technology in Kreuzberg has a large collection of historical technical artifacts. The Museum für Naturkunde (Berlin\\'s natural history museum) exhibits natural history near Berlin Hauptbahnhof. It has the largest mounted dinosaur in the world (a Giraffatitan skeleton). A well-preserved specimen of Tyrannosaurus rex and the early bird Archaeopteryx are at display as well.In Dahlem, there are several museums of world art and culture, such as the Museum of Asian Art, the Ethnological Museum, the Museum of European Cultures, as well as the Allied Museum. The Brücke Museum features one of the largest collection of works by artist of the early 20th-century expressionist movement. In Lichtenberg, on the grounds of the former East German Ministry for State Security, is the Stasi Museum. The site of Checkpoint Charlie, one of the most renowned crossing points of the Berlin Wall, is still preserved. A private museum venture exhibits a comprehensive documentation of detailed plans and strategies devised by people who tried to flee from the East.\\nThe Beate Uhse Erotic Museum claimed to be the largest erotic museum in the world until it closed in 2014.The cityscape of Berlin displays large quantities of urban street art. It has become a significant part of the city\\'s cultural heritage and has its roots in the graffiti scene of Kreuzberg of the 1980s. The Berlin Wall itself has become one of the largest open-air canvasses in the world. The leftover stretch along the Spree river in Friedrichshain remains as the East Side Gallery. Berlin today is consistently rated as an important world city for street art culture.\\nBerlin has galleries which are quite rich in contemporary art. Located in Mitte, KW Institute for Contemporary Art, KOW, Sprüth Magers; Kreuzberg there are a few galleries as well such as Blain Southern, Esther Schipper, Future Gallery, König Gallerie.\\n\\n\\n=== Nightlife and festivals ===\\n\\nBerlin\\'s nightlife has been', doc_id='01b2c0ab-ed94-4369-a5f4-60bdfb2a367f', embedding=None, doc_hash='1a3df9e9ed3932f52b705db63d78259daeb4f491c56d5c38d6ca91566270324b', extra_info=None, node_info={'start': 67160, 'end': 70983, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '4bd8b765-455b-4713-8ebe-fd5e3d2b9bb6', <DocumentRelationship.PREVIOUS: '2'>: '5e9e61d3-d4c3-4398-8df4-835a46e93094', <DocumentRelationship.NEXT: '3'>: 'ab2d7ed1-dd8d-46be-a409-fe844ab7d024'}), score=0.8747717210550219), NodeWithScore(node=Node(text='East Side Gallery is an open-air exhibition of art painted directly on the last existing portions of the Berlin Wall. It is the largest remaining evidence of the city\\'s historical division.\\nThe Gendarmenmarkt is a neoclassical square in Berlin, the name of which derives from the headquarters of the famous Gens d\\'armes regiment located here in the 18th century. Two similarly designed cathedrals border it, the Französischer Dom with its observation platform and the Deutscher Dom. The Konzerthaus (Concert Hall), home of the Berlin Symphony Orchestra, stands between the two cathedrals.\\n\\nThe Museum Island in the River Spree houses five museums built from 1830 to 1930 and is a UNESCO World Heritage site. Restoration and construction of a main entrance to all museums, as well as reconstruction of the Stadtschloss continues. Also on the island and next to the Lustgarten and palace is Berlin Cathedral, emperor William II\\'s ambitious attempt to create a Protestant counterpart to St. Peter\\'s Basilica in Rome. A large crypt houses the remains of some of the earlier Prussian royal family. St. Hedwig\\'s Cathedral is Berlin\\'s Roman Catholic cathedral.\\n\\nUnter den Linden is a tree-lined east–west avenue from the Brandenburg Gate to the site of the former Berliner Stadtschloss, and was once Berlin\\'s premier promenade. Many Classical buildings line the street, and part of Humboldt University is there. Friedrichstraße was Berlin\\'s legendary street during the Golden Twenties. It combines 20th-century traditions with the modern architecture of today\\'s Berlin.\\nPotsdamer Platz is an entire quarter built from scratch after the Wall came down. To the west of Potsdamer Platz is the Kulturforum, which houses the Gemäldegalerie, and is flanked by the Neue Nationalgalerie and the Berliner Philharmonie. The Memorial to the Murdered Jews of Europe, a Holocaust memorial, is to the north.The area around Hackescher Markt is home to fashionable culture, with countless clothing outlets, clubs, bars, and galleries. This includes the Hackesche Höfe, a conglomeration of buildings around several courtyards, reconstructed around 1996. The nearby New Synagogue is the center of Jewish culture.\\nThe Straße des 17. Juni, connecting the Brandenburg Gate and Ernst-Reuter-Platz, serves as the central east–west axis. Its name commemorates the uprisings in East Berlin of 17 June 1953. Approximately halfway from the Brandenburg Gate is the Großer Stern, a circular traffic island on which the Siegessäule (Victory Column) is situated. This monument, built to commemorate Prussia\\'s victories, was relocated in 1938–39 from its previous position in front of the Reichstag.\\nThe Kurfürstendamm is home to some of Berlin\\'s luxurious stores with the Kaiser Wilhelm Memorial Church at its eastern end on Breitscheidplatz. The church was destroyed in the Second World War and left in ruins. Nearby on Tauentzienstraße is KaDeWe, claimed to be continental Europe\\'s largest department store. The Rathaus Schöneberg, where John F. Kennedy made his famous \"Ich bin ein Berliner!\" speech, is in Tempelhof-Schöneberg.\\nWest of the center, Bellevue Palace is the residence of the German President. Charlottenburg Palace, which was burnt out in the Second World War, is the largest historical palace in Berlin.\\nThe Funkturm Berlin is a 150-meter-tall (490 ft) lattice radio tower in the fairground area, built between 1924 and 1926. It is the only observation tower which stands on insulators and has a restaurant 55 m (180 ft) and an observation deck 126 m (413 ft) above ground, which is reachable by a windowed elevator.\\nThe Oberbaumbrücke over the Spree river is Berlin\\'s most iconic bridge, connecting the now-combined boroughs of', doc_id='43ea11af-75ec-4231-8642-f0793ca8f16e', embedding=None, doc_hash='b91e6e48ebc3974365998ceba24a78c13c6f3787f531d4eef0e4510dacc18b0e', extra_info=None, node_info={'start': 27072, 'end': 30780, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '4bd8b765-455b-4713-8ebe-fd5e3d2b9bb6', <DocumentRelationship.PREVIOUS: '2'>: 'd58552d2-7a50-4f7e-8953-12cb4e311acf', <DocumentRelationship.NEXT: '3'>: 'd794a1e3-cfcd-416b-be85-ec57d0e22b2a'}), score=0.8701000929240188)], extra_info={'01b2c0ab-ed94-4369-a5f4-60bdfb2a367f': None, '43ea11af-75ec-4231-8642-f0793ca8f16e': None})"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jol2lIpTMpa",
        "outputId": "dfdfb928-b392-49c0-e790-94343c5765f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Berlin is home to many historical museums, including the Museum Island on the River Spree, which houses five museums built between 1830 and 1930 and is a UNESCO World Heritage site. The Altes Museum, Neues Museum, Alte Nationalgalerie, Pergamon Museum, and Bode Museum are all located on the Museum Island. The German Museum of Technology in Kreuzberg has a large collection of historical technical artifacts, and the Museum für Naturkunde (Berlin's natural history museum) exhibits natural history near Berlin Hauptbahnhof. The Jewish Museum has a standing exhibition on two millennia of German-Jewish history, and the Beate Uhse Erotic Museum was once the largest erotic museum in the world. The Brücke Museum features one of the largest collections of works by artists of the early 20th-century expressionist movement, and the Stasi Museum is located on the grounds of the former East German Ministry for State Security. The site of Checkpoint Charlie, one of the most renowned crossing points of the Berlin Wall, is also preserved.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW9QmBH7emZA"
      },
      "source": [
        "# Hypothetical document embeddings (HyDE)\n",
        "\n",
        "Source: https://gpt-index.readthedocs.io/en/latest/examples/query_transformations/HyDEQueryTransformDemo.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAGv8tgJepeZ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
        "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
        "from IPython.display import Markdown, display\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FkQ7Z5VerlN"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader('llama_index/examples/paul_graham_essay/data').load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7Sqh4cne58j"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV0wioHHe7_C"
      },
      "outputs": [],
      "source": [
        "query_str = \"what did paul graham do after going to RISD\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "7KAqXB9RfGXv",
        "outputId": "a637c18d-dd20-45dd-ad2e-ca93f005d9b6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "<b>\n",
              "After going to RISD, Paul Graham did freelance work for a group that did projects for customers. He also moved to Providence and then to New York City, where he became a New York artist. He looked for an apartment to buy and then moved to Cambridge, Massachusetts, where he started working on a new dialect of Lisp called Arc.</b>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Now, we use HyDEQueryTransform to generate a hypothetical document and use it for embedding lookup.\n",
        "hyde = HyDEQueryTransform(include_original=True)\n",
        "query_engine = index.as_query_engine()\n",
        "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
        "response = hyde_query_engine.query(query_str)\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvmDAfIJfMRp"
      },
      "outputs": [],
      "source": [
        "#In this example, HyDE improves output quality significantly, by hallucinating accurately what Paul Graham did after RISD (see below), and thus improving the embedding quality, and final output.\n",
        "query_bundle = hyde(query_str)\n",
        "hyde_doc = query_bundle.embedding_strs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "P9o10XhafShS",
        "outputId": "a8ca2d72-76c8-44d4-cc18-1d2285fe122e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAfter graduating from the Rhode Island School of Design (RISD) in 1985, Paul Graham went on to pursue a career in computer programming. He worked as a software developer for several companies, including Viaweb, which he co-founded in 1995. Viaweb was eventually acquired by Yahoo in 1998, and Graham used the proceeds to become a venture capitalist. He founded Y Combinator in 2005, a startup accelerator that has helped launch over 2,000 companies, including Dropbox, Airbnb, and Reddit. Graham has also written several books on programming and startups, and he continues to be an active investor in the tech industry.'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyde_doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQxA5GUjjUn"
      },
      "source": [
        "# Use LlamaIndex with LangChain\n",
        "\n",
        "Source: https://github.com/jerryjliu/llama_index/blob/main/examples/langchain_demo/LangchainDemo.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "opkGBl1njlZC",
        "outputId": "59ae41c8-bc07-42a6-ae59-a6df15cb17f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello Bob! How can I assist you today?'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "# Using LlamaIndex as a Callable Tool\n",
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader('llama_index/examples/paul_graham_essay/data').load_data()\n",
        "index = VectorStoreIndex.from_documents(documents=documents)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"LlamaIndex\",\n",
        "        func=lambda q: str(index.as_query_engine().query(q)),\n",
        "        description=\"useful for when you want to answer questions about the author. The input to this tool should be a complete english sentence.\",\n",
        "        return_direct=True\n",
        "    ),\n",
        "]\n",
        "\n",
        "# set Logging to DEBUG for more detailed outputs\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "agent_executor = initialize_agent(tools, llm, agent=\"conversational-react-description\", memory=memory)\n",
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "vpTrF1qRj50c",
        "outputId": "dd956273-ab8b-4512-dc6a-048639d11b66"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nThe author grew up writing essays, learning Italian, exploring Florence, painting people, working with computers, attending RISD, living in a rent-stabilized apartment, building an online store builder, editing Lisp expressions, publishing essays online, writing essays, painting still life, working on spam filters, cooking for groups, and buying a building in Cambridge.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.run(input=\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNoWcbRmkmrI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zXxLwcfkE4F"
      },
      "outputs": [],
      "source": [
        "# Using LlamaIndex as a memory module\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.llms import OpenAIChat\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "from llama_index import ListIndex\n",
        "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
        "index = ListIndex([])\n",
        "# set Logging to DEBUG for more detailed outputs\n",
        "# NOTE: you can also use a conversational chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Rd0hxDjSkMsI",
        "outputId": "876da6a5-140b-4113-d3f7-621a751fe473"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello Bob! How can I assist you today?'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory = GPTIndexChatMemory(\n",
        "    index=index,\n",
        "    memory_key=\"chat_history\",\n",
        "    query_kwargs={\"response_mode\": \"compact\"},\n",
        "    # return_source returns source nodes instead of querying index\n",
        "    return_source=True,\n",
        "    # return_messages returns context in message format\n",
        "    return_messages=True\n",
        ")\n",
        "llm = OpenAIChat(temperature=0)\n",
        "# llm=OpenAI(temperature=0)\n",
        "agent_executor = initialize_agent([], llm, agent=\"conversational-react-description\", memory=memory)\n",
        "agent_executor.run(input=\"hi, i am bob\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sg9DjiRwkOMF",
        "outputId": "9d12fb68-4c4a-4d8c-c9cd-ed22e4d915a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your name is Bob.'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_executor.run(input=\"what's my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35UJTZ7YkRQS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
