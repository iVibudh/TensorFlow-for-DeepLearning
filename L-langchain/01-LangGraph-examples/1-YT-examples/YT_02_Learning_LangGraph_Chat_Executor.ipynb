{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Learning LangGraph - Chat Executor"
      ],
      "metadata": {
        "id": "E5TwMbBvpk4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAoQEqlXGrWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95956980-2b89-4e80-9850-122d6d34ed23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.6/229.6 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet -U langchain langchain_openai langgraph langchainhub langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "modified from https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb"
      ],
      "metadata": {
        "id": "BL-NMZ7ve3Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph_02\""
      ],
      "metadata": {
        "id": "guac0Zh7Gz4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model"
      ],
      "metadata": {
        "id": "nGkci88EkVwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0, streaming=True)"
      ],
      "metadata": {
        "id": "58MBHiikkQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "2_I3howTkdUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "import random\n",
        "\n",
        "@tool(\"lower_case\", return_direct=True)\n",
        "def to_lower_case(input:str) -> str:\n",
        "  \"\"\"Returns the input as all lower case.\"\"\"\n",
        "  return input.lower()\n",
        "\n",
        "@tool(\"random_number\", return_direct=True)\n",
        "def random_number_maker(input:str) -> str:\n",
        "    \"\"\"Returns a random number between 0-100. input the word 'random'\"\"\"\n",
        "    return random.randint(0, 100)\n",
        "\n",
        "tools = [to_lower_case,random_number_maker]"
      ],
      "metadata": {
        "id": "OLeIVeaEJltj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tools)"
      ],
      "metadata": {
        "id": "IQkcYH78mRmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "aqJWD8X1ke5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AgentState"
      ],
      "metadata": {
        "id": "DRyUkY3cktGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "I1H-xbWNkpSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes"
      ],
      "metadata": {
        "id": "FbQtOmzQk27f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.agents import AgentFinish\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state):\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we finish\n",
        "    if \"function_call\" not in last_message.additional_kwargs:\n",
        "        return \"end\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state):\n",
        "    messages = state['messages']\n",
        "    response = model.invoke(messages)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Define the function to execute tools\n",
        "def call_tool(state):\n",
        "    messages = state['messages']\n",
        "    # Based on the continue condition\n",
        "    # we know the last message involves a function call\n",
        "    last_message = messages[-1]\n",
        "    # We construct an ToolInvocation from the function_call\n",
        "    action = ToolInvocation(\n",
        "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
        "    )\n",
        "    print(f\"The agent action is {action}\")\n",
        "    # We call the tool_executor and get back a response\n",
        "    response = tool_executor.invoke(action)\n",
        "    print(f\"The tool result is: {response}\")\n",
        "    # We use the response to create a FunctionMessage\n",
        "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [function_message]}"
      ],
      "metadata": {
        "id": "2HoxaGZbkvi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph"
      ],
      "metadata": {
        "id": "UvaLZp3jlM9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "\n",
        "# Set the entrypoint as `agent` where we start\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        "    # Finally we pass in a mapping.\n",
        "    # The keys are strings, and the values are other nodes.\n",
        "    # END is a special node marking that the graph should finish.\n",
        "    # What will happen is we will call `should_continue`, and then the output of that\n",
        "    # will be matched against the keys in this mapping.\n",
        "    # Based on which one it matches, that node will then be called.\n",
        "    {\n",
        "        # If `tools`, then we call the tool node.\n",
        "        \"continue\": \"action\",\n",
        "        # Otherwise we finish.\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge('action', 'agent')\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "2Vxw2TOClGYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it"
      ],
      "metadata": {
        "id": "AAVFBk3GlY5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELSoxDK_lRO_",
        "outputId": "5882db0a-eeea-4094-ee36-d21ca8d0e46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The agent action is tool='random_number' tool_input={'input': 'random'}\n",
            "The tool result is: 83\n",
            "The agent action is tool='lower_case' tool_input={'input': 'eighty-three'}\n",
            "The tool result is: eighty-three\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content='give me a random number and then write in words and make it lower case'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"random\"\\n}', 'name': 'random_number'}}),\n",
              "  FunctionMessage(content='83', name='random_number'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"eighty-three\"\\n}', 'name': 'lower_case'}}),\n",
              "  FunctionMessage(content='eighty-three', name='lower_case'),\n",
              "  AIMessage(content='The random number is 83. In words, it is \"eighty-three\" and in lower case, it is \"eighty-three\".')]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "id": "bhW-wd2Olh6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b3aa2b-f74a-4956-ca91-39b5caf8d9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The agent action is tool='lower_case' tool_input={'input': 'Merlion'}\n",
            "The tool result is: merlion\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content=\"plear write 'Merlion' in lower case\"),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"Merlion\"\\n}', 'name': 'lower_case'}}),\n",
              "  FunctionMessage(content='merlion', name='lower_case'),\n",
              "  AIMessage(content='The word \"Merlion\" in lower case is \"merlion\".')]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdwMgR5Ag6pi",
        "outputId": "f107fe06-29e5-4686-de05-2612956e9fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content='what is a Merlion?'),\n",
              "  AIMessage(content='A Merlion is a mythical creature with the head of a lion and the body of a fish. It is a symbol of Singapore and is often used to represent the city-state. The name \"Merlion\" is a combination of the words \"mer\" (meaning sea) and \"lion\". The Merlion is depicted spouting water from its mouth and is a popular tourist attraction in Singapore.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6nGDuI6g-Dg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}